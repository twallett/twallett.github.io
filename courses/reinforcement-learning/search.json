[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "",
    "text": "Instructor Information",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "",
    "text": "Name: Tyler Wallett\nTerm: Fall 2025\n\nClass location: MON 114\nClass hours: 06:10 PM - 08:40 PM\nOffice location: Samson Hall Room 310\n\nOffice hours: Monday‚Äôs 2 - 4 PM\n\nE-mail: twallett@gwu.edu\n\nGitHub: twallett",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Course Description",
    "text": "Course Description\nThe aim of this course is to provide a comprehensive understanding of the reinforcement learning framework. The course will explore the key distinctions between reinforcement learning and other artificial intelligence learning paradigms, delve into relevant industry applications, and examine both classical and deep learning approaches. Additionally, the course will cover the taxonomy of reinforcement learning and offer hands-on experience through practical implementations using OpenAI Gymnasium (Brockman et al. 2016) and other learning environments.\nThe classical approach will focus on learning methods designed to find optimal solutions in tabular environments, whereas the deep learning approach will tackle the challenge of finding approximate optimal solutions in large or continuous environments through the use of deep learning architectures.\nThe course will introduce the taxonomy of reinforcement learning by focusing on model-free value-based and policy-based methods. Model-based reinforcement learning will be covered briefly, as it is allocated only one lecture.\nTo conclude, a discussion on advanced topics, applications, and outlook of reinforcement learning will be provided.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement reinforcement learning frameworks using numpy and tensorflow. \nDesign decision-making systems using classical and deep learning architectures. \nExplain the reinforcement learning taxonomy. \nIdentify reinforcement learning‚Äôs challenges, current research, and future outlook.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Resources",
    "text": "Resources\n\nReinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto (Web Link)\nThe Reinforcement Learning Course by Hugging Face (Web Link)\nSpinning Up in Deep RL by OpenAI (Web Link)\nOpenAI Gymnasium API documentation (Web Link)\nTensorflow Python API documentation (Web Link)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#software-requirements",
    "href": "index.html#software-requirements",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nProgramming Language: Python.\n\npip install numpy tensorflow pygame gymnasium pickle tqdm tensorboard\n\nCloud-based GPU Environment: Google Colab.\nVersion Control: GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Course Outline",
    "text": "Course Outline\nSummary of the Course Outline section.\n\n\n\nWeek\nTopic\nQuiz/Exams\nLearning Objectives\n\n\n\n\nAug.¬†28, 2025\nIntroduction\n\n‚Ä¢ Why Should I Learn Reinforcement Learning?  ‚Ä¢ What is Reinforcement Learning?  ‚Ä¢ Where is Reinforcement Learning Applied?  ‚Ä¢ How is Reinforcement Learning Structured? \n\n\nSep.¬†4, 2025\nMath Foundations\n\n‚Ä¢ Set Theory  ‚Ä¢ Axiomatic Probability  ‚Ä¢ Conditioning  ‚Ä¢ Independence  ‚Ä¢ Random Variables  ‚Ä¢ Expectation  ‚Ä¢ Probability Distribution\n\n\nSep.¬†11, 2025\nMulti-Armed Bandits\nQuiz 1\n‚Ä¢ Multi-Armed Bandit Framework  ‚Ä¢ \\(\\epsilon\\)-Greedy  ‚Ä¢ Upper Confidence Boundary (UCB)  ‚Ä¢ Thompson Sampling\n\n\nSep.¬†18, 2025\nDynamic Programming\nQuiz 2\n‚Ä¢ OpenAI Gymansium GridWorldEnv  ‚Ä¢ Markov Chain  ‚Ä¢ Markov Decision Process (MDPs)  ‚Ä¢ Iterative Policy Evaluation  ‚Ä¢ Value Iteration\n\n\nSep.¬†25, 2025\nMonte Carlo\nQuiz 3\n‚Ä¢ OpenAI Gymansium GridWorldEnv  ‚Ä¢ Monte Carlo Prediction  ‚Ä¢ Exploring Starts Monte Carlo  ‚Ä¢ On-Policy Monte Carlo  ‚Ä¢ Off-Policy Monte Carlo\n\n\nOct.¬†2, 2025\nTemporal Difference\nQuiz 4\n‚Ä¢ OpenAI Gymansium GridWorldEnv  ‚Ä¢ Temporal Difference (TD) Prediction  ‚Ä¢ SARSA  ‚Ä¢ Q-Learning  ‚Ä¢ Double Q-Learning  ‚Ä¢ (Optional) n-step Bootstrapping\n\n\nOct.¬†9, 2025\nFall Break\n\n\n\n\nOct.¬†16, 2025\nFunction Approximation\nExam 1\n‚Ä¢ OpenAI Gymansium MountainCar-v0  ‚Ä¢ Value Function Approximation (VFA)  ‚Ä¢ On-Policy Function Approximation  ‚Ä¢ Semi-gradient SARSA  ‚Ä¢ Limitations of Off-Policy Function Approximation\n\n\nOct.¬†23, 2025\nDeep Q-Networks\nQuiz 5\n‚Ä¢ OpenAI Gymansium ALE/Breakout-v5  ‚Ä¢ Deep Learning  ‚Ä¢ Deep Q-Networks (DQN)\n\n\nOct.¬†30, 2025\nPolicy Gradients I\nQuiz 6\n‚Ä¢ OpenAI Gymansium CartPole-v1 and Pusher-v5  ‚Ä¢ Policy Gradient Theorem  ‚Ä¢ Addressing Sparse Rewards  ‚Ä¢ Action Selections  ‚Ä¢ Vanilla Policy Gradient (VPG)\n\n\nNov.¬†6, 2025\nPolicy Gradients II\nQuiz 7\n‚Ä¢ OpenAI Gymansium HalfCheetah-v5  ‚Ä¢ Trust Regions  ‚Ä¢ Monotonic Improvement  ‚Ä¢ Proximal Policy Optimization (PPO)\n\n\nNov.¬†13, 2025\nMonte Carlo Tree Search\nExam 2\n‚Ä¢ OpenAI Gymansium CartPole-v0  ‚Ä¢ Model-based Reinforcement Learning  ‚Ä¢ Monte Carlo Tree Search (MCTS)\n\n\nNov.¬†20, 2025\nConclusion\n\n‚Ä¢ Advanced Topics in Reinforcement Learning  ‚Ä¢ Identify the Reinforcement Learning Application  ‚Ä¢ Outlook of Reinforcement Learning\n\n\nNov.¬†27, 2025\nThanksgiving Break\n\n\n\n\nDec.¬†4, 2025\nFinal Project Presentation & Submission",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nDATS 6101 - Introduction to Data Science",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "index.html#assignments-grading",
    "href": "index.html#assignments-grading",
    "title": "DATS 6450 ‚Äì Reinforcement Learning",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\nSummary of the Assignments & Grading section.\n\n\n\nAssignment\nPoints\n\n\n\n\nQuizzes (5 best scores)\n25\n\n\nExam 1\n25\n\n\nExam 2\n25\n\n\nFinal Project\n25\n\n\n\n\n\n\n\n\n\nNoteAverage Learning Per Week\n\n\n\nStudents are expected to spend a minimum of 100 minutes of out-of-class work for every 50 minutes of direct instruction, for a minimum total of 2.5 hours a week. A 3-credit course should include 2.5 hours of direct instruction and a minimum of 5 hours of independent learning or 7.5 hours per week.\n\n\n\n\n\n\n\n\nNoteOnline Resources\n\n\n\nFor technical requirements and support, student services, obtaining a GWorld card, and state contact information please check HERE\n\n\n\n\n\n\n\n\nNoteClassroom Recording\n\n\n\nThe particular class recordings will be available to students who are registered on an individual basis, upon request. Please let me know in advance if you have any medical issues or emergencies that will prevent you from joining the class.\n\n\n\n\n\n\n\n\nTipVirtual Academic Support\n\n\n\nWriting and research consultations are available online. See HERE. Coaching, offered through the Office of Student Success, is available in a virtual format. See HERE. Academic Commons offers several short videos addressing different virtual learning strategies for the unique circumstances of the fall 2020 semester. See HERE. They also offer a variety of live virtual workshops to equip students with the tools they need to succeed in a virtual environment. See HERE.\n\n\n\n\n\n\n\n\nWarningSafety and Security\n\n\n\nIn an emergency: call GWPD 202-994-6111 or 911. For situation-specific actions: review the Emergency Response Handbook in HERE. In an active violence situation: Get Out, Hide Out, or Take Out. See HERE. Stay informed: safety.gwu.edu/stay-informed.\n\n\n\n\n\n\nBrockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. ‚ÄúOpenAI Gym.‚Äù https://arxiv.org/abs/1606.01540.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>DATS 6450 ‚Äì Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html",
    "href": "lecture1/lecture1-1.html",
    "title": "1.1 Why Should I Learn Reinforcement Learning?",
    "section": "",
    "text": "Advances in Artificial Intelligence\nIn essence, the AI model‚Äôs behavior closely mirrors human-like actions. This is expected, given its training on extensive datasets derived from human behavior. However, continued training on the same datasets will likely result in models that perform at a human-equivalent level. To achieve significant breakthroughs, we must explore learning methods that transcend typical human capabilities.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why Should I Learn Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html#advances-in-artificial-intelligence",
    "href": "lecture1/lecture1-1.html#advances-in-artificial-intelligence",
    "title": "1.1 Why Should I Learn Reinforcement Learning?",
    "section": "",
    "text": "June 2018: OpenAI introduces the Generative Pre-trained Transformer (GPT), laying the foundation for subsequent LLMs.\nFebruary 2019: OpenAI releases GPT-2, demonstrating significant improvements in text generation capabilities.\nJune 2020: GPT-3 is unveiled, featuring 175 billion parameters and showcasing advanced language understanding and generation.\nJanuary 2021: OpenAI announces DALL-E, a model capable of generating images from textual descriptions.\nApril 2022: DALL-E 2 is introduced, offering enhanced image resolution and greater realism in generated images.\nNovember 2022: OpenAI releases ChatGPT, a conversational AI based on the GPT-3.5 architecture, enabling more interactive and contextually relevant dialogues.\n\n\n\n\n\nAt GWU, where minds aspire,  Reinforcement learners never tire  In search of policies, bold and bright,  They train their agents, day and night.   With Sutton, Barto as their guide,  They walk the path, rewards beside.  Exploring states with epsilon‚Äôs grace,  They find the optimal embrace.   Gridworlds vast, mazes deep,  In code they sow, in dreams they reap.  The future‚Äôs theirs, they push, they strive‚Äì  GWU‚Äôs learners, alive, alive!\n\n\n\nPoem and images generated by Large Language Model: ChatGPT 3.5 & Diffusion Models: DALL-E, correspondingly.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why Should I Learn Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-1.html#the-goal-of-reinforcement-learning",
    "href": "lecture1/lecture1-1.html#the-goal-of-reinforcement-learning",
    "title": "1.1 Why Should I Learn Reinforcement Learning?",
    "section": "The Goal of Reinforcement Learning",
    "text": "The Goal of Reinforcement Learning\nUltimately, as scientists, we want to discover new solutions for a task so that when an agent, or decision-maker, is placed in a novel situation it can respond intelligently (Levine 2019).\n\n\n\n\n\n\nTipEmergent Behavior\n\n\n\nReinforcement Learning is concerned with seeking emergent behavior, or behavior that goes beyond what people might do or think of. \n\n\n\nExample: AlphaGO\n\n\n\n\n\nAlphaGo Move 37\n\n\n\n‚ÄúIt‚Äôs not a human move. I‚Äôve never seen a human play this move.‚Äù ‚Äì Commentator on Move 37, AlphaGo (2017)\n\n\n\n\nExample: Matrix Multiplication (Fawzi et al. 2022)\n\n\n\n\n\nDiscovering Faster Matrix Multiplication Algorithms with Reinforcement Learning\n\n\n\n‚ÄúTrained from scratch, AlphaTensor discovers matrix multiplication algorithms that are more efficient than existing human and computer-designed algorithms.‚Äù - Discovering faster matrix multiplication algorithms with reinforcement learning\n\n\n\n\n\n\nFawzi, Alhussein, Matej Balog, Atri Huang, et al. 2022. ‚ÄúDiscovering Faster Matrix Multiplication Algorithms with Reinforcement Learning.‚Äù Nature 610: 47‚Äì53. https://doi.org/10.1038/s41586-022-05172-4.\n\n\nLevine, Sergey. 2019. ‚ÄúIntroduction to Deep Reinforcement Learning.‚Äù Course Lecture Slides, Deep RL Course, UC Berkeley. https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-1.pdf.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>1.1 Why Should I Learn Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html",
    "href": "lecture1/lecture1-2.html",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "",
    "text": "Learning\nGood news! We can sum up the core idea of Reinforcement Learning in just one powerful sentence (Brunskill 2022):\nBut what exactly does that mean? Let‚Äôs break it down!\nAt its core, learning in Reinforcement Learning occurs through trial and error, where an agent refines its actions based on evaluative feedback from the environment.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html#learning",
    "href": "lecture1/lecture1-2.html#learning",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "",
    "text": "TipEvaluative Feedback\n\n\n\nEvaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action. Intuitively, this type of feedback can be thought of as learning through experience.\n\n\n\nExample: Learning How to Walk\n\n\n\n\n\nLearning to Walk\n\n\n\nLearning to walk involves trial and error, where feedback comes from the outcome of each attempt‚Äîsuccess or falling‚Äîrather than explicit instruction. This aligns with evaluative feedback in reinforcement learning, where the agent learns from the consequences of its actions, not direct guidance.\n\n\n\n\nExample: Learning to Distinguish Right from Wrong\n\n\n\n\n\nLearning to Distinguish Right from Wrong\n\n\n\nLearning to distinguish right from wrong often relies on experiencing the outcomes of decisions and receiving approval or disapproval from others. This reflects evaluative feedback in reinforcement learning, where behavior is shaped by rewards or penalties rather than explicit rules.\n\n\nUnlike both Supervised/Unsupervised Learning which rely on instructive feedback through gradient-based optimization.\n\n\n\n\n\n\nTipInstructive Feedback\n\n\n\nInstructive feedback indicates the correct action to take, independently of the action actually taken. Intuitively, this type of feedback can be thought of as learning from ground truth.\n\n\n\n\n\n\nSupervised/Unsupervised Learning\n\n\n\n\n\nExample: Cheetah\nSupervised/Unsupervised learning focus on identifying what makes an image a cheetah by learning patterns from a dataset of animal images. In contrast, Reinforcement Learning is about teaching a cheetah how to run by interacting with its environment (Lecture 10).\n\n\n\n\n\nSupervised Learning\n\n\n\n‚ÄúHere‚Äôs some examples (images), now learn patterns in these examples‚Ä¶‚Äù\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\n‚ÄúHere‚Äôs an environment, now learn patterns by exploring it‚Ä¶‚Äù",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html#optimal",
    "href": "lecture1/lecture1-2.html#optimal",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "Optimal",
    "text": "Optimal\nThe goal of Reinforcement Learning is to maximize rewards over time by finding the best possible strategy. This involves seeking:\n\nA maximized discounted sum of rewards, or goal \\(G\\).\nOptimal Value Functions \\(V^{*}\\).\nOptimal Action-Value Functions \\(Q^{*}\\).\nOptimal Policies \\(\\pi^{*}\\).\nA balance between exploration vs.¬†exploitation \\(\\epsilon\\).",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-2.html#sequential-decision-making",
    "href": "lecture1/lecture1-2.html#sequential-decision-making",
    "title": "1.2 What is Reinforcement Learning?",
    "section": "Sequential Decision-Making",
    "text": "Sequential Decision-Making\nUnlike a one-time choice, Reinforcement Learning involves a chain of decisions where each action affects the next.\n\n\n\n\nMarkov Decision Process\n\n\n\n\n\\(\\pi: S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, ... , S_{T-1}, A_{T-1}, R_{T}\\)\n\n\nMarkov Decision Process (MDP) is a formal framework for modeling decision-making.\nThe agent selects actions \\(A_t\\) over multiple time steps, shaping its future states \\(S_t\\) and rewards \\(R_t\\).\nEach decision affects not only immediate rewards but also the trajectory \\(\\tau\\) of future outcomes.\n\n\n\n\n\nBrunskill, Emma. 2022. ‚ÄúCS234: Reinforcement Learning - Lecture 1.‚Äù Course Lecture Slides, Stanford University. https://web.stanford.edu/class/cs234/slides/lecture1pre.pdf.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>1.2 What is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html",
    "href": "lecture1/lecture1-3.html",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "",
    "text": "Recommendation Systems\nReinforcement Learning powers modern recommendation systems by dynamically adapting to user preferences, optimizing content suggestions in platforms like Netflix, YouTube, and Spotify. Techniques such as Multi-Armed Bandits (MAB) and Q-Learning‚Äîused in tools like Google Research‚Äôs RecSim‚Äîenable these systems to learn from user interactions and improve recommendations over time.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#recommendation-systems",
    "href": "lecture1/lecture1-3.html#recommendation-systems",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "",
    "text": "Trained using MAB (Lecture 3)\nLink to Online Article\n\n\n\nTrained using Q-Learning (Lecture 6)\nLink to GitHub Repository",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#games",
    "href": "lecture1/lecture1-3.html#games",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Games",
    "text": "Games\nReinforcement Learning has revolutionized gaming by enabling AI to master complex environments, from Atari classics (Mnih et al. 2013) to advanced strategy games, using deep learning techniques like Deep Q-Networks (DQN) to achieve superhuman performance.\n\n\nTrained using DQN (Lecture 8)\nLink to Research Paper",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#robotics",
    "href": "lecture1/lecture1-3.html#robotics",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Robotics",
    "text": "Robotics\nIn robotics, Reinforcement Learning enables autonomous agents to learn complex motor skills, such as dexterous manipulation and locomotion, through continuous interaction and training with algorithms like Proximal Policy Optimization (PPO). Platforms such as NVIDIA Isaac Gym provide high-performance simulation environments that accelerate RL training for these tasks.\n\n\nTrained using PPO (Lecture 10)\nLink to Blog\n\n\n\nTrained using MCTS (Lecture 11)\nLink to Documentation",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#autonomous-vehicles",
    "href": "lecture1/lecture1-3.html#autonomous-vehicles",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Autonomous Vehicles",
    "text": "Autonomous Vehicles\nSelf-driving cars rely on Reinforcement Learning to navigate complex environments, optimize decision-making, and improve safety, often incorporating PPO and deep learning to refine real-time control strategies.\n\n   Partly trained using PPO (Lecture 10)",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#natural-language-processing",
    "href": "lecture1/lecture1-3.html#natural-language-processing",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nReinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022) enhances AI language models like ChatGPT, allowing them to refine responses based on user interactions and align better with human preferences.\n\n\nTrained using PPO (Lecture 10)\nLink to Research Paper",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-3.html#finance",
    "href": "lecture1/lecture1-3.html#finance",
    "title": "1.3 Where is Reinforcement Learning Applied?",
    "section": "Finance",
    "text": "Finance\nIn financial markets, Reinforcement Learning is applied to portfolio optimization (Acero et al. 2024), algorithmic trading, and risk management, leveraging techniques like PPO to make data-driven investment decisions.\n\n\nTrained using PPO (Lecture 10)\nLink to Research Paper\n\n\n\n\n\nAcero, Fernando, Parisa Zehtabi, Nicolas Marchesotti, Michael Cashmore, Daniele Magazzeni, and Manuela Veloso. 2024. ‚ÄúDeep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization.‚Äù https://arxiv.org/abs/2403.16667.\n\n\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. ‚ÄúPlaying Atari with Deep Reinforcement Learning.‚Äù https://arxiv.org/abs/1312.5602.\n\n\nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. ‚ÄúTraining Language Models to Follow Instructions with Human Feedback.‚Äù https://arxiv.org/abs/2203.02155.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>1.3 Where is Reinforcement Learning Applied?</span>"
    ]
  },
  {
    "objectID": "lecture1/lecture1-4.html",
    "href": "lecture1/lecture1-4.html",
    "title": "1.4 How is Reinforcement Learning Structured?",
    "section": "",
    "text": "Finally, to wrap up our introduction, let‚Äôs take a look at:\n\nHow is Reinforcement Learning Structured? ü§î\n\n\n\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\nLecture 1 ‚Äì Introduction.\nA discussion on why it matters, what it is, where it is applied, and how it is structured.\nLecture 2 ‚Äì Mathematical foundations for understanding Reinforcement Learning.\nKey math topics covered include Set Theory, Linear Algebra, Calculus, and Probability.\nLecture 3 ‚Äì Learn to make decisions in one-state environments using the Multi-Armed Bandits framework.\nKey algorithms covered are \\(\\epsilon\\)-greedy, Upper Confidence Bound (UCB), and Thompson Sampling.\nLectures 4‚Äì6 ‚Äì Learn to make decisions in multiple finite-state environments by leveraging experience and bootstrapping.\nKey algorithms covered are Dynamic Programming (DP), Monte Carlo (MC), Temporal Difference (TD), and n-step Bootstrapping.\nLectures 7‚Äì11 ‚Äì Learn to make decisions in multiple infinite-state environments using gradient-based learning and classical RL methods (MC and TD).\nKey algorithms covered are Value Function Approximation (VFA), Deep Q-Networks (DQN), Policy Gradients (VPG), Advanced Policy Gradients (APG), and Monte Carlo Tree Search (MCTS).\nLecture 12 ‚Äì Conclusion.\nA discussion on advanced topics, real-world applications, and the future outlook of Reinforcement Learning.",
    "crumbs": [
      "Lecture 1: Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>1.4 How is Reinforcement Learning Structured?</span>"
    ]
  },
  {
    "objectID": "lecture2/lo.html",
    "href": "lecture2/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 2: Mathematical Foundations (Hammack 2013) (Bertsekas and Tsitsiklis 2008) üéØ\n\n\n\n\nSet Theory\nProbability:\n\nAxiomatic Probability\nConditioning\nIndependence\nDiscrete Random Variables\nContinuous Random Variables\nProbability Distributions\n\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\n\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to Probability. 2nd ed. Belmont, MA: Athena Scientific.\n\n\nHammack, Richard H. 2013. Book of Proof. Richard Hammack.",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html",
    "href": "lecture2/lecture2-1.html",
    "title": "2.1 Set Theory",
    "section": "",
    "text": "Sets\nA set is a collection of things.\nThe things are called elements of a set.\n\\[\nColors = \\{\\text{Red}, \\text{Blue}, \\text{Green}\\}\n\\]\n\\[\nNumbers = \\{1,2,3\\}\n\\]\nSets can be finite or infinite.\n\\[\nSome\\ Even\\ Integers = \\{2,4,6\\}\n\\]\n\\[\nAll\\ Even\\ Integers = \\{..., -4, -2, 0, 2, 4, ...\\}\n\\]\nThe number of elements in a set is called the cardinality.\n\\[\n|Colors| = 3\n\\]\nTwo sets are equal if they share exactly the same elements.\n\\[\nA = \\{2,4,6\\}, B = \\{4,2,6\\}, C = \\{4,2,7\\}\n\\]\n\\[\nA = B\n\\] \\[\nA \\neq C\n\\]\nTo express that \\(2\\) is an element of \\(A\\), we denote:\n\\[\n2 \\in A\n\\] \\[\n\\text{2 exists in A}\n\\]\n\\[\n5 \\notin A\n\\] \\[\n\\text{5 does not exist in A}\n\\]\nSome sets are so significant that we reserve special symbols for them:\n\\[\n\\emptyset = \\{\\} \\quad \\textbf{(empty set)}\n\\]\n\\[\n\\mathbb{N} = \\{1, 2, 3, ... \\} \\quad \\textbf{(natural numbers)}\n\\]\n\\[\n\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ... \\} \\quad \\textbf{(integers)}\n\\]\n\\[\n\\mathbb{R} = \\{..., -0.22,...,0,...,1,..., \\pi, ... \\} \\quad \\textbf{(real numbers)}\n\\]\nWe visualize \\(\\mathbb{R}\\) as an infinitely long number line.",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#set-builder-notation",
    "href": "lecture2/lecture2-1.html#set-builder-notation",
    "title": "2.1 Set Theory",
    "section": "Set-Builder Notation",
    "text": "Set-Builder Notation\nA special notation called set-builder notation is used to describe sets that are too big or complex to list between braces.\n\\[\nAll\\ Even\\ Integers_{1} = \\{..., -4, -2, 0, 2, 4, ...\\}\n\\]\n\\[\nAll\\ Even\\ Integers_{2} = \\{2x: x \\in \\mathbb{Z} \\}\n\\]\n\\[\n\\text{The set of all numbers of the form } 2x \\text{ such that } x \\in \\mathbb{Z}\n\\]\n\\[\nAll\\ Even\\ Integers_{1} = All\\ Even\\ Integers_{2}\n\\]\n\n\n\n\n\n\nNoteExercise\n\n\n\nWrite the following sets in set-builder notation:\n\n\\(\\{ 2, 4, 8, 16, 32, 64, ... \\}\\)\n\\(\\{ 0, 1, 4, 9, 16, 25, 36, ... \\}\\)\n\\(\\{ 3, 4, 5, 6, 7, 8 \\}\\)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\\(\\{ 2^x: x \\in \\mathbb{N} \\}\\)\n\\(\\{ x^2: x \\in \\mathbb{Z} \\}\\)\n\\(\\{ x \\in \\mathbb{Z}: 3 \\le x \\le 8 \\}\\)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#subsets",
    "href": "lecture2/lecture2-1.html#subsets",
    "title": "2.1 Set Theory",
    "section": "Subsets",
    "text": "Subsets\nSuppose \\(A\\) and \\(B\\) are sets.\nIf every element of \\(A\\) is also an element of \\(B\\), then we say \\(A\\) is a subset of \\(B\\), denoted \\(A \\subseteq B\\).\nWe write \\(A \\not\\subseteq B\\) if \\(A\\) is not a subset of \\(B\\), that is, if it is not true that every element of \\(A\\) is also an element of \\(B\\).\nThus \\(A \\not\\subseteq B\\) means that there is at least one element of \\(A\\) that is not an element of \\(B\\).\n\\[\n\\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{R}\n\\]\n\\[\nA = \\{1,2\\}, B = \\{2,3,4\\}\n\\] \\[\nA \\not\\subseteq B\n\\]\n\n\n\n\n\n\nNoteExercise\n\n\n\nList all the subsets of the following sets:\n\n\\(\\{1,2,3\\}\\)\n\\(\\{1,\\{2,3\\}\\}\\)\n\\(\\{\\mathbb{N}, \\mathbb{Z}, \\mathbb{R}\\}\\)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\\(\\{\\}, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\)\n\\(\\{\\}, \\{1\\}, \\{\\{2,3\\}\\}, \\{1,\\{2,3\\}\\}\\)\n\\(\\{\\}, \\{\\mathbb{N}\\}, \\{\\mathbb{Z}\\}, \\{\\mathbb{R}\\}, \\{\\mathbb{N},\\mathbb{Z}\\}, \\{\\mathbb{N},\\mathbb{R}\\}, \\{\\mathbb{Z},\\mathbb{R}\\}, \\{\\mathbb{N},\\mathbb{Z},\\mathbb{R}\\}\\)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#union-intersection-and-difference",
    "href": "lecture2/lecture2-1.html#union-intersection-and-difference",
    "title": "2.1 Set Theory",
    "section": "Union, Intersection, and Difference",
    "text": "Union, Intersection, and Difference\nSuppose \\(A\\) and \\(B\\) are sets.\nA union of \\(A\\) and \\(B\\) is the set: \\[\nA \\cup B = \\{x: x \\in A \\text{ or } x \\in B \\}\n\\]\nA intersection of \\(A\\) and \\(B\\) is the set: \\[\nA \\cap B = \\{x: x \\in A \\text{ and } x \\in B \\}\n\\]\nA difference of \\(A\\) and \\(B\\) is the set: \\[\nA - B = \\{x: x \\in A \\text{ and } x \\notin B \\}\n\\]\n\n\n\n\n\n\n\n\n\nNoteExercise\n\n\n\nShade in the region matching the expression:\n\n\\((A \\cap B) \\cap C\\)\n\\((A \\cup B) \\cap C\\)\n\\((A \\cup B) - C\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#complements",
    "href": "lecture2/lecture2-1.html#complements",
    "title": "2.1 Set Theory",
    "section": "Complements",
    "text": "Complements\nSuppose \\(A\\) is a set.\nA universal set is a larger set that encompasses other sets.\nThe complement of \\(A\\), denoted \\(\\bar{A}\\), is the set \\(\\bar{A} = U - A\\).\n\\[\nP = \\{2, 3, 5, 7, ...\\} \\quad \\textbf{(prime numbers)}\n\\]\n\\[\n\\bar{P} = \\mathbb{N} - P = \\{1, 4, 6, ...\\}\n\\]\n\n\n\n\n\n\nNoteExercise\n\n\n\nFind \\(\\bar{A}\\):\n\\[\nA = \\{1,2,3\\}, U = \\{0,1,2,3,4,5\\}\n\\]\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\[\n\\bar{A} = \\{0, 4, 5\\}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#ordered-pair",
    "href": "lecture2/lecture2-1.html#ordered-pair",
    "title": "2.1 Set Theory",
    "section": "Ordered Pair",
    "text": "Ordered Pair\nAn ordered pair is a list \\((x,y)\\) of two elements \\(x\\) and \\(y\\), enclosed in parentheses and separated by a comma.\n\\[\n(1,2)\n\\] \\[\n(2,1)\n\\]\nHowever:\n\\[\n(1,2) \\neq (2,1)\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#cartesian-product",
    "href": "lecture2/lecture2-1.html#cartesian-product",
    "title": "2.1 Set Theory",
    "section": "Cartesian Product",
    "text": "Cartesian Product\nSuppose \\(A\\) and \\(B\\) are sets.\nA cartesian product is simply the multiplication of sets denoted as \\(A\\) x \\(B\\) and defined as\n\\[\nA x B = \\{(a,b): a \\in A, \\ b \\in B \\}\n\\]\n\\[\nA = \\{a, b, c\\}, \\ B = \\{1, 2, 3\\}\n\\]\n\\[\nA \\ \\text{x} \\ B = \\{ (a,1), (a,2), (a,3), (b,1), (b,2), (b,3), (c,1), (c,2), (c,3) \\}\n\\]\nOrdered tripples such as \\((x,y,z)\\) are also possible.\n\\[\nA = \\{a, b\\}, \\ B = \\{1, 2\\}, \\ C = \\{I, II\\}\n\\]\n\\[\nA \\ \\text{x} \\ B \\ \\text{x} \\ C = \\{ (a,1,I), (a,1,II), (a,2,I), (a,2,II), (b,1,I), (b,1,II), (b,2,I), (b,2,II) \\}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-1.html#cartesian-power",
    "href": "lecture2/lecture2-1.html#cartesian-power",
    "title": "2.1 Set Theory",
    "section": "Cartesian Power",
    "text": "Cartesian Power\nA cartesian power is also possible for any integer \\(n\\) as\n\\[\nA^n = A \\ \\text{x} \\ A \\ \\text{x} \\ ... \\ \\text{x} \\ A = \\{ (x_1, x_2, ..., x_n):x_1,x_2,...,x_n \\in A\\}\n\\]\nOne famous cartesian power is \\(\\mathbb{R}^2\\), also known as the cartesian plane or a two-dimensional plane.\n\\[\n\\mathbb{R} \\ \\text{x} \\ \\mathbb{R} = \\mathbb{R}^2\n\\]\n\n\n\n\\(\\mathbb{R}^3\\) three-dimensional planes are also possible.\n\\[\n\\mathbb{R} \\ \\text{x} \\ \\mathbb{R} \\ \\text{x} \\ \\mathbb{R} = \\mathbb{R}^3\n\\]\n\n\n\nAnd we can generalize up to \\(n\\) dimensions.\n\\[\n\\mathbb{R} \\ \\text{x} \\ \\mathbb{R} \\ \\text{x} \\ ... \\ \\text{x} \\ \\mathbb{R} = \\mathbb{R}^n\n\\]\n\n\n\n\n\n\nIn Data Science, we often work in high-dimensional spaces‚Äîsometimes with thousands or even millions of dimensions. GPT-4, for example, is rumored to have over a trillion \\(\\mathbb{R}^{1,000,000,000,000,000}\\).\n\n\n\n\n\n\n\n\n\nNoteExercise\n\n\n\nFor the following sets list the elements of their corresponding cartesian product:\n\\(A = \\{1,2,3\\}\\) \\(B = \\{1, (2,3) \\}\\) \\(C = \\{\\mathbb{Z}, \\mathbb{R}\\}\\)\n\n\\(A\\) x \\(B\\)\n\\(A\\) x \\(C\\)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\\(A \\ \\text{x} \\ B = \\{ (1,1), (1,(2,3)), (2,1), (2, (2,3)), (3,1), (3,(2,3)) \\}\\)\n\\(A \\ \\text{x} \\ C = \\{ (1,\\mathbb{Z}), (1,\\mathbb{R}), (2,\\mathbb{Z}), (2,\\mathbb{R}), (3,\\mathbb{Z}), (3,\\mathbb{R}) \\}\\)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>2.1 Set Theory</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html",
    "href": "lecture2/lecture2-2.html",
    "title": "2.2 Axiomatic Probability",
    "section": "",
    "text": "Methodology\nSteps to perform a probabilistic model:",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#methodology",
    "href": "lecture2/lecture2-2.html#methodology",
    "title": "2.2 Axiomatic Probability",
    "section": "",
    "text": "Specify sample space.\nDefine probability law (must align with probability axioms).\nIdentify event of interest.\nCalculate‚Ä¶",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#sample-space",
    "href": "lecture2/lecture2-2.html#sample-space",
    "title": "2.2 Axiomatic Probability",
    "section": "Sample Space",
    "text": "Sample Space\nA sample space is a set of all possible outcomes from an experiment. \n\\[\nSample \\ Space = \\{\\text{Heads}, \\text{Tails}\\}\n\\]\nAn experiment is any procedure that can be repeated and has a well-defined set of outcomes. \n\\[\n\\text{Flipping a fair coin}\n\\]\nAn outcome is the end result of an experiment, or an element in the sample space.\n\\[\n\\text{Heads}  \n\\]\n\n\n\n\n\n\nNoteIllustration: Discrete Sample Space\n\n\n\nExperiment: Rolling two fair die at the same time.\n\\[\nSample \\ Space = \\{ (x, y) : x,y \\in \\mathbb{N}, 1 \\leq x, y \\leq 6  \\}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNoteIllustration: Continuous Sample Space\n\n\n\nExperiment: Measure two continuous variables in the range \\([0,1]\\)\n\\[ Sample\\ Space = \\{ (x, y) : x,y \\in \\mathbb{R}, 0 \\leq x, y \\leq 1  \\} \\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#events-and-experiment",
    "href": "lecture2/lecture2-2.html#events-and-experiment",
    "title": "2.2 Axiomatic Probability",
    "section": "Events and Experiment",
    "text": "Events and Experiment\nAn event is a subset of the sample space.\nEvents are important because they ultimately get assigned probabilities.\nExperiment: Rolling a die once.\n\\[\nSample \\ Space = \\{1,2,3,4,5,6\\}\n\\]\nWhat is the event of rolling a \\(1\\)?\n\\[\n\\{1\\} \\subseteq \\{1,2,3,4,5,6\\}\n\\]\nWhat is the event of rolling an odd number?\n\\[\n\\{1, 3, 5\\} \\subseteq \\{1,2,3,4,5,6\\}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-2.html#probability-axioms-and-probability-law",
    "href": "lecture2/lecture2-2.html#probability-axioms-and-probability-law",
    "title": "2.2 Axiomatic Probability",
    "section": "Probability Axioms and Probability Law",
    "text": "Probability Axioms and Probability Law\nSuppose \\(A\\) and \\(B\\) are events.\nKolmogorov probability axioms are the foundations of axiomatic probability theory:\n\nNonnegativity: \\(P(Event) \\geq 0\\)\nNormalization: \\(P(Sample\\ Space) = 1\\)\nAdditivity: If \\(A \\cap B = \\emptyset, P(A \\cup B) = P(A) + P(B)\\)\n\nProbability laws are additional axioms mathematically derived from Kolmogorov probability axioms.\n\n\n\n\n\n\nNoteExercise\n\n\n\nExperiment: Rolling two fair die at the same time.\nLet all outcomes be equally likely.\n\\[\nP(A) = \\frac{|A|}{|Sample\\ Space|}\n\\]\n\n\n\nFind the following probabilities:\n\n\\(P(die_1 = 1)\\)\n\\(P(\\text{max}(die_1, die_2) = 2)\\)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\\(P(die_1 = 1) = \\frac{6}{36} \\approx 0.1\\bar{6}\\)\n\\(P(\\text{max}(die_1, die_2) = 2) = \\frac{2}{36} \\approx 0.0\\bar{5}\\)\n\n\n\n\n\n\n\n\n\n\nNoteExercise\n\n\n\nExperiment: Measure two continuous variables in the range \\([0,1]\\)\n\\[\nSample\\ Space = \\{ (x, y) : x,y \\in \\mathbb{R}, 0 \\leq x, y \\leq 1  \\}\n\\]\n\n\n\nFind the following probabilities:\n\n\\(P(x = 0.5 , y = 0.5)\\)\n\\(P(x+y \\geq 1)\\)\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\\(P(x = 0.5 , y = 0.5) = 0\\)\n\\(P(x+y \\geq 1) = 0.5\\)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>2.2 Axiomatic Probability</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html",
    "href": "lecture2/lecture2-3.html",
    "title": "2.3 Conditioning",
    "section": "",
    "text": "Conditional Probability\nSuppose \\(A\\) and \\(B\\) are events.\nConditional probability is the likelihood of an event occurring given that we know another event has occurred.\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nNotice that the conditioned event becomes the new sample space.\nIf \\(P(B) \\neq 0\\), then the probability of the intersection normalized by the conditioned space. Else \\(P(B) = 0\\), then \\(P(A|B)\\) is undefined.",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#product-rule",
    "href": "lecture2/lecture2-3.html#product-rule",
    "title": "2.3 Conditioning",
    "section": "Product Rule",
    "text": "Product Rule\nSuppose \\(A\\) and \\(B\\) are events.\nThe product rule states that the probability of two events \\(A\\) and \\(B\\) occurring together \\(A \\cap B\\) is given by the probability of one event occurring given the other \\(P(A|B)\\) or \\(P(B|A)\\) multiplied by the probability of the other event.\n\\[\nP(A \\cap B) = P(A|B) P(B) = P(B|A) P(A)\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-3.html#total-probability-theorem",
    "href": "lecture2/lecture2-3.html#total-probability-theorem",
    "title": "2.3 Conditioning",
    "section": "Total Probability Theorem",
    "text": "Total Probability Theorem\nSuppose \\(A_{1,...,n}\\) and \\(B\\) are events.\nThe total probability theorem allows us to compute the likelihood of an event by summing over conditional probabilities of different partitions of the sample space.\n\\[\nP(B) = P(B|A_1) P(A_1) + P(B|A_2) P(A_2) + P(B|A_3) P(A_3)\n\\]\n\n\n\nIn general terms:\n\\[\nP(B) = P(B|A_1) P(A_1) + ... + P(B|A_n) P(A_n)\n\\]\n\n\n\n\n\n\nNoteExercise\n\n\n\nExperiment: Classifying emails as spam \\(S\\) or not spam \\(NS\\) based on the word \\(W\\) or not the word \\(NW\\) ‚Äúfree‚Äù.\n\\[\nP(S) = 0.4\n\\]\n\\[\nP(NS) = 0.6\n\\]\n\\[\nP(W|S) = 0.7\n\\]\n\\[\nP(W|NS) = 0.1\n\\]\nFind the probability of \\(P(S|W)\\):\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\[\nP(S|W) = \\frac{P(S \\cap W)}{P(W)} = \\underbrace{\\frac{P(W|S)P(S)}{P(W|S)P(S) + P(W|NS)P(NS)}}_{Bayes' \\ Theorem} = \\frac{0.7 * 0.4}{0.7 * 0.4 + 0.1 * 0.6} = 0.82\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>2.3 Conditioning</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html",
    "href": "lecture2/lecture2-4.html",
    "title": "2.4 Independence",
    "section": "",
    "text": "Independent Events\nSuppose \\(A\\) and \\(B\\) are two events.\nTwo events are independent if the occurrence of event \\(B\\) provides no information about the occurrence of event \\(A\\):\n\\[\nP(A|B) = P(A)\n\\]\nAnother definition of independence:\n\\[\nP(A \\cap B) = P(A) P(B)\n\\]\nFor multiple events:\n\\[\nP(A_{1} \\cap A_{2} \\cap ... A_{n}) = P(A_{1}) P(A_{2}) ... P(A_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-4.html#conditioning-and-independence",
    "href": "lecture2/lecture2-4.html#conditioning-and-independence",
    "title": "2.4 Independence",
    "section": "Conditioning and Independence",
    "text": "Conditioning and Independence\nSuppose \\(A\\), \\(B\\), and \\(C\\) are events.\nIf \\(A\\) and \\(B\\) are independent, conditioning on \\(C\\) may remove that independence.\nWhen we condition on \\(C\\), events \\(A\\) and \\(B\\) may no longer be independent.\n\n\n\n\n\n\n\n\n\nNoteExercise\n\n\n\nThe king comes from a family of two children. What is the probability that his sibling is female \\(F\\) and not male \\(M\\)?\nLet all outcomes be equally likely.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\[\nSample \\ Space = \\{(FF), (FM), (MF), (MM)\\} = \\{(\\not F \\not F), (FM), (MF), (MM)\\}\n\\]\n\\[\nP(F|M) = \\frac{P(F \\cap M)}{M} = \\frac{2}{3} \\approx 0.6\\bar{6}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>2.4 Independence</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html",
    "href": "lecture2/lecture2-5.html",
    "title": "2.5 Discrete Random Variables",
    "section": "",
    "text": "Discrete Random Variable\nA discrete random variable is a mapping \\(X\\) of all of the outcomes of a sample space to numerical values \\(x \\in \\mathbb{Z}\\).\n\\[\nX: Outcome \\in Sample \\ Space \\to x \\in \\mathbb{Z}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#probability-mass-function-pmf",
    "href": "lecture2/lecture2-5.html#probability-mass-function-pmf",
    "title": "2.5 Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nA probability mass function (PMF) is a mapping of values \\(x\\) of discrete random variables \\(X\\) to probabilities \\([0,1]\\).\n\\[\np_{X}(x) = P(X = x)\n\\]\nPMF has properties: \\[\np_{X}(x) \\geq 0, \\quad \\sum_{x} p_{X}(x) = 1\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#cumulative-distribution-function-cdf",
    "href": "lecture2/lecture2-5.html#cumulative-distribution-function-cdf",
    "title": "2.5 Discrete Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for discrete random variables is defined as:\n\\[\nF_{X}(x) = P(X \\leq x) = \\sum_{k \\leq x} p_{X}(k)\n\\]\nRelation to PMF: \\[\np_{X}(x) = \\frac{dF_{X}(x)}{dx}\n\\]\n\\[\nF_{X}(x) = \\sum_{k \\leq x}p_{X}(k)\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#illustration-pmf-and-cdf",
    "href": "lecture2/lecture2-5.html#illustration-pmf-and-cdf",
    "title": "2.5 Discrete Random Variables",
    "section": "Illustration: PMF and CDF",
    "text": "Illustration: PMF and CDF",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#expectation-and-variance-of-pmfs",
    "href": "lecture2/lecture2-5.html#expectation-and-variance-of-pmfs",
    "title": "2.5 Discrete Random Variables",
    "section": "Expectation and Variance of PMFs",
    "text": "Expectation and Variance of PMFs\nDiscrete expectation, or mean, is the average numerical value that the discrete random variable takes over the PMF. \\[\nE[X] = \\sum_{x} x \\ p_{X}(x)\n\\]\nDiscrete variance is the expected squared difference from the mean of a PMF. \\[\nVar[X] = E[(X - E[X])^{2}] = \\sum_{x}(x - E[X])^2 p_{X}(x)\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#joint-and-marginal-pmfs",
    "href": "lecture2/lecture2-5.html#joint-and-marginal-pmfs",
    "title": "2.5 Discrete Random Variables",
    "section": "Joint and Marginal PMFs",
    "text": "Joint and Marginal PMFs\nThe joint PMF calculates the intersection of two discrete random variables: \\[\np_{X,Y}(x_{1},x_{2}) = P(X = x_{1}, Y = x_{2})\n\\]\nFrom the previous definition we can also compute the marginal PMF for a particular random variable: \\[\np_{X}(x_{1}) = \\sum_{x_{2}} p_{X,Y}(X = x_{1},Y = x_{2})\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#conditional-expectation-of-pmfs",
    "href": "lecture2/lecture2-5.html#conditional-expectation-of-pmfs",
    "title": "2.5 Discrete Random Variables",
    "section": "Conditional Expectation of PMFs",
    "text": "Conditional Expectation of PMFs\nThe conditional PMF gives the probability distribution of a discrete random variable given that another variable has a specific value. \\[\np_{X|Y}(x_{1}| x_{2}) = \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]\nThe conditional expectation is the expected value of a discrete random variable given that another variable is fixed at a specific value. \\[\nE[X| Y = x_{2}] = \\sum_{x_{1}} x_{1} \\frac{p_{X,Y}(x_{1}, x_{2})}{p_{Y}(x_{2})}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-5.html#independence",
    "href": "lecture2/lecture2-5.html#independence",
    "title": "2.5 Discrete Random Variables",
    "section": "Independence",
    "text": "Independence\nTwo discrete random variables are independent if:\n\\[\np_{X,Y}(x_{1}, x_{2}) = p_{X}(x_{1}) p_{Y}(x_{2})\n\\]\nFor multiple discrete random variables: \\[\np_{X_{1},...,X_{n}}(x_{1}, ..., x_{n}) = p_{X_{1}}(x_{1}) ... \\ p_{X_{n}}(x_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>2.5 Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html",
    "href": "lecture2/lecture2-6.html",
    "title": "2.6 Continuous Random Variables",
    "section": "",
    "text": "Continuous Random Variable\nA continuous random variable is a mapping \\(X\\) of all the events of a sample space to numerical values \\(x \\in \\mathbb{R}\\). \\[\nX: Event \\in Sample \\ Space \\to x \\in \\mathbb{R}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#probability-density-function-pdf",
    "href": "lecture2/lecture2-6.html#probability-density-function-pdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nA probability density function (PDF) is a mapping of values \\(x\\) of intervals of continuous random variables \\(X\\) to probabilities \\([0,1]\\).\n\\[\nP(a \\leq X \\leq b) = \\int_{a}^{b} f_{X}(x) \\ dx\n\\]\nPDF has properties: \\[\nf_{X}(x) \\geq 0, \\ \\int_{-\\infty}^{\\infty} f_{X}(x) \\ dx = 1\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#cumulative-distribution-function-cdf",
    "href": "lecture2/lecture2-6.html#cumulative-distribution-function-cdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) for continuous random variables is defined as:\n\\[ F_{X}(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_{X}(u) \\ du \\]\nRelation to PDF:\n\\[\nf_{X}(x) = \\frac{dF_{X}(x)}{dx}\n\\] \\[\nF_{X}(x) = \\int_{-\\infty}^{x} f_{X}(u) \\ du\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#illustration-pdf-and-cdf",
    "href": "lecture2/lecture2-6.html#illustration-pdf-and-cdf",
    "title": "2.6 Continuous Random Variables",
    "section": "Illustration: PDF and CDF",
    "text": "Illustration: PDF and CDF",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#expectation-and-variance-of-pdfs",
    "href": "lecture2/lecture2-6.html#expectation-and-variance-of-pdfs",
    "title": "2.6 Continuous Random Variables",
    "section": "Expectation and Variance of PDFs",
    "text": "Expectation and Variance of PDFs\nContinuous expectation, or mean, is the average numerical value that the continuous random variable takes over the PDF. \\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\ f_{X}(x) dx\n\\]\nContinuous variance is the expected squared difference from the mean of a PDF. \\[\n\\text{Var}[X] = E[(X - E[X])^{2}] = \\int_{-\\infty}^{\\infty} (x - E[X])^2 \\ f_{X}(x) dx\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#joint-and-marginal-pdfs",
    "href": "lecture2/lecture2-6.html#joint-and-marginal-pdfs",
    "title": "2.6 Continuous Random Variables",
    "section": "Joint and Marginal PDFs",
    "text": "Joint and Marginal PDFs\nThe joint PDF calculates the intersection of two continuous random variables: \\[\nP((X,Y) \\in A) = \\int \\int_{A} f_{X,Y}(x_{1}, x_{2}) dx_{1} \\ dx_{2}\n\\]\nFrom the previous definition we can also compute the marginal PDF for a particular random variable:\n\\[\nf_{X}(x_{1}) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x_{1}, x_{2}) dx_{2}\n\\]\n\\[\nf_{Y}(x_{2}) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x_{1},x_{2}) dx_{1}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#conditional-expectation-of-pdfs",
    "href": "lecture2/lecture2-6.html#conditional-expectation-of-pdfs",
    "title": "2.6 Continuous Random Variables",
    "section": "Conditional Expectation of PDFs",
    "text": "Conditional Expectation of PDFs\nThe conditional PDF gives the probability distribution of a continuous random variable given that another variable has a specific value. \\[\nf_{X|Y}(x_{1}| x_{2}) = \\frac{f_{X,Y}(x_{1}, x_{2})}{f_{Y}(x_{2})}\n\\]\nThe conditional expectation is the expected value of a continuous random variable given that another variable is fixed at a specific value. \\[\n\\mathbb{E}[X| Y = x_{2}] = \\int_{x_{1} \\in X} x_{1} \\frac{f_{X,Y}(x_{1}, x_{2})}{f_{Y}(x_{2})} dx_{1}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-6.html#independence",
    "href": "lecture2/lecture2-6.html#independence",
    "title": "2.6 Continuous Random Variables",
    "section": "Independence",
    "text": "Independence\nTwo continuous random variables are independent if: \\[\nf_{X,Y}(x_{1}, x_{2}) = f_{X}(x_{1}) f_{Y}(x_{2})\n\\]\nThis can be extended to multiple random variables: \\[\nf_{X_{1},...,X{n}}(x_{1}, ..., x_{n}) = f_{X_{1}}(x_{1}) ... \\ f_{X_{n}}(x_{n})\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>2.6 Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html",
    "href": "lecture2/lecture2-7.html",
    "title": "2.7 Probability Distributions",
    "section": "",
    "text": "Bernoulli Distribution\n\\[\np_{X}(x) =\n\\begin{cases}\n    p & \\text{if } x = 1, \\\\\n    q = 1-p & \\text{if } x = 0.\n\\end{cases}\n\\]",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#bernoulli-distribution",
    "href": "lecture2/lecture2-7.html#bernoulli-distribution",
    "title": "2.7 Probability Distributions",
    "section": "",
    "text": "The discrete random variable can take value \\(1\\) with probability \\(p\\) or value \\(0\\) with probability \\(q = 1 - p\\)\nNot to be confused with the binomial distribution, since only one trial is being conducted.\n\\(\\mathbb{E}[X] = p\\)\n\\(Var[X] = pq\\) \n\n\n\nviewof p = Inputs.range([0, 1], {\n  step: 0.01,\n  value: 0.5,\n  label: tex`p`,\n  width: 200\n})\n\ndata = {\n  return [\n    {outcome: \"0\", probability: 1 - p},\n    {outcome: \"1\", probability: p}\n  ];\n}\n\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {\n    grid: true,\n    label: \"Probability\",\n    domain: [0, 1]\n  },\n  x: {\n    label: \"Outcome\",\n    padding: 0.2\n  },\n  marks: [\n    Plot.barY(data, {\n      x: \"outcome\",\n      y: \"probability\",\n      fill: \"steelblue\"\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  &lt;p&gt;${tex`\\mathbb{E}[X] =`} ${p.toFixed(3)}&lt;/p&gt;\n  &lt;p&gt;${tex`\\text{Var}(X) =`} ${(p * (1-p)).toFixed(3)}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nPython Code: Bernoulli Distribution\nTo create Bernoulli distributed data using numpy:\nimport numpy as np\n\ninterval = [0,1]\nsize = (1000,1)\np = [1-0.5, 0.5]\n\ndata = np.random.choice(interval, size, p = p)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#gaussian-distribution",
    "href": "lecture2/lecture2-7.html#gaussian-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Gaussian Distribution",
    "text": "Gaussian Distribution\n\\[\nf_{X}(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2} }}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\n\\]\n\nUsed frequently to represent real-valued random variables whose distributions are not known.\nIts importance is derived from the central limit theorem that states, under some conditions, the average of many samples of a random variable is itself a random variable that converges to a Gaussian distribution as it increases.\n\\(E[X] = \\mu\\)\n\\(Var[X] = \\sigma^{2}\\)\n\n\n\nviewof mu = Inputs.range([-1, 1], {\n  step: 0.1,\n  value: 0,\n  label: tex`\\mu`,\n  width: 200\n})\n\nviewof sigma = Inputs.range([0.1, 2], {\n  step: 0.1,\n  value: 1,\n  label: tex`\\sigma`,\n  width: 200\n})\n\n// Generate points for the normal distribution curve\npointsGaussian = {\n  const x = d3.range(-5, 5, 0.1);\n  return x.map(x =&gt; ({\n    x,\n    y: (1 / (sigma * Math.sqrt(2 * Math.PI))) * \n       Math.exp(-0.5 * Math.pow((x - mu) / sigma, 2))\n  }));\n}\n\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {\n    grid: true,\n    label: \"Density\"\n  },\n  x: {\n    label: \"x\",\n    domain: [-5, 5]\n  },\n  marks: [\n    Plot.line(pointsGaussian, {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  &lt;p&gt;${tex`\\mathbb{E}[X] =`} ${mu.toFixed(3)}&lt;/p&gt;\n  &lt;p&gt;${tex`\\text{Var}(X) =`} ${(sigma * sigma).toFixed(3)}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nPython Code: Gaussian Distribution\nTo create Gaussian distributed data using numpy:\nimport numpy as np\n\nmu = 0\nsigma = 1\nsize = (1000,1)\n\ndata = np.random.normal(mu, sigma, size)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture2/lecture2-7.html#beta-distribution",
    "href": "lecture2/lecture2-7.html#beta-distribution",
    "title": "2.7 Probability Distributions",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\\[\nf_{X}(x) = {\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}\\,x^{\\alpha -1}(1-x)^{\\beta -1}\n\\]\nwhere \\(\\Gamma\\) is the gamma function defined as:\n\\[\n\\Gamma (z)=\\int _{0}^{\\infty}t^{z-1}e^{-t}\\,dt\n\\]\n\nGamma functions are used to model factorial functions of complex numbers \\(z\\).\nBeta functions are used to model behavior of random variables in intervals of finite length.\n\\(E[X] = \\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(Var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\n\n\n\nviewof alpha = Inputs.range([0.1, 10], {\n  step: 0.1,\n  value: 1,\n  label: tex`\\alpha`,\n  width: 200\n})\n\nviewof beta = Inputs.range([0.1, 10], {\n  step: 0.1,\n  value: 1,\n  label: tex`\\beta`,\n  width: 200\n})\n\n// Gamma function approximation using Lanczos approximation\nfunction gamma(z) {\n    const p = [676.5203681218851, -1259.1392167224028, 771.32342877765313,\n        -176.61502916214059, 12.507343278686905, -0.13857109526572012,\n        9.9843695780195716e-6, 1.5056327351493116e-7];\n    \n    if (z &lt; 0.5) {\n        return Math.PI / (Math.sin(Math.PI * z) * gamma(1 - z));\n    }\n    \n    z -= 1;\n    let x = 0.99999999999980993;\n    for (let i = 0; i &lt; p.length; i++) {\n        x += p[i] / (z + i + 1);\n    }\n    \n    const t = z + p.length - 0.5;\n    return Math.sqrt(2 * Math.PI) * Math.pow(t, z + 0.5) * Math.exp(-t) * x;\n}\n\n// Beta function using gamma function\nfunction betaFunc(x, y) {\n    return (gamma(x) * gamma(y)) / gamma(x + y);\n}\n\n// Beta probability density function\nfunction betaPDF(x, a, b) {\n    if (x &lt;= 0 || x &gt;= 1) return 0;\n    return Math.pow(x, a - 1) * Math.pow(1 - x, b - 1) / betaFunc(a, b);\n}\n\n// Generate points for the beta distribution curve\npoints = Array.from({length: 100}, (_, i) =&gt; {\n  let x = 0.001 + i * 0.01;\n  return { x, y: betaPDF(x, alpha, beta) };\n});\n\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {\n    grid: true,\n    label: \"Density\"\n  },\n  x: {\n    label: \"x\",\n    domain: [0, 1]\n  },\n  marks: [\n    Plot.line(points, {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  &lt;p&gt;${tex`\\mathbb{E}[X] =`} ${(alpha/(alpha + beta)).toFixed(3)}&lt;/p&gt;\n  &lt;p&gt;${tex`\\text{Var}(X) =`} ${((alpha * beta)/((alpha + beta)**2 * (alpha + beta + 1))).toFixed(3)}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\nPython Code: Beta Distribution\nTo create Beta distributed data using numpy:\nimport numpy as np \n\nalpha = 0.5\nbeta = 0.5\nsize = (1000,1)\n\ndata = np.random.beta(alpha, beta, size)",
    "crumbs": [
      "Lecture 2: Math Foundations",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>2.7 Probability Distributions</span>"
    ]
  },
  {
    "objectID": "lecture3/lo.html",
    "href": "lecture3/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 3: Multi-Armed Bandits (Sutton and Barto 2018) üéØ\n\n\n\n\nMulti-Amed Bandits Framework.\n\\(\\epsilon\\)-greedy.\nUpper Confidence Boundary (UCB).\nThompson Sampling.\nBernoulli & Gaussian generated environment using numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html",
    "href": "lecture3/lecture3-1.html",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "",
    "text": "Bandit\nWe saw from Lecture 1 that Reinforcement Learning is about sequential decision-making:\nA bandit is a slot machine.\nIt is used as an analogy to represent the action an agent can make in one state.\nEach action selection is like a play of one of the slot machine‚Äôs levers, and the rewards are the payoffs for hitting the jackpot, according to its underlying probability distribution.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#bandit",
    "href": "lecture3/lecture3-1.html#bandit",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "",
    "text": "\\(\\Huge{\\to}\\)\nRewards",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#multi-armed-bandit",
    "href": "lecture3/lecture3-1.html#multi-armed-bandit",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Multi-Armed Bandit",
    "text": "Multi-Armed Bandit\nA Multi-Armed Bandit can be interpreted as k-actions, or k-arms of the slot machines, to decide from.\nThrough repeated action selections, you maximize your winnings by concentrating actions on the best levers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nHow do we decide the most appropriate action?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nWe calculate the expectation of a Bandit!\nEach bandit has an expected reward given a particular action is selected, called the action value.\n\\[\nQ_t(a) = \\mathbb{E}[R_t | A_t = a]\n\\]\n\nNote that \\(Q_t(a)\\) tells us how good an action is at time step \\(t\\) for a particular action \\(a\\). This is fundamental concept to Value-based Reinforcement Learning algorithms.\n\n\n\n\n\\(Q_t(a)\\) is the conditional expectation of the rewards \\(R_t\\) given the selection of an action \\(A_t\\). \\(R_t\\) is the random variable for the reward at time step \\(t\\). \\(A_t\\) is the random variable for the action selected at time step \\(t\\).",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#action-value-predicate-method",
    "href": "lecture3/lecture3-1.html#action-value-predicate-method",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Action Value: Predicate Method",
    "text": "Action Value: Predicate Method\nOne way to compute action values is using a predicate:\n\\[\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i * \\mathbf{1}_{A_i = a}}{\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a}}\n\\]\n\n\n\\(Q_t(a)\\) is the action value for a particular action \\(a\\). \\(\\mathbf{1}\\) is a predicate, which denotes whether \\(A_i = a\\) is true or false.\nIf the denominator is \\(0\\), then we denote \\(Q_t(a)\\) as \\(0\\).",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-1.html#action-value-incremental-update",
    "href": "lecture3/lecture3-1.html#action-value-incremental-update",
    "title": "3.1 Multi-Armed Bandit Framework",
    "section": "Action Value: Incremental Update",
    "text": "Action Value: Incremental Update\nTo avoid computationally expensive updates using the predicate method, we can update action values in an incremental fashion:\n\\[\n\\underbrace{Q_{t+1}}_{\\text{New Estimate}} = \\underbrace{Q_t}_{\\text{Old Estimate}} + \\underbrace{\\frac{1}{t}}_{\\text{Step Size}} (\\underbrace{R_t}_{\\text{Target}} - \\underbrace{Q_t}_{\\text{Old Estimate}})\n\\]\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nShould we always pick actions with the highest expected value?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nNo, always picking actions with the highest expected value will disallow us to explore other actions!\nLet‚Äôs see how we can solve this problem in the next page üòä",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>3.1 Multi-Armed Bandit Framework</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-2.html",
    "href": "lecture3/lecture3-2.html",
    "title": "3.2 Œµ-Greedy",
    "section": "",
    "text": "Can you stay curious without straying too far from what works? üéØ\n\n\n\nWe are exploring when we randomly select an action.\nWe are exploiting, or acting greedy, when an action is selected based on its expected value.\n\n\n\n\n\n\nWarningProblem\n\n\n\nExploring all of the time does not permit you to exploit your knowledge of expected values.\nExploiting all of the time does not permit you to explore all of the options.\nHow can we select actions with the highest expected value while leveraging exploration?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nSuppose you are in a Multi-Armed Bandit scenario:\n\n\\(S\\) ‚Äì You are hungry and want to treat yourself to a restaurant meal.\n\n\\(A_{1,\\dots,k}\\) ‚Äì You can choose from \\(k\\) different restaurants in your area.\n\n\\(R\\) ‚Äì After eating, you rate your experience ‚Äî maybe based on taste, service, or price satisfaction.\n\n\n\n\n\n\nAction \\(A_1\\): Choosing Arepas. Reward \\(R\\): Based on past visits, this option seems to give the highest satisfaction ‚Äî taste, service, and value are consistently strong.\n\n\n\n\n\n\n\nAction \\(A_2\\): Choosing Chipotle. Reward \\(R\\): A familiar option with decent satisfaction, though not as rewarding as Arepas on average.\n\n\n\n\n\n\n\nAction \\(A_3\\): Trying Falafel Bowls. Reward \\(R\\): An unexplored option ‚Äî the reward is uncertain until you try it.\n\n\n\n\n\nHow can you select a restaurant meal that maximizes your enjoyment while still being open to exploring something new like Falafel Bowls?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nEpsilon Greedy is an algorithm that allows us to balance our decision-making in this simple manner.\nEpsilon (\\(\\epsilon\\)) is a fixed proportion that decides whether we explore or exploit our actions.\n\\[\n  A_t \\gets\n  \\begin{cases}\n      \\text{a random action with probability } \\epsilon \\\\\n      \\arg\\max_a Q(a) \\text{ with probability } 1 - \\epsilon\n  \\end{cases}\n\\]\nIn our real life example, at first, you try different restaurants (exploration) to see what‚Äôs good. Over time, you start favoring the ones with better rewards (exploitation). But occasionally, you still try a new one ‚Äî just in case it‚Äôs better than your current favorite.\n\n\nPseudocode",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>3.2 &epsilon;-Greedy</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-3.html",
    "href": "lecture3/lecture3-3.html",
    "title": "3.3 Upper Confidence Boundary (UCB)",
    "section": "",
    "text": "What if you trusted things that haven‚Äôt let you down ‚Äî but gave others a fair shot too? üîç\n\n\n\nOne risk that we incur with \\(\\epsilon\\)-greedy is that it randomly explores ‚Äî which might lead to suboptimal or redundant choices.\n\n\n\n\n\n\nWarningProblem\n\n\n\nIs there a way that we can explore more intelligently?\nHint: Think about Lecture 2\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nSuppose you are in a Multi-Armed Bandit scenario:\n\n\\(S\\) ‚Äì It‚Äôs a beautiful day, and you‚Äôre deciding where to go for a run.\n\n\\(A_{1,\\dots,k}\\) ‚Äì You can choose from \\(k\\) different running trails nearby.\n\n\\(R\\) ‚Äì After each run, you mentally rate the experience ‚Äî scenery, terrain, or how energized you felt.\n\n\n\n\n\n\nAction \\(A_1\\): Running Trail A. Reward \\(R\\): This loop has consistently provided the highest enjoyment ‚Äî strong scenery and terrain make it the best-known choice.\n\n\n\n\n\n\n\nAction \\(A_2\\): Running Trail B. Reward \\(R\\): A modest performer so far, but not explored much. With more runs, it could reveal higher potential than currently estimated.\n\n\n\n\n\n\n\nAction \\(A_3\\): Running Trail C. Reward \\(R\\): A completely new option ‚Äî its value is entirely uncertain, and initial selections may not be the most informative.\n\n\n\n\n\nShould you keep exploiting Trail A‚Äôs high reward, or sometimes choose Trail B ‚Äî which has modest results but higher potential upside ‚Äî instead of immediately exploring the completely unknown Trail C?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUpper Confidence Boundaries (UCB) allow us to select among the non-greedy actions according to their potential for actually being optimal.\n\\[\nA_t \\gets \\arg\\max_a \\left[ Q(a) + \\sqrt{\\frac{2 \\ln(t)}{N(a)}} \\right]\n\\]\n\n\n\nBalancing Exploration and Exploitation\nEach time \\(a\\) is selected, the uncertainty is presumably reduced \\(N(a)\\) increments, and as it appears in the denominator, the uncertainty term decreases.\n\\[\nVAR \\downarrow = \\sqrt{\\frac{2 \\ln(t)}{N(a)\\uparrow}}\n\\]\nEach time an action other than \\(a\\) is selected, \\(t\\) increases but \\(N(a)\\) does not; because \\(t\\) appears in the numerator, the uncertainty estimate increases.\n\\[\nVAR \\uparrow = \\sqrt{\\frac{2 \\ln(t) \\uparrow}{N(a)}}\n\\]\nIn our Real Life Example UCB says ‚ÄúDon‚Äôt just pick randomly. Prefer trails that are either great or still uncertain ‚Äî they might surprise you.‚Äù\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sqrt{\\frac{2 \\ln(t)}{N(a)}}\\) is the measure of variance of the action \\(a\\). The natural logarithm increases get smaller over time but are unbounded, so all actions will be selected.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>3.3 Upper Confidence Boundary (UCB)</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-4.html",
    "href": "lecture3/lecture3-4.html",
    "title": "3.4 Thompson Sampling",
    "section": "",
    "text": "What if you made decisions by imagining how the world might be ‚Äî over and over? üß†\n\n\n\nWe want to explore options proportionally to how likely they are to be optimal, not just randomly or based on confidence bounds.\n\n\n\n\n\n\nWarningProblem\n\n\n\nIs there a way that we can select actions based out of a distribution of our beliefs?\nHint: Think about Lecture 2\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nSuppose you are in a Multi-Armed Bandit scenario:\n\n\\(S\\) ‚Äì It‚Äôs Friday night, and you‚Äôre deciding what show or YouTube channel to watch.\n\n\\(A_{1,\\dots,k}\\) ‚Äì You can choose from \\(k\\) different shows or creators.\n\n\\(R\\) ‚Äì After each episode or video, you mentally rate how entertaining or satisfying it was.\n\n\n\n\n\n\nAction \\(A_1\\): Watching Seinfeld. Reward \\(R\\): Your comfort show ‚Äî consistently delivers strong enjoyment, so its reward distribution is centered high.\n\n\n\n\n\n\n\nAction \\(A_2\\): Watching The Office. Reward \\(R\\): Only a few episodes watched ‚Äî the distribution is more uncertain, leaving room for it to surprise you.\n\n\n\n\n\n\n\nAction \\(A_3\\): Watching House. Reward \\(R\\): A completely new show ‚Äî no data yet, so its distribution is wide and uncertain.\n\n\n\n\nHow can you select a show using your prior beliefs, while still leaving room for updates as new evidence comes in?\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThompson sampling is an algorithm that leverages the beta distribution for its action value method and action selections.\nInitialize beta distributed with parameters:\n\\[\n\\alpha(a) = (\\alpha_{1}, . . . , \\alpha_{k}) = 1\n\\] \\[\n\\beta(a) = (\\beta_{1}, . . . , \\beta_{k}) = 1\n\\]\nNow for each action \\(a\\), the prior probability density function of our action value method \\(Q(a)\\) is:\n\\[\n  Q(a) = \\frac{\\Gamma(\\alpha(a) + \\beta(a))}{\\Gamma(\\alpha(a)) \\ \\Gamma(\\beta(a))} a^{\\alpha(a)-1} (1 - a)^{\\beta(a)-1}\n\\]\n\n\nBalancing Exploration and Exploitation\nThe agent updates its prior belief using the following action value method:\n\\[\n\\alpha(A_{t}) \\gets \\alpha(A_{t}) + R_{t}\n\\] \\[\n\\beta(A_{t}) \\gets \\beta(A_{t}) + 1 - R_{t}\n\\]\nNotice that for those actions selected \\(A_t\\), we increase its corresponding \\(\\alpha\\) parameter (\\(R_t = 1\\)) and maintain its \\(\\beta\\) parameter the same as before (\\(1 - R_t = 1 - 1 = 0\\)).\nThis update allows the algorithm to draw accurate samples and strike a balance between exploring and exploiting.\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>3.4 Thompson Sampling</span>"
    ]
  },
  {
    "objectID": "lecture3/lecture3-5.html",
    "href": "lecture3/lecture3-5.html",
    "title": "3.5 (Optional) Contextual Multi-Armed Bandits (CMAB)",
    "section": "",
    "text": "What if the best choice depended not just on luck ‚Äî but on the situation you‚Äôre in? üé≠\n\n\n\n\n\n\n\n\n\nNoteLinUCB\n\n\n\nLinUCB is a contextual multi-armed bandit algorithm where the expected reward of arm \\(a\\) at time \\(t\\) is modeled as a linear function of its feature vector \\(\\mathbf{x}_{t,a} \\in \\mathbb{R}^d\\).\n\n  Yahoo! 2010 (Li et al. 2010)  Link to Research Paper\n\n\n\n\n\n\n\n\n\n\nLi, Lihong, Wei Chu, John Langford, and Robert E. Schapire. 2010. ‚ÄúA Contextual-Bandit Approach to Personalized News Article Recommendation.‚Äù In Proceedings of the 19th International Conference on World Wide Web, 661‚Äì70. ACM.",
    "crumbs": [
      "Lecture 3: Multi-Armed Bandits",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>3.5 (Optional) Contextual Multi-Armed Bandits (CMAB)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lo.html",
    "href": "private/lecture4/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 4: Dynamic Programming (Sutton and Barto 2018) üéØ\n\n\n\n\nMarkov Chain.\nMarkov Decision Process (MDPs).\nDynamic Programming.\nHomemade GridWorld OpenAI environment using gymnasium, pygame & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-1.html",
    "href": "private/lecture4/lecture4-1.html",
    "title": "4.1 Markov Chain",
    "section": "",
    "text": "Markov Chain\nWe saw from Lecture 3 how to make optimal decisions in non-associative environments:\nA Markov Chain is a model for transitions that are not controlled between fully observable states.\nA State is a node.\nA State Transition is one outward-going arrow.\nIn this framework, we are interested in how state probabilities evolve over time and the corresponding values of each state.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-1.html#markov-chain",
    "href": "private/lecture4/lecture4-1.html#markov-chain",
    "title": "4.1 Markov Chain",
    "section": "",
    "text": "State transitions are conditional probabilities of going to the next state given the current state.\n\n\n\n\n\nProbability Matrix\nSuppose a frog jumps from one lily pad to another with state transition probabilities:\n\n\n\n\\[\n\\mathbf{P} = \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.8 & 0.4 \\end{bmatrix}\n\\]\n\n\nRewards Matrix\nSuppose the frog has associated rewards:\n\n\n\n\\[\n\\mathbf{R} = \\begin{bmatrix} 6 & 1 \\\\ 1 & -2 \\end{bmatrix}\n\\]\n\n\nValue Function\nWe want to calculate the expected value of moving from state \\(i\\) to state \\(j\\) for all situations \\(s \\in \\{1,2,...,S\\}\\):\n\\[\n\\mathbf{v}(t) = \\mathbf{q} + \\mathbf{v}(t-1) \\mathbf{P}\n\\]\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\\[\n\\begin{align*}\nv_{j}(t) & = \\sum_{i=1}^{S} p_{i,j} \\ [r_{i,j}+v_{i}(t-1)] \\\\\n& = \\sum_{i=1}^{S} p_{i,j} \\ r_{i,j} + \\sum_{i=1}^{S} p_{i,j} \\ v_{i}(t-1) \\\\\n& = \\textbf{q} + \\sum_{i=1}^{S} p_{i,j}\\ v_{i}(t-1)\n\\end{align*}\n\\]\n\n\n\nFirst, we need to calculate \\(\\textbf{q}\\), the expected reward in the next transition out of state \\(i\\):\n\\[\n\\textbf{q} =\n\\begin{bmatrix}\n    2 & -0.2\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\\[\n\\mathbf{q} = \\sum_{i=1}^{S} p_{i,j} r_{i,j}\n\\]\n\\[\nq_{1} = p_{1,1} \\ r_{1,1} + r_{2,1} \\ p_{2,1}\n\\]\n\\[\nq_{2} = p_{1,2} \\ r_{1,2} + r_{2,2} \\ p_{2,2}\n\\]\n\n\n\n\\[\n\\begin{bmatrix} v_{1}(t) \\ v_{2}(t) \\end{bmatrix} = \\begin{bmatrix} 2 \\ -0.2 \\end{bmatrix} + \\begin{bmatrix} v_{1}(t-1) \\ v_{2}(t-1) \\end{bmatrix} \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.8 & 0.4 \\end{bmatrix}\n\\]\nAt \\(t=100\\): \\[\n\\mathbf{v}(100) = \\begin{bmatrix} 77.88 & 76.59 \\end{bmatrix}\n\\]\nIn other words, the frogs expected value at \\(t = 100\\) is that lilly pad \\(1\\) will be greater (with \\(77.88\\) expected flies) than that of lilly pad \\(2\\) (with \\(76.59\\) expected flies)\n\n\n\n\n\nDiscounting Factor\nThe \\(\\gamma\\) allows us to place a higher value on the present rewards, rather than future uncertain rewards.\n\\[\n\\mathbf{v}(t) = \\mathbf{q} + \\gamma \\mathbf{v}(t-1) \\mathbf{P}\n\\]\n\\[\n\\begin{bmatrix} v_{1}(t) \\ v_{2}(t) \\end{bmatrix} = \\begin{bmatrix} 2 \\ -0.2 \\end{bmatrix} + \\gamma \\begin{bmatrix} v_{1}(t-1) \\ v_{2}(t-1) \\end{bmatrix} \\begin{bmatrix} 0.2 & 0.6 \\\\ 0.8 & 0.4 \\end{bmatrix}\n\\]\nAt \\(\\gamma=0.9\\) and \\(t=100\\): \\[\n\\mathbf{v}(100) = \\begin{bmatrix} 8.47 & 7.15 \\end{bmatrix}\n\\]\n\n\n\n\n\nPython Code\nimport numpy as np\n\nGAMMA = 0.9\nP = np.array([[0.2, 0.6], [0.8, 0.4]])\nR = np.array([[6, 1], [1, -2]])\nq = np.sum(P * R, axis=1)\nv_initial = np.array([0, 0])\n\ndef value_function(v, P, q, t=100):\n    for _ in range(t):\n        v = q + GAMMA * np.dot(v, P)\n    return v\n\nv_result = value_function(v_initial, P, q)\nprint(v_result)",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>4.1 Markov Chain</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html",
    "href": "private/lecture4/lecture4-2.html",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "",
    "text": "Markov Decision Process\nMarkov Chain‚Äôs are a framework for an associative environment.\nA Markov Chain is a model for transitions that are not controlled between fully observable states.\nA Markov Decision Process (MDP) is a model for transitions that are controlled between fully observable states.\nThe Agent is the learner and decision-maker.\nThe Environment is everything external to the agent.\nFrom an Initial State, the agent interacts with the environment through its Actions.\nThese actions continuously give rise to different States and Rewards.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#markov-decision-process",
    "href": "private/lecture4/lecture4-2.html#markov-decision-process",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "",
    "text": "Markov Decision Process\n\n\n\n\n\n\n\n\n\nNoteExample: GridWorld\n\n\n\n\n\n\n\n\n\nActions are equally likely to occur. \nActions that take the agent out of the environment receive a reward of \\(-1\\), actions that take the agent to the terminal state (shaded in gray) receive a reward of \\(+1\\), and all other actions receive a reward of \\(0\\). \nOur objective is to calculate the shortest path from any state to the optimal state.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#sequential-interaction",
    "href": "private/lecture4/lecture4-2.html#sequential-interaction",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Sequential Interaction",
    "text": "Sequential Interaction\nFor a finite discrete number of time steps \\(t = 0, 1, 2, 3...,T\\) (where \\(T\\) is the terminal time step marking the end of an episode) the sequential interaction is:\n\nThe agent receives an interpretation from the state \\(s_t \\in S\\).\nThe agent makes an action \\(a_t \\in A(s_t)\\) based on the situation.\nThe agent receives a reward \\(r_{t+1} \\in R \\subseteq \\mathbb{R}\\) from its environment and finds itself in a new state \\(s_{t+1}\\) based on the action taken.\n\nThe sequence continues in the form:\n\\[\ns_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3,...\n\\]\n\n\n\n\n\n\nNoteExample: GridWorld\n\n\n\nSequential interaction for one episode:\n\n\n\n\n\nNotice that, for now, state transitions are deterministic. In other words, we assume a perfect model of the environment. We do not care about stochastic state transitions (this is something that we will visit in the next lectures).",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#policy",
    "href": "private/lecture4/lecture4-2.html#policy",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Policy",
    "text": "Policy\nA Policy is a mapping from states to probabilities of selecting each possible action, denoted as:\n\\[\n\\pi(a|s)\n\\]\n\n\n\n\n\n\\[\n\\sum_{a \\in A(s)} \\pi(a|s) = 1 \\quad \\text{for all } s \\in S\n\\]\n\n\n\n\n\n\nNoteExample: GridWorld\n\n\n\n\n\n\n\n\nSince all actions are equally likely, we are said to be following a random policy:\n\\[\n\\pi(a_0|s_0) = \\frac{1}{4}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#dynamic-function",
    "href": "private/lecture4/lecture4-2.html#dynamic-function",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Dynamic Function",
    "text": "Dynamic Function\nThe dynamic function is a mapping of the state transition probabilities of the MDP for each possible reward:\n\\[\np(s^{'}, r | s, a)\n\\]\n\n\n\n\n\nAs mentioned before, the dynamics of GridWorld are deterministic leading to the same new state given each state and action:\n\\[\n\\sum_{s^{'} \\in S}\\sum_{r \\in R} p(s^{'}, r| s, a) = 1 \\quad \\text{for all } s \\in S \\text{ and } a \\in A(s)\n\\]\n\n\n\n\n\n\nNoteExample: GridWorld\n\n\n\n\n\n\n\n\nAs mentioned before, the dynamics of GridWorld are deterministic, leading to the same new state given each state and action:\n\\[\np(s_{1}, r_{1}| s_0, a_{0}) = 1\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#goal",
    "href": "private/lecture4/lecture4-2.html#goal",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Goal",
    "text": "Goal\nOur Goal \\(G_{t}\\) is to maximize the expected return of the discounted reward sequence:\n\\[\n\\begin{aligned}\nG_{t} = r_{t+1} + \\gamma r_{t+2} + \\ldots + \\gamma^{T-1} r_{T} \\\\\n     = r_{t+1} + \\gamma (r_{t+2} + \\ldots + \\gamma^{T-2} r_{T}) \\\\\n     = r_{t+1} + \\gamma G_{t+1}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNoteExercise\n\n\n\nSuppose \\(\\gamma = 0.5\\) and the following sequence of rewards is received \\(r_{1} = -1\\), \\(r_{2} = 2\\), \\(r_{3} = 6\\), \\(r_{4} = 3\\), and \\(r_{5} = 2\\), with \\(T = 5\\). What are \\(G_{0}\\), \\(G_{1}\\), ‚Ä¶, \\(G_{5}\\)?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\nG_{5} &= r_{6} + r_{7} + \\dots = 0 \\\\\nG_{4} &= r_{5} + 0.5(G_{5}) = 2 \\\\\nG_{3} &= r_{4} + 0.5(G_{4}) = 4 \\\\\nG_{2} &= r_{3} + 0.5(G_{3}) = 8 \\\\\nG_{1} &= r_{2} + 0.5(G_{2}) = 6 \\\\\nG_{0} &= r_{1} + 0.5(G_{1}) = 2 \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#value-functions",
    "href": "private/lecture4/lecture4-2.html#value-functions",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Value Functions",
    "text": "Value Functions\nValue Functions calculate the expected reward when starting from the state \\(s\\) and then interacting with the environment according to the policy \\(\\pi\\), denoted as:\n\\[\nv_{\\pi}(s) = \\mathbb{E_{\\pi}}[G_{t}| \\ s]\n\\]\n\n\n\n\n\n\nBellman Equation\nFor any policy \\(\\pi\\) and any state \\(s\\), the Bellman equation holds:\n\\[\n\\begin{aligned}\nv_{\\pi}(s) &= \\mathbb{E_{\\pi}}[G_{t}| \\ s] \\\\\n&= \\mathbb{E_{\\pi}}[r_{t+1} + \\gamma G_{t+1}| \\ s] \\\\\n&= \\sum_{a} \\pi(a|s) \\sum_{s^{'},r} p(s^{'},r|s,a)[r_{t+1} + \\gamma \\mathbb{E_{\\pi}}[G_{t+1}| \\ s]] \\\\\n&= \\underbrace{\\sum_{a} \\pi(a|s)}_{Policy} \\underbrace{\\sum_{s^{'}, r} p(s^{'},r|s,a)}_{Dynamic \\ Function}\\underbrace{[r_{t+1} + \\gamma v_{\\pi}(s^{'})]}_{Discounted \\ Reward \\ Sequence}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNoteExercise\n\n\n\nFor the first episode, calculate the value of each state using the Bellman equation:\n\\[\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s^{'}, r} p(s^{'},r|s,a)[r + \\gamma v_{\\pi}(s^{'})]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#action-value-functions",
    "href": "private/lecture4/lecture4-2.html#action-value-functions",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Action Value Functions",
    "text": "Action Value Functions\nAction Value Functions estimate how good it is for an agent to follow policy \\(\\pi\\) given the action taken under the previous state:\n\\[\nq_{\\pi}(s, a) = \\mathbb{E_{\\pi}}[G_{t}| \\ s, \\ a]\n\\]\n\n\n\n\n\n\nBellman Equation\nThe action value function is also expressed in terms of the Bellman equation:\n\\[\n\\begin{align}\n    q_{\\pi}(s, a) & = \\mathbb{E_{\\pi}}[G_{t}| \\ s, \\ a] \\\\\n     & = \\underbrace{\\sum_{s^{'}, r} p(s^{'},r|s,a)}_{Dynamic \\ Function} \\underbrace{[r + \\gamma v_{\\pi}(s^{'})]}_{Discounted \\ Reward \\  Sequence}\n\\end{align}\n\\]\nNotice that the policy is no longer calculated (since an action has already taken place according to the policy), and that the quality of following policy \\(\\pi\\) is calculated in \\(v_{\\pi}(s^{'})\\).",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-2.html#optimal-policy",
    "href": "private/lecture4/lecture4-2.html#optimal-policy",
    "title": "4.2 Markov Decision Process (MDP)",
    "section": "Optimal Policy",
    "text": "Optimal Policy\nAn optimal policy \\(\\pi\\) is defined to be better than or equal to a policy \\(\\pi^{'}\\) if its expected return is greater than or equal to that of \\(\\pi^{'}\\) for all states:\n\\[\n\\pi \\geq \\pi^{'} \\quad \\text{I.F.F.} \\quad v_{\\pi} \\geq v_{\\pi^{'}} \\quad \\forall s \\in S\n\\]\nSince there may be more than one optimal policy, we denote all optimal policies by \\(\\pi_{*}\\).\n\nOptimal Value Function\n\\[\nv_{*}(s) = \\max_{a} \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\n\n\n\n\nOptimal Action-Value Function\n\\[\nq_{*}(s, a) = \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNoteExercise\n\n\n\nFor the first episode, calculate the optimal value for each state using the bellman equation:\n\\[\nv_{*}(s) = \\max_{a} \\sum_{s^{'}, r} p(s^{'},r|s,a) \\big[ r + \\gamma v_{*}(s^{'}) \\big]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\nNotice the behavior of the optimal policy as \\(\\pi_{*} \\to \\infty\\)",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>4.2 Markov Decision Process (MDP)</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-3.html",
    "href": "private/lecture4/lecture4-3.html",
    "title": "4.3 Iterative Policy Evaluation",
    "section": "",
    "text": "How do we measure the long-term value of a plan if we keep trying it over and over again? üîÅ\n\n\n\nMarkov Decision Process (MDP) is a framework for an associative environment.\nA MDP is a model for transitions that are controlled between fully observable states.\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we find an optimal policy \\(\\pi_{*}\\), assuming that we have perfect model of state transitions \\(P(s', r \\mid s, a)\\)?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nDynamic Programming is a collection of algorithms that can be used to compute optimal policies \\(\\pi_{*}\\) in tabular state spaces.\nThese algorithms have limited utility in Reinforcement Learning due to:\n\nAssumption of a perfect model: All state transitions \\(P(s', r \\mid s, a)\\) are known in advance.\nComputational expense: Dynammic Programming typically requires full sweeps over the state space \\(\\forall s \\in S\\), which is only feasible in small, tabular environments.\n\n\n\n\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nWe know how good it is to follow the current policy from \\(s\\) ‚Äî that is \\(v_{\\pi}(s)\\) ‚Äî but would it be better or worse to change to a new policy \\(\\pi^{'}\\)?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nImagine your commute to work every day:\n\n\\(S\\) ‚Äî The location you‚Äôre currently in (e.g., your home or a traffic junction). More generally, \\(S_{1,...,k}\\) can represent multiple possible locations.\n\\(A_{1,...,k}\\) ‚Äî The route you choose (e.g., highway, back streets, scenic route, parkway, or alternate street).\n\n\\(R\\) ‚Äî Your reward could be getting to work quickly, stress-free, or on time.\n\n\n\n\n\n\nAction \\(A_1\\): Choosing the highway from your current location \\(S\\). Reward \\(R\\): Travel time may be shorter or longer depending on traffic.\n\n\n\n\n\n\n\nAction \\(A_2\\): Taking the side street from your current location \\(S\\). Reward \\(R\\): Usually reliable, but travel time is a bit longer on average.\n\n\n\n\n\n\n\nAction \\(A_3\\): Taking the parkway from your current location \\(S\\). Reward \\(R\\): Might have moderate traffic but pleasant scenery.\n\n\n\n\n\n\n\nAction \\(A_4\\): Using an alternate back road from your current location \\(S\\). Reward \\(R\\): Unfamiliar, so travel time is uncertain.\n\n\n\n\n\nSuppose your current route always takes the side street, and on average it takes 35 minutes to reach work. One day, you try a different route and notice it only takes 30 minutes, even though it looked longer on the map.\nHow could you systematically figure out which route is truly the best, and decide whether to stick with your usual route or switch to a new one?\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nOne way to check if it is better to switch from policy \\(\\pi\\) to \\(\\pi^{'}\\) is by checking if the following inequality holds:\n\\[\nq_{\\pi}(s, \\pi^{'}(s)) \\geq v_{\\pi}(s)\n\\]\nIf selecting \\(a\\) in \\(s\\) and thereafter following policy \\(\\pi\\) is better than just following \\(\\pi\\), there must be a better policy \\(\\pi^{'}\\).\nThe special case when this inequality is true is referred to as the policy improvement theorem.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>4.3 Iterative Policy Evaluation</span>"
    ]
  },
  {
    "objectID": "private/lecture4/lecture4-4.html",
    "href": "private/lecture4/lecture4-4.html",
    "title": "4.4 Value Iteration",
    "section": "",
    "text": "What if you could leap straight to the best decision by updating values in one sweep? ‚ö°\n\n\n\nDynamic Programming is a collection of algorithms that can be used to compute optimal policies \\(\\pi_{*}\\).\nOne drawback of policy iteration is that policy evaluation is done iteratively, requiring convergence exactly to \\(v_{\\pi}\\) which occurs only in the limit.\n\n\n\n\n\n\nWarningProblem\n\n\n\nCan we find a way to improve policies without waiting for full convergence of \\(v_{\\pi}\\)?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nImagine your commute to work every day:\n\n\\(S\\) ‚Äî The location you‚Äôre currently in (e.g., your home or a traffic junction). More generally, \\(S_{1,...,k}\\) can represent multiple possible locations.\n\\(A_{1,...,k}\\) ‚Äî The route you can choose (e.g., highway, back streets, scenic route, parkway, or alternate street).\n\n\\(R\\) ‚Äî Your reward could be getting to work quickly, stress-free, or on time.\n\n\n\n\n\n\nAction \\(A_1\\): Choosing the highway from your current location \\(S\\). Reward \\(R\\): Travel time may vary depending on traffic, but contributes to the overall value of being in this state.\n\n\n\n\n\n\n\nAction \\(A_2\\): Taking the side street from your current location \\(S\\). Reward \\(R\\): Usually a reliable route with moderate travel time, contributing to the value of this state.\n\n\n\n\n\n\n\nAction \\(A_3\\): Taking the parkway from your current location \\(S\\). Reward \\(R\\): Might have moderate traffic but pleasant scenery, adding to the state value.\n\n\n\n\n\n\n\nAction \\(A_4\\): Using an alternate back road from your current location \\(S\\). Reward \\(R\\): Unfamiliar route, so contribution to state value is uncertain.\n\n\n\n\n\nSuppose you want to find the optimal route from home to work. Rather than just trying one alternative, you evaluate all possible routes from each location, estimate the expected travel time (or reward) recursively, and keep updating your estimates until they converge.\nHow could you systematically assign a value to each location and decide which route maximizes your overall commute reward?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nValue Iteration truncates the policy evaluation step after just one sweep.\n\n\n\n\n\n\n\n\n\n\n\n\nTipGeneralized Policy Iteration\n\n\n\n\n\nGeneralized Policy Iteration (GPI) refers to the general idea of letting policy evaluation and policy improvement processes interact, regardless of anything else.\nAlmost all of Reinforcement Learning can be described as the policy always being improved with respect to the value function, and the value function always being driven toward the value function for the policy.",
    "crumbs": [
      "Lecture 4: Dynamic Programming",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>4.4 Value Iteration</span>"
    ]
  },
  {
    "objectID": "private/lecture5/lo.html",
    "href": "private/lecture5/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 5: Monte Carlo (Sutton and Barto 2018) üéØ\n\n\n\n\nMonte Carlo Prediction.\nOn-Policy Monte Carlo.\nOff-Policy Monte Carlo.\nHomemade GridWorld OpenAI environment using gymnasium, pygame & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press.",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture5/lecture5-1.html",
    "href": "private/lecture5/lecture5-1.html",
    "title": "5.1 Monte Carlo Prediction",
    "section": "",
    "text": "What happens if we let experience speak ‚Äî one episode at a time? üé≤\n\n\n\nDynamic Programming is a collection of algorithms that can be used to compute optimal policies \\(\\pi_{*}\\).\nThese algorithms have limited utility in Reinforcement Learning due to:\n\nAssumption of a perfect model: All state transitions \\(P(s', r \\mid s, a)\\) are known in advance.\nComputational expense: Dynammic Programming typically requires full sweeps over the state space \\(\\forall s \\in S\\), which is only feasible in small, tabular environments.\n\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we find an optimal policy \\(\\pi_{*}\\), assuming that we have no prior knowledge of state transitions \\(P(s', r \\mid s, a)\\)?\n\n\n\n\n\n\n\n\nNoteExample: GridWorld\n\n\n\nAssume \\(\\gamma = 0.9\\)\nSuppose we followed the trajectory of \\(\\pi\\) for one episode:\n\n\n\n\n\nThe following illustrates a Monte Carlo update for the trajectory:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nMonte Carlo is a powerful learning rule for estimating value functions \\(v_{\\pi}\\) and action value functions \\(q_{\\pi}\\) in associative environments.\nThe power of Monte Carlo resides in its ability to learn the dynamics of any environment, without assuming any prior knowledge, only using experience.\nMonte Carlo methods are based on averaging sample returns of trajectories following a policy \\(\\pi\\).\n\nOnly on the completion of an episode are value estimates \\(v_{\\pi}(s)\\) and action value estimates \\(q_{\\pi}(s,a)\\) updated.\n\n\nIllustration: Monte Carlo Prediction\n\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nRecall that returns are calculated as follows:\n\\[\nG_{t} = r_{t+1} + \\gamma G_{t+1}\n\\]",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>5.1 Monte Carlo Prediction</span>"
    ]
  },
  {
    "objectID": "private/lecture5/lecture5-2.html",
    "href": "private/lecture5/lecture5-2.html",
    "title": "5.2 Monte Carlo Exploring Starts",
    "section": "",
    "text": "Can randomness at the beginning help us see everything eventually? üß≠\n\n\n\nWithout a model, state values \\(v_{\\pi}(s)\\) alone are not sufficient. Without a model, we cannot derive action preferences from state values alone.\n\n\n\n\n\n\nNoteRecall\n\n\n\n\\[\nv_{\\pi}(s) = \\mathbb{E_{\\pi}}[G_{t}| \\ s]\n\\]\n\\(v_{\\pi}(s)\\) - how good it is to be in a state \\(s\\).\n\n\nTo make action selections, we must explicitly estimate the value of each action \\(q_{\\pi}(s,a)\\).\n\n\n\n\n\n\nNoteWhat we are really looking for‚Ä¶\n\n\n\n\\[\nq_{\\pi}(s, a) = \\mathbb{E_{\\pi}}[G_{t}| \\ s, \\ a]\n\\]\n\\(q_{\\pi}(s,a)\\) - how good it is to take action \\(a\\) given that I am in state \\(s\\).\n\n\nMonte Carlo methods are similar to state value estimation, focusing on visits to state-action pairs \\((s,a)\\).\nThis is crucial for Control, which refers to finding approximate optimal policies \\(\\approx \\pi_{*}\\).\nFollowing the idea of Generalized Policy Iteration (GPI), we evaluate and improve action values \\(q_{\\pi}(s,a)\\) to find optimal policies.\nHowever, one major issue is that of estimating action values \\(q_{\\pi}(s,a)\\) is that some state action pairs \\((s,a)\\) may never be visited during an episode.\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we leverage Monte Carlo‚Äôs learning rule to approximate the optimal policy \\(\\approx \\pi_{*}\\), while assuring that each state action pair \\((s,a)\\) is visited?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis brings us back to the same dilemma we faced in the Multi-Armed Bandit chapter, that is: Balancing exploration and exploitation.\nOne ‚Äúquick-fix‚Äù is to start each episode from a random state \\(s\\) and take any action \\(a\\) with probability greater than \\(0\\).\nThis ‚Äúquick-fix‚Äù is referred to as Exploring Starts.\n\n\nPseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>5.2 Exploring Starts Monte Carlo</span>"
    ]
  },
  {
    "objectID": "private/lecture5/lecture5-3.html",
    "href": "private/lecture5/lecture5-3.html",
    "title": "5.3 On-Policy Monte Carlo",
    "section": "",
    "text": "What if we only ever learn from the way we already behave? üîÅ\n\n\n\nWe need a better method of establishing control, or approximating optimal policies \\(\\approx \\pi_{*}\\), in associative environments without relying on unrealistic assumptions.\n\n\n\n\n\n\nTipOn-Policy Learning\n\n\n\nOn-Policy learning evaluates or improves the policy \\(\\pi\\) that is used to make decisions.\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\n\n\n\n\n\nFyodor Dostoevsky\n\n\n\n\n\n\n‚ÄúIt is better to go wrong in one‚Äôs own way than to go right in someone else‚Äôs.‚Äù\n‚Äî Fyodor Dostoevsky\n\nLike Dostoevsky suggests, you stick with your current way of doing things ‚Äî even if it‚Äôs imperfect. You learn from your own behavior ‚Äî your own actions \\(a\\) taken in states \\(s\\), based on the current policy \\(\\pi\\) ‚Äî and improve over time through authentic experience.\nFor example:\n\nIs it better to try your own coding solution and learn from mistakes, rather than copying someone else‚Äôs code?\nHow does observing the outcomes of your own actions help you improve your policy over time?\n\n\n\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we leverage Monte Carlo‚Äôs learning rule to approximate the optimal policy \\(\\approx \\pi_{*}\\), without having to rely on the unrealistic assumption of an initial random state and action \\((s,a)\\)?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nRecall, \\(\\epsilon\\)-greedy methods for balancing exploration and exploitation.\n\n\n\\(\\epsilon\\)-soft policy\nThese policies are usually referred to as \\(\\epsilon\\)-soft policies as they require that the probability of every action is non-zero for all states and actions pairs, that is:\n\\[\n\\pi(a|s) &gt; 0 \\quad \\text{for all} \\quad s \\in S \\quad \\text{and} \\quad a \\in A(s)\n\\]\nTo calculate the probabilities of selecting an action according to the \\(\\epsilon\\)-greedy policy \\(\\pi(a|s)\\), we use the following update rule:\n\\[\n\\pi(a|s) \\gets \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|A(S_{t})|}  & \\text{if} \\quad a = A_{t} \\quad \\text{(exploitation)} \\\\\n\\frac{\\epsilon}{|A(S_{t})|} & \\text{if} \\quad a \\neq A_{t} \\quad \\text{(exploration)}\n\\end{cases}\n\\]\nBy using \\(\\epsilon\\)-soft policies, we ensure that every action \\(a\\) has a non-zero chance of being explored ‚Äî even while following our current policy \\(\\pi\\).\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>5.3 On-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "private/lecture5/lecture5-4.html",
    "href": "private/lecture5/lecture5-4.html",
    "title": "5.4 Off-Policy Monte Carlo",
    "section": "",
    "text": "Can we watch others and still learn something for ourselves? üëÄ\n\n\n\nLearning directly from the policy \\(\\pi\\) is powerful, but it would be even better if we could learn about \\(\\pi\\) while observing or following a different policy, especially one that helps us explore more broadly or leverage past experiences.\n\n\n\n\n\n\nTipOff-Policy Learning\n\n\n\nOff-Policy methods evaluate or improve a policy \\(\\pi\\) different from that used to generate the data \\(b\\). Typically this is accomplished using two policies:\n\nA target policy, denoted \\(\\pi\\), is the policy being learned.\nA behavior policy, denoted \\(b\\), is the policy used to generate behavior.\n\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\n\n\n\n\n\n\n‚ÄúIf I have seen further, it is by standing on the shoulders of giants.‚Äù ‚Äî Isaac Newton\n\nNewton is considered one of the greatest scientists of all time. Among his many contributions:\n\nüß≤ Formulated the laws of motion and universal gravitation.\nüåà Demonstrated that white light is made of a spectrum of colors.\nüìê Developed calculus (independently of Leibniz).\nüî≠ Improved the telescope and advanced optical theory.\n\nHe would not have been able to accomplish these things without learning from the work of others ‚Äî Kepler, Galileo, Descartes, and more ‚Äî whose insights formed the foundation for his own breakthroughs.\n\n\n\n\n\nKepler - Discovered the laws of planetary motion, showing that planets move in ellipses and not circles.\n\n\n\n\n\n\n\nGalileo - Studied motion and inertia, laying the groundwork for Newton‚Äôs first two laws of motion.\n\n\n\n\n\n\n\nDescartes - Developed analytic geometry and early ideas of mechanistic physics, helping bridge mathematics and physical laws.\n\n\n\n\n\nJust like in off-policy learning, Newton didn‚Äôt need to replicate others‚Äô paths \\(\\pi_{\\text{Kepler, Galileo, Descartes}}\\) exactly. Instead, he learned from their trajectories \\(b_{\\text{Kepler, Galileo, Descartes}}\\) to improve his own \\(\\pi_{Newton}\\).\n\n\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we leverage Monte Carlo‚Äôs learning rule to approximate the optimal policy \\(\\approx \\pi_{*}\\), while leveraging other policies \\(b\\) apart from our own \\(\\pi\\)?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nImportance Sampling is a general technique for estimating expected values under one distribution given samples from another.\nWe apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio.\n\\[\n\\text{Pr}\\{A_{t}, S_{t+1}, A_{t+1}, \\dots , S_{T} \\mid S_{t}, A_{t:T-1} \\sim \\pi \\} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]\n\n\nIncremental Method\nSimilarly to the Multi-Armed Bandits chapter, action values \\(q_{\\pi}(s,a)\\) can be calculated incrementally.\nIn order to do so, we must first begin by calculating a cumulative sum of the weights:\n\\[\nC(S_{t},A_{t}) = C(S_{t},A_{t}) + W\n\\]\nThen, we average returns of corresponding action values:\n\\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\frac{W}{C(S_{t},A_{t})}[G - Q(S_{t},A_{t})]\n\\]\nFinally, we update the weight according to our importance sampling ratio:\n\\[\nW = W \\frac{\\pi(A_{k} \\mid S_{k})}{b(A_{k} \\mid S_{k})}\n\\]\nWe can assure Off-Policy methods to achieve control by choosing \\(b\\) to be \\(\\epsilon\\)-soft.\nThe target policy \\(\\pi\\) converges to optimal at all encountered states even though actions are selected according to a different soft policy \\(b\\), which may change between or even within episodes.\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 5: Monte Carlo",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>5.4 Off-Policy Monte Carlo</span>"
    ]
  },
  {
    "objectID": "private/lecture6/lo.html",
    "href": "private/lecture6/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 6: Temporal Difference (Sutton and Barto 2018) üéØ\n\n\n\n\nTemporal Difference (TD) Prediction.\nSARSA.\nQ-Learning.\nDouble Q-Learning.\n(Optional) n-step Bootstrapping.\nHomemade GridWorld OpenAI environment using gymnasium, pygame & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press.",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture6/lecture6-1.html",
    "href": "private/lecture6/lecture6-1.html",
    "title": "6.1 Temporal Difference (TD) Prediction",
    "section": "",
    "text": "What if you made a guess ‚Äî and then updated it right away, before seeing how it ends? ‚è≥\n\n\n\nMonte Carlo is a powerful learning rule for estimating value functions \\(v_{\\pi}\\) and action value functions \\(q_{\\pi}\\) in associative environments.\nThe power of Monte Carlo resides in its ability to learn the dynamics of any environment, without assuming any prior knowledge, only using experience.\nMonte Carlo methods are based on averaging sample returns of trajectories following a policy \\(\\pi\\).\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we design a learning rule that updates value estimates during an episode, rather than waiting until the entire trajectory \\(\\tau\\) is complete?\n\n\n\n\n\n\n\n\nNoteExample: GridWorld\n\n\n\nAssume \\(\\gamma = 0.9\\)\nSuppose we follow the trajectory of \\(\\pi\\) for one episode:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nTemporal Difference (TD) is a learning rule that is a combination of Monte Carlo and Dynamic Programming ideas.\n\nTD methods, like Monte Carlo, learn from experience by updating estimates of nonterminal states along a trajectory \\(\\tau\\).\nTD methods, like Dynamic Programming, update based on an existing estimate \\(V(S_{t+1})\\).\n\nTD methods at time \\(t + 1\\) immediately form a target and make a useful update using the observed reward \\(R_{t+1}\\) and the estimate \\(V(S_{t+1})\\) in a incremental fashion:\n\\[\nV(S_{t}) = V(S_{t}) + \\underbrace{\\alpha}_{Step \\ Size} [ \\underbrace{\\underbrace{R_{t+1} + \\gamma V(S_{t+1})}_{Target \\ Update} - V(S_{t})}_{TD \\  Error}]\n\\]\n\n\nIllustration: Temporal Difference (TD) Prediction\n\n\n\n\n\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>6.1 Temporal Difference (TD) Prediction</span>"
    ]
  },
  {
    "objectID": "private/lecture6/lecture6-2.html",
    "href": "private/lecture6/lecture6-2.html",
    "title": "6.2 SARSA",
    "section": "",
    "text": "What if your learning came from walking the path you‚Äôre already on ‚Äî mistakes and all? üö∂‚Äç‚ôÇÔ∏è\n\n\n\nTo move from prediction to control, we must estimate the action-value function \\(q_{\\pi}(s, a)\\) for the current behavior policy \\(\\pi\\) ‚Äî and ensure we update it for all state-action pairs \\((s, a)\\) encountered during interaction with the environment.\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we leverage the Temporal Difference learning rule to approximate the optimal policy \\(\\approx \\pi_{*}\\) ‚Äî while still exploring, ensuring every state-action \\((s,a)\\) pair is visited, and updating only from actions actually taken by the current policy \\(\\pi\\)?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nSuppose you‚Äôre using a music recommendation app:\n\n\\(S\\) ‚Äî Your current mood or environment (e.g., studying, running, relaxing).\n\n\\(A\\) ‚Äî The app suggests a song based on your current preferences.\n\n\\(R\\) ‚Äî You either enjoy the song, skip it, or dislike it ‚Äî giving implicit feedback.\n\n\\(S', A'\\) ‚Äî The next song plays, and you make another decision.\n\n\n\n\n\n\nState \\(S\\): Feeling energetic. Action \\(A\\): Play Salsa (Oscar De Le√≥n). Reward \\(R\\): Always enjoyable ‚Äî positive feedback reinforces this choice. Next state \\(S'\\) and action \\(A'\\) follow after this song.\n\n\n\n\n\n\n\nState \\(S\\): Feeling upbeat. Action \\(A\\): Play Reggaeton (Feid). Reward \\(R\\): Mostly hits, sometimes skipped ‚Äî app updates preference based on this feedback.\n\n\n\n\n\n\n\nState \\(S\\): Feeling calm. Action \\(A\\): Play Jazz. Reward \\(R\\): New genre, unknown feedback ‚Äî exploration allows the app to learn preferences.\n\n\n\n\n\nHow can your music app learn what you like while still exploring new genres ‚Äî using your actual listening behavior, one song at a time?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nSARSA is an on-policy TD control method that updates action-value estimates based \\(q_{\\pi}(s, a)\\) on the trajectory actually followed by the current policy \\(\\pi\\) ‚Äî including any exploration it performs.\n\\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\alpha [ R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t})]\n\\]\nThis rule uses every element of the quintuple of events (\\(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1}\\))\nThat‚Äôs SARSA: learning step-by-step from your real actions, even if your current ‚Äúplaylist‚Äù isn‚Äôt perfect.\nUnlike Monte Carlo methods, which wait until the end of a full playlist (or episode) to update preferences, SARSA updates incrementally ‚Äî after every song ‚Äî using immediate feedback and your next action.\n\n\nIllustration: SARSA\n\n\n\n\n\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>32</span>¬† <span class='chapter-title'>6.2 SARSA</span>"
    ]
  },
  {
    "objectID": "private/lecture6/lecture6-3.html",
    "href": "private/lecture6/lecture6-3.html",
    "title": "6.3 Q-Learning",
    "section": "",
    "text": "What if you learned from what could have been done ‚Äî even if you didn‚Äôt do it? üëÄ\n\n\n\nIdeally, we would like to have a TD method that can be off-policy.\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we leverage the Temporal Difference learning rule to approximate the optimal policy \\(\\approx \\pi_*\\) by learning from actions we didn‚Äôt take ‚Äî using the best possible next action \\(A\\) to update our strategy, even if it wasn‚Äôt chosen by our current policy \\(\\pi\\)?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nSuppose you‚Äôre studying chess by watching a famous game between two grandmasters:\n\n\\(S\\) ‚Äî You recognize a familiar board position from their match.\n\\(A\\) ‚Äî In your current game, you‚Äôre unsure what to play ‚Äî but you recall the grandmaster‚Äôs move in that same position.\n\\(R\\) ‚Äî You remember that the move they chose led to a strong positional advantage (high reward).\n\\(S'\\) ‚Äî You predict the next likely position on the board and evaluate the possible replies.\n\nEven though you‚Äôre not playing exactly like the grandmasters did ‚Äî and your opponent may respond differently ‚Äî you use the best move from their game as your update target, trusting that it‚Äôs the optimal next action in that state.\n\n\n\n\n\nBobby Fischer vs.¬†Boris Spassky (Game 6)\n\n\n\n\n\n\n\n\n\n\\(S\\) You arrive at a familiar position - Queen‚Äôs Gambit Declined (Tartakower Variation)\n\n\n\n\n\n\n\n\\(A\\) You recall a grandmaster‚Äôs move from memory - (8. cxd5)\n\n\n\n\n\n\n\n\\(A \\to S'\\) You use Bobby Fischer‚Äôs move to force an exchange\n\n\n\n\nYou don‚Äôt have to repeat the entire grandmaster game ‚Äî you just need to know what move worked best in each position and adjust your own strategy accordingly.\nHow can you improve your chess strategy by learning from the best moves of others ‚Äî even if you never made those moves yourself?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nQ-Learning is a TD off-policy method that can both update and estimate the optimal action-value function directly:\n\\[\nQ(S_{t},A_{t}) = Q(S_{t},A_{t}) + \\alpha [ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1},a) - Q(S_{t},A_{t})]\n\\]\nQ-Learning is a breakthrough in reinforcement learning due to its simplification of algorithm analysis and early convergence proofs.\n\n\nIllustration: Q-Learning\n\n\n\n\n\n\n\n\nPseudocode",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>33</span>¬† <span class='chapter-title'>6.3 Q-Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture6/lecture6-4.html",
    "href": "private/lecture6/lecture6-4.html",
    "title": "6.4 Double Q-Learning",
    "section": "",
    "text": "What if two minds were better than one ‚Äî especially to keep each other honest? üß†üß†\n\n\n\nThe problem with relying with the conceptions of previous behavior \\(b\\) is that it is susceptible to biases.\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we leverage the Temporal Difference learning rule to approximate the optimal policy \\(\\approx \\pi_*\\) by learning from actions we didn‚Äôt take ‚Äî without falling into biases of conceptions of previous behavior \\(b\\)?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nMuch of classical economics assumes humans make perfectly rational decisions ‚Äî but Daniel Kahneman‚Äôs Thinking, Fast and Slow revealed how our minds often fall into predictable biases.\n\n\n\n\n\nThinking, Fast and Slow by Daniel Kahneman\n\n\n\n\n\n\n\nDaniel Kahneman - Nobel Prize Winner in Economic Sciences in 2002\n\n\n\n\n\nKahneman describes two modes of thought:\n\nüß† System 1 ‚Äî Fast, automatic, emotional, and often biased; rooted in evolutionarily older brain structures like the limbic system, designed for quick survival decisions.\nüß† System 2 ‚Äî Slow, deliberate, logical, and effortful; associated with the prefrontal cortex, which supports planning, reasoning, and self-control.\n\n\nWhich of the two lines is longer?\n\n\n\n\n\n\n\n\n\nNoteExample: Lines\n\n\n\n\n\nAnswer: They are of the same length.\n\n\n\n\nImagine a pond that gets filled by an invasive species (like algae) that doubles in size every day. If the river becomes completely full on the 30th day, on which day was it half full?\n\n\n\n\n\n\nNoteExample: Pond\n\n\n\n\n\nAnswer: Day 29.\n\n\n\nHow can keeping your fast, intuitive decisions in check with a slower, more reflective system help you avoid overconfidence ‚Äî and make better choices over time?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nDouble Q-Learning addresses this bias by creating two action value estimates \\(Q_{1}(s,a)\\) and \\(Q_{2}(s,a)\\).\nWith equal likelihood, one action value estimate yields the maximization action \\(A_{t}\\) and the other provides the action value estimate \\(Q(S_{t}, A_{t})\\).\n\\[\nQ_{1}(S_{t},A_{t}) = Q_{1}(S_{t},A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{2}(S_{t+1},\\max_{a} Q_{1}(S_{t+1},a)) - Q_{1}(S_{t},A_{t})]\n\\]\n\\[\nQ_{2}(S_{t},A_{t}) = Q_{2}(S_{t},A_{t}) + \\alpha [R_{t+1} + \\gamma Q_{1}(S_{t+1},\\max_{a} Q_{2}(S_{t+1},a)) - Q_{2}(S_{t},A_{t})]\n\\]\nThis dual-system model mirrors Double Q-learning:\n\nOne Q-estimator (\\(Q_1\\)) makes a quick action selection ‚Äî like System 1.\nThe other Q-estimator (\\(Q_2\\)) does the critical evaluation ‚Äî like System 2.\n\nThis separation reduces maximization bias, much like System 2 tempers System 1‚Äôs impulsive decisions.\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\n\n\n\nMaximization bias is a maximization of actual action value estimates \\(Q(s,a)\\) is higher than those of the true action values \\(q(s,a)\\), leading to a bias.",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>6.4 Double Q-Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture6/lecture6-5.html",
    "href": "private/lecture6/lecture6-5.html",
    "title": "6.5 (Optional) n-step Bootstrapping",
    "section": "",
    "text": "What if you waited a little longer before updating ‚Äî not too soon, not too late? ‚è±Ô∏è\n\n\n\n\n\n\n\n\n\nNoten-step TD Prediction\n\n\n\nn-step Bootstrapping is a learning rule that is a combination of Monte Carlo and Temporal Difference ideas.\n\nLike Monte Carlo, n-step methods learn from experience.\nLike Temporal Difference, n-step methods bootstrap multiple time steps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoten-step SARSA\n\n\n\nn-step SARSA extends the standard SARSA algorithm to incorporate multi-step returns. Instead of updating based on a single-step transition, it utilizes an accumulated return over n steps, striking a balance between bias and variance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoten-step Tree Backup\n\n\n\nn-step Tree Backup is an extension of Q-learning that allows updates without the requirement of selecting an on-policy action. It generalizes the Expected SARSA algorithm by propagating multiple steps of information while weighting future actions by their probability under the policy.",
    "crumbs": [
      "Lecture 6: Temporal Difference",
      "<span class='chapter-number'>35</span>¬† <span class='chapter-title'>6.5 (Optional) n-step Bootstrapping</span>"
    ]
  },
  {
    "objectID": "private/lecture7/lo.html",
    "href": "private/lecture7/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 7: Function Approximation (Sutton and Barto 2018) üéØ\n\n\n\n\nLet‚Äôs talk about Exam 1 and Final Project!\nValue Function Approximation.\nOn-Policy Approximation.\nLimitations of Off-Policy Approximation.\nMountainCarContinuous-v0 environment using gymnasium & numpy.\n\n\n\n\n\nTaxonomy of Reinforcement Learning\n\n\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press.",
    "crumbs": [
      "Lecture 7: Function Approximation",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture7/lecture7-1.html",
    "href": "private/lecture7/lecture7-1.html",
    "title": "7.1 Value Function Approximation",
    "section": "",
    "text": "Types of Value Function Approximators\nAssume state \\(\\mathbf{s}\\) is represented by a vector of continuous values.\n\\[\n\\mathbf{s} = \\begin{bmatrix} s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_n \\end{bmatrix}\n\\]\nwhere \\(s_i \\in \\mathbb{R}\\) for all \\(i = 1, 2, \\ldots, n\\)\nThere are many ways of constructing \\(\\hat{V}(s; \\mathbf{w})\\):\nWe will focus only on differentiable methods:\nThe purpose is to update our parameters \\(\\mathbf{w}\\) using mean-squared error (MSE) and stochastic gradient descent (SGD).",
    "crumbs": [
      "Lecture 7: Function Approximation",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>7.1 Value Function Approximation</span>"
    ]
  },
  {
    "objectID": "private/lecture7/lecture7-1.html#types-of-value-function-approximators",
    "href": "private/lecture7/lecture7-1.html#types-of-value-function-approximators",
    "title": "7.1 Value Function Approximation",
    "section": "",
    "text": "Ensemble methods (decision trees, nearest neighbors, etc.)\nFourier basis\nMuch more‚Ä¶\n\n\n\nLinear combination of features (today‚Äôs lecture)\nNeural networks (next lecture: DQN)",
    "crumbs": [
      "Lecture 7: Function Approximation",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>7.1 Value Function Approximation</span>"
    ]
  },
  {
    "objectID": "private/lecture7/lecture7-1.html#updating-value-function-approximators",
    "href": "private/lecture7/lecture7-1.html#updating-value-function-approximators",
    "title": "7.1 Value Function Approximation",
    "section": "Updating Value Function Approximators",
    "text": "Updating Value Function Approximators\nOur loss function will optimize for our parameter vector \\(\\mathbf{w}\\) while minimizing MSE between our approximate value \\(\\hat{V}(s; \\mathbf{w})\\) and our ‚Äútrue value‚Äù \\(V_{\\pi}(s)\\):\n\\[\nF(\\mathbf{w}_{t}) = \\mathbb{E}_{\\pi}[(V_{\\pi}(S_{t}) - \\hat{V}(S_{t}; \\mathbf{w}_{t}))^{2}]\n\\]\n\n\nRecall Mean Squared Error (MSE) for supervised learning:\n\\[\nF(\\mathbf{x}_{k}) = \\mathbb{E}[(\\mathbf{t}_{k} - \\mathbf{a}_{k})^2]\n\\]\nRecall Stochastic Gradient Descent (SGD) for supervised learning:\n\\[\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha \\nabla_{\\mathbf{x}_{k}} F(\\mathbf{x}_{k})\n\\]\nSGD update for parameters \\(\\mathbf{w}\\):\n\\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\alpha(V_{\\pi}(S_{t}) - \\hat{V}(S_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{V}(S_{t}; \\mathbf{w}_{t})\n\\]\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\nPlug derivative of MSE loss into SGD equation:\n\\[\n\\begin{align}\n\\mathbf{w}_{t+1} &= \\mathbf{w}_{t} - \\alpha \\nabla_{\\mathbf{w}_{t}} F(\\mathbf{w}_{t}) \\\\[10pt]\n&= \\mathbf{w}_{t} - \\alpha (-2(V_{\\pi}(S_{t}) - \\hat{V}(S_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{V}(S_{t}; \\mathbf{w}_{t})) \\\\[10pt]\n&= \\mathbf{w}_{t} + 2\\alpha(V_{\\pi}(S_{t}) - \\hat{V}(S_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{V}(S_{t}; \\mathbf{w}_{t}) \\\\[10pt]\n&= \\mathbf{w}_{t} + \\alpha(V_{\\pi}(S_{t}) - \\hat{V}(S_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{V}(S_{t}; \\mathbf{w}_{t})\n\\end{align}\n\\]",
    "crumbs": [
      "Lecture 7: Function Approximation",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>7.1 Value Function Approximation</span>"
    ]
  },
  {
    "objectID": "private/lecture7/lecture7-1.html#state-preprocessing",
    "href": "private/lecture7/lecture7-1.html#state-preprocessing",
    "title": "7.1 Value Function Approximation",
    "section": "State Preprocessing",
    "text": "State Preprocessing\nPrior to calculating \\(\\hat{V}(s; \\mathbf{w})\\), we must preprocess \\(\\mathbf{s}\\) to construct proper feature representations:\n\\[\n\\mathbf{f}(s) = \\begin{bmatrix} s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_d \\end{bmatrix}\n\\]\nSome types of feature representations \\(\\mathbf{f}\\) include:\n\nOne-hot encoding\nPolynomials\nRadial basis functions\nState normalization (homework)\nTile coding (homework)\n\nState normalization ensures consistent scaling between \\(0\\) and \\(1\\):\n\\[\n\\mathbf{f}(s) = \\begin{bmatrix}\n\\frac{s_1 - \\text{lower bound}_{1}}{\\text{upper bound}_{1} - \\text{lower bound}_{1}} \\\\\n\\frac{s_2 - \\text{lower bound}_{2}}{\\text{upper bound}_{2} - \\text{lower bound}_{2}} \\\\\n\\vdots \\\\\n\\frac{s_d - \\text{lower bound}_{d}}{\\text{upper bound}_{d} - \\text{lower bound}_{d}}\n\\end{bmatrix}\n\\]\nTile coding is a one-hot representation for multi-dimensional continuous spaces that is flexible and computationally efficient.\n\\[\n\\mathbf{f}(s) = \\begin{bmatrix}\n\\delta(s, T_1) \\\\\n\\delta(s, T_2) \\\\\n\\vdots \\\\\n\\delta(s, T_d)\n\\end{bmatrix}\n\\text{where} \\ d \\ \\text{is the number of tilings}\n\\]\n\\[\n\\delta(s, T_i) =\n\\begin{cases}\n1 & \\text{if } s \\in T_i \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nBased on your mathematical intuition using SGD, are we guaranteed convergence to a local or global minimum?\n\\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\alpha(V_{\\pi}(S_{t}) - \\hat{V}(S_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{V}(S_{t}; \\mathbf{w}_{t})\n\\]\nHint: Think about Lecture 1\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nNo ‚Äî because this is not supervised learning.\n\n\\(V_{\\pi}(S_t)\\) is not a true target, but an estimate based on samples or bootstrapping.\n\n\\(\\nabla_{\\mathbf{w}_t} \\hat{V}(S_t; \\mathbf{w}_t)\\) is a noisy gradient, not the true gradient of a fixed loss function.\n\n\n\n\n\n\n\n\nMoore, Andrew William. 1990. ‚ÄúEfficient Memory-Based Learning for Robot Control.‚Äù University of Cambridge.",
    "crumbs": [
      "Lecture 7: Function Approximation",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>7.1 Value Function Approximation</span>"
    ]
  },
  {
    "objectID": "private/lecture7/lecture7-2.html",
    "href": "private/lecture7/lecture7-2.html",
    "title": "7.2 On-Policy Function Approximation",
    "section": "",
    "text": "What if you could estimate how good a move is ‚Äî even when the board is too large to memorize? üéØ\n\n\n\nApproximating state values is not sufficient to achieve control.\n\\[\n\\hat{V}(s; \\mathbf{w}) \\approx V_{\\pi}(s)\n\\]\n\n\n\n\n\n\nWarningProblem\n\n\n\nWhat function should we focus on approximating in order to achieve control (approximate optimal policies \\(\\approx \\pi_*\\)) with continuous state information \\(\\mathbf{s}\\)?\n\n\n\n\n\n\n\n\nNoteExample: MountainCar\n\n\n\nConsider the following OpenAI Gymnasium MountainCar environment (Moore 1990):\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe need to approximate action-value functions.\n\\[\n\\hat{Q}(s,a; \\mathbf{w}) \\approx Q_{\\pi}(s,a)\n\\]\nIn doing so, we will incorporate previously learned techniques, such as SARSA from Temporal Difference learning to devise a learning algorithm.\n\n\nUpdating Action Value Function Approximators\nMSE loss for action values:\n\\[\nF(\\mathbf{w}_{t}) = \\mathbb{E}_{\\pi}[(Q_{\\pi}(S_{t},A_{t}) - \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t}))^{2}]\n\\]\nSGD update for parameters \\(\\mathbf{w}\\):\n\\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\alpha(Q_{\\pi}(S_{t},A_{t}) - \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t})\n\\]\n\n\n\nTD Update\nFinally, to implement TD learning, we substitute \\(Q_{\\pi}(s,a)\\) for our TD targets at each step:\n\\[\n\\langle S_{0}, \\underbrace{\\ R_{1} + \\gamma \\hat{Q}(S_{1},A_{1}; \\mathbf{w}_{0})}_{\\text{TD-target for Initial State}}\\rangle, \\ ... \\ ,\\langle S_{T-1},\\underbrace{R_{T}}_{\\text{TD-target for Terminal State}}\\rangle\n\\]\nThus, our update equation for TD on-policy approximation:\n\\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\alpha(R_{t+1} + \\gamma \\hat{Q}(S_{t+1},A_{t+1}; \\mathbf{w}_{t}) - \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t}))\\nabla_{\\mathbf{w}_{t}} \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t})\n\\]\n\n\n\nIllustration: Semi-Gradient SARSA\n\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nHow can we summarize the Semi-Gradient SARSA update equation in 3 components?\n\\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\underbrace{\\alpha}\\underbrace{(R_{t+1} + \\gamma \\hat{Q}(S_{t+1},A_{t+1}; \\mathbf{w}_{t}) - \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t}))}\\underbrace{\\nabla_{\\mathbf{w}_{t}} \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t})}\n\\]\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\nThe update is a product of how much to learn, how wrong we were, and how to change.\n\\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\underbrace{\\alpha}_{\\text{Learning Rate}}\\underbrace{(R_{t+1} + \\gamma \\hat{Q}(S_{t+1},A_{t+1}; \\mathbf{w}_{t}) - \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t}))}_{\\text{TD-Error}}\\underbrace{\\nabla_{\\mathbf{w}_{t}} \\hat{Q}(S_{t},A_{t}; \\mathbf{w}_{t})}_{\\text{Gradient}}\n\\]\n\n\n\n\n\n\n\n\n\nWarningOff-Policy Function Approximation\n\n\n\nBaird‚Äôs Counterexample shows that even with linear function approximation, off-policy TD methods like Q-learning can diverge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world demonstration of this divergence: as training progresses, value estimates explode instead of converging.\nConvergence of control algorithms:\n\n\n\nAlgorithm\nTabular\nLinear\nNeural Networks\n\n\n\n\nMonte-Carlo Control\n‚úÖ\n(‚úÖ)\n‚ùå\n\n\nSARSA\n‚úÖ\n(‚úÖ)\n‚ùå\n\n\nQ-learning\n‚úÖ\n‚ùå\n‚ùå\n\n\n\n\n\n\n\n\n\nMoore, Andrew William. 1990. ‚ÄúEfficient Memory-Based Learning for Robot Control.‚Äù University of Cambridge.",
    "crumbs": [
      "Lecture 7: Function Approximation",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>7.2 On-Policy Function Approximation</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lo.html",
    "href": "private/lecture8/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 8: Deep Q-Networks üéØ\n\n\n\n\nDeep Learning.\nDeep Q-Networks (DQN).\nALE/Breakout-v5 environment using gymnasium & tensorflow.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-1.html",
    "href": "private/lecture8/lecture8-1.html",
    "title": "8.1 Deep Learning",
    "section": "",
    "text": "Multi-Layered Perceptrons (MLPs) (Hagan et al. 2014)\nAssume state \\(\\mathsf{S}\\) is represented by a tensor of continuous values.\nA single parameter vector \\(\\mathbf{w}\\) is insufficient to learn useful representations of the state tensor \\(\\mathsf{S}\\).\nIn Reinforcement Learning, MLPs are commonly used when states are represented by continuous-valued vectors \\(\\mathbf{s}\\).",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>40</span>¬† <span class='chapter-title'>8.1 Deep Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-1.html#multi-layered-perceptrons-mlps-hagan2014neural",
    "href": "private/lecture8/lecture8-1.html#multi-layered-perceptrons-mlps-hagan2014neural",
    "title": "8.1 Deep Learning",
    "section": "",
    "text": "NoteIllustration: MLP \\(\\theta\\)",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>40</span>¬† <span class='chapter-title'>8.1 Deep Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-1.html#mlp-forward-propagation",
    "href": "private/lecture8/lecture8-1.html#mlp-forward-propagation",
    "title": "8.1 Deep Learning",
    "section": "MLP Forward Propagation",
    "text": "MLP Forward Propagation\nAt the beginning of MLP forward propagation, let \\(\\mathbf{p}\\) be the input vector:\n\\[\n\\mathbf{a}^0 = \\mathbf{p}\n\\]\nThe input passes through \\(M-1\\) hidden layers:\n\\[\n\\mathbf{a}^{m+1} = \\mathbf{f}^{m+1}(\\mathbf{W}^{m+1} \\cdot \\mathbf{a}^m + \\mathbf{b}^{m+1}) \\quad \\text{for } m = 0, 1, \\dots, M-2\n\\]\n\n\n\\(\\mathbf{a}^m\\): Output vector of layer \\(m\\)\n\\(\\mathbf{W}^{m+1}\\): Weight matrix of layer \\(m+1\\)\n\\(\\mathbf{b}^{m+1}\\): Bias vector of layer \\(m+1\\)\n\\(\\mathbf{f}^{m+1}()\\): Activation function of layer \\(m+1\\)\nFor the final (output) layer \\(M\\):\n\\[\n\\mathbf{a}^M = \\text{softmax}(\\mathbf{W}^{M} \\cdot \\mathbf{a}^{M-1} + \\mathbf{b}^{M})\n\\]\n\n\n\\(\\mathbf{a}^{M-1}\\): Output vector of the last hidden layer\n\\(\\mathbf{W}^M\\), \\(\\mathbf{b}^M\\): Weights and bias for output layer\n\\(\\text{softmax}()\\): Softmax activation function\n\nMLP Backpropagation\nIn MLP backpropagation, the sensitivity of the output layer is:\n\\[\n\\mathbf{s}^M = \\mathbf{a} - \\mathbf{t}\n\\]\n\n\n\\(\\mathbf{s}^M\\): Sensitivity of the output layer\n\\(\\mathbf{t}\\): Target class vector\nTo update sensitivities for preceding layers:\n\\[\n\\mathbf{s}^m = \\mathbf{F}^m \\cdot \\mathbf{n}^m \\cdot \\mathbf{W}^{m+1^\\top} \\cdot \\mathbf{s}^{m+1} \\quad \\text{for } m = M-1, \\dots, 2, 1\n\\]\n\n\n\\(\\mathbf{s}^m\\): Sensitivity of layer \\(m\\)\n\\(\\mathbf{F}^m\\): Derivative of activation function of layer \\(m\\)\n\\(\\mathbf{n}^m\\): Input vector to layer \\(m\\)\n\\(\\mathbf{W}^{m+1^\\top}\\): Transpose of weight matrix of layer \\(m+1\\)\n\\(\\mathbf{s}^{m+1}\\): Sensitivity of layer \\(m+1\\)\n\n\nMLP Weight and Bias Updates\nThe weights are updated using the rule:\n\\[\n\\mathbf{W}_{k+1}^m = \\mathbf{W}_k^m - \\alpha \\, \\mathbf{s}^m \\cdot (\\mathbf{a}^{{m-1}^\\top})\n\\]\n\n\n\\(\\mathbf{W}_k^m\\): Weight matrix of layer \\(m\\) at iteration \\(k\\)\n\\(\\mathbf{W}_{k+1}^m\\): Weight matrix at iteration \\(k+1\\)\n\\(\\mathbf{s}^m\\): Sensitivity of layer \\(m\\)\n\\(\\mathbf{a}^{{m-1}^\\top}\\): Transpose of the output from layer \\(m-1\\)\n\\(\\alpha\\): Learning rate\nThe biases are updated as:\n\\[\n\\mathbf{b}_{k+1}^m = \\mathbf{b}_k^m - \\alpha \\, \\mathbf{s}^m\n\\]\n\n\n\\(\\mathbf{b}_k^m\\): Bias vector of layer \\(m\\) at iteration \\(k\\)\n\\(\\mathbf{b}_{k+1}^m\\): Bias vector at iteration \\(k+1\\)",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>40</span>¬† <span class='chapter-title'>8.1 Deep Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-1.html#convolutional-neural-networks-cnns-nndesigndeeplearning2024",
    "href": "private/lecture8/lecture8-1.html#convolutional-neural-networks-cnns-nndesigndeeplearning2024",
    "title": "8.1 Deep Learning",
    "section": "Convolutional Neural Networks (CNNs) (Martin T. Hagan 2024)",
    "text": "Convolutional Neural Networks (CNNs) (Martin T. Hagan 2024)\nIn Reinforcement Learning, CNNs are commonly used when states are represented by continuous-valued tensors \\(\\mathsf{S}\\).\n\n\n\n\n\n\nNoteIllustration: CNN \\(\\theta\\)",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>40</span>¬† <span class='chapter-title'>8.1 Deep Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-1.html#cnn-forward-propagation",
    "href": "private/lecture8/lecture8-1.html#cnn-forward-propagation",
    "title": "8.1 Deep Learning",
    "section": "CNN Forward Propagation",
    "text": "CNN Forward Propagation\nAt the beginning of CNN forward propagation, let \\(\\mathsf{P}\\) be the input tensor:\n\\[\n\\mathsf{A}^0 = \\mathsf{P}\n\\]\nFor each convolutional layer, the input passes through a convolutional kernel followed by a non-linear activation function:\n\\[\n\\mathsf{A}^m = \\mathbf{f}^m\\left( \\mathbf{W}^m \\ast \\mathsf{A}^{m-1} + \\mathbf{B}^m \\right) \\quad \\text{for } m = 1, 2, \\dots, M\n\\]\n\n\n\\(\\mathsf{A}^{m-1}\\): Input to layer \\(m\\)\n\\(\\mathbf{W}^m\\): Convolutional kernel(s) at layer \\(m\\)\n\\(\\mathbf{B}^m\\): Bias tensor at layer \\(m\\)\n\\(\\mathbf{f}^m()\\): Activation function (e.g., ReLU) at layer \\(m\\)\n\\(\\ast\\): Convolution operator\nOptionally, a pooling operation can be applied after certain convolutional layers:\n\\[\n\\mathsf{A}^m_{\\text{pool}} = \\boxplus^{\\text{pool}} \\mathsf{A}^m\n\\]\n\n\n\\(\\boxplus^{\\text{pool}}\\): Pooling operator (e.g., max or average) to reduce spatial dimensions\nFinally, we flatten the final feature maps into a vector for input into the fully connected layers:\n\\[\n\\mathbf{a}^{\\text{flat}} = \\text{flatten}(\\mathsf{A}^M)\n\\]\n\n\n\\(\\text{flatten}()\\): Converts the final 3D tensor into a 1D feature vector\n\\(\\mathsf{A}^M\\): Output of the final convolutional layer\nThis flattened vector then serves as input to the MLP component:\n\\[\n\\mathbf{a}^0_{\\text{MLP}} = \\mathbf{a}^{\\text{flat}}\n\\]",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>40</span>¬† <span class='chapter-title'>8.1 Deep Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-1.html#cnn-backpropagation",
    "href": "private/lecture8/lecture8-1.html#cnn-backpropagation",
    "title": "8.1 Deep Learning",
    "section": "CNN Backpropagation",
    "text": "CNN Backpropagation\nDuring CNN backpropagation, we compute gradients of the loss function \\(F\\) with respect to various intermediate variables.\n\\[\n\\mathsf{dA}^m \\equiv \\frac{\\partial F}{\\partial \\mathsf{A}^m}, \\quad\n\\mathsf{dZ}^m \\equiv \\frac{\\partial F}{\\partial \\mathsf{Z}^m}, \\quad\n\\mathbf{dW}^m \\equiv \\frac{\\partial F}{\\partial \\mathbf{W}^m}, \\quad\n\\mathbf{dB}^m \\equiv \\frac{\\partial F}{\\partial \\mathbf{B}^m}\n\\]\n\n\n\\(\\mathsf{A}^m\\): Activated output of layer \\(m\\)\n\\(\\mathsf{Z}^m\\): Pre-activation output of layer \\(m\\) (before activation function)\n\\(\\mathbf{W}^m\\): Convolutional kernels of layer \\(m\\)\n\\(\\mathbf{B}^m\\): Bias tensor of layer \\(m\\)\n\nCNN Weight and Bias Updates\nThe convolutional weights are updated using gradient descent as follows:\n\\[\n\\mathbf{W}_{k+1}^m = \\mathbf{W}_k^m - \\alpha \\, \\mathbf{dW}^m\n\\]\n\n\n\\(\\mathbf{W}_k^m\\): Convolutional filter(s) of layer \\(m\\) at iteration \\(k\\)\n\\(\\mathbf{W}_{k+1}^m\\): Updated filter(s) at iteration \\(k+1\\)\n\\(\\mathbf{dW}^m\\): Gradient of the loss w.r.t. the weights of layer \\(m\\)\n\\(\\alpha\\): Learning rate\nThe biases are updated similarly:\n\\[\n\\mathbf{B}_{k+1}^m = \\mathbf{B}_k^m - \\alpha \\, \\mathbf{dB}^m\n\\]\n\n\n\\(\\mathbf{B}_k^m\\): Bias tensor of layer \\(m\\) at iteration \\(k\\)\n\\(\\mathbf{B}_{k+1}^m\\): Updated bias tensor at iteration \\(k+1\\)\n\\(\\mathbf{dB}^m\\): Gradient of the loss w.r.t. the biases of layer \\(m\\)\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nAssume that our state tensor \\(\\mathsf{S}\\) is an image.\nBased on your intuition, do you think it makes sense to use raw pixels as input instead of preprocessed state features?\n\n\n\n\\(\\mathsf{S} =\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nIntuitively, using raw pixels seems counterintuitive for reinforcement learning because individual pixel values have no inherent meaning about state quality - traditional RL features like ‚Äúlocation in GridWorld map‚Äù or ‚Äúposition and velocity‚Äù seem much more informative for value estimation than RGB intensities.\nHowever, neural networks can help us learn useful representations directly from raw pixels. By stacking multiple layers, neural networks can automatically extract hierarchical features‚Äîsuch as edges, shapes, and objects‚Äîfrom the image input. This enables the agent to learn which aspects of the raw state are important for decision making, without requiring manual feature engineering.\n\n\n\n\n\n\n\nHagan, Martin T., Howard B. Demuth, Mark H. Beale, and Orlando De Jes√∫s. 2014. Neural Network Design. 2nd ed. Martin Hagan. https://hagan.okstate.edu/NNDesign.pdf.\n\n\nMartin T. Hagan, Amir Jafari. 2024. ‚ÄúNNDesignDeepLearning.‚Äù https://github.com/NNDesignDeepLearning/NNDesignDeepLearning.",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>40</span>¬† <span class='chapter-title'>8.1 Deep Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture8/lecture8-2.html",
    "href": "private/lecture8/lecture8-2.html",
    "title": "8.2 Deep Q-Networks (DQN)",
    "section": "",
    "text": "What if a neural network could learn to act ‚Äî one game, one pixel, one reward at a time? üïπÔ∏è\n\n\n\nNeural networks \\(\\theta\\) such as MLPs and CNNs can help us learn representations of state tensors \\(\\mathsf{S}\\).\n\n\n\n\\(\\mathsf{S} =\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningProblem\n\n\n\nNow we just need an algorithm that:\n\nLeverages neural networks \\(\\theta\\).\nLeverages a classical Reinforcement Learning method (TD: Q-learning).\nImplements batch form.\nEmpirically performs well.\n\n\n\n\n\n\n\n\n\nNoteExample: Breakout\n\n\n\nConsider the following Atari Breakout environment (Bellemare et al. 2013):\n\n\n\nSuppose we set our batch to \\(4\\).\nThe state information \\(\\mathsf{S}_{\\text{batch}}\\) is typically represented as a stack of four consecutive tensor image frames \\(\\mathsf{S}\\), capturing temporal dynamics and motion across time:\n\\[\n\\mathsf{S}_{\\text{batch}} = \\begin{bmatrix}\n\\mathsf{S}_t \\gets \\text{frame at time } t \\\\\n\\mathsf{S}_{t-1} \\gets \\text{frame at time } t-1 \\\\\n\\mathsf{S}_{t-2} \\gets \\text{frame at time } t-2 \\\\\n\\mathsf{S}_{t-3} \\gets \\text{frame at time } t-3 \\\\\n\\end{bmatrix}\n\\]\nThe environment has a discrete action space \\(\\mathcal{A}\\):\n\\[\n\\mathcal{A} = \\{0 \\gets \\text{Do nothing}, 1 \\gets \\text{Fire}, 2 \\gets \\text{Move right}, 3 \\gets \\text{Move left}\\}\n\\]\nThe environment‚Äôs state transition dynamics \\(P(s‚Äô, r \\mid s, a)\\) are governed by a physics engine that updates the positions of the ball, paddle, and bricks based on collisions and the selected action. These dynamics are deterministic but complex due to pixel-based changes.\nThe reward \\(R\\) is defined by the number of bricks destroyed:\n\nHitting and destroying a brick typically yields a reward of \\(+1\\).\nMissing the ball (letting it fall) typically ends a life but gives \\(0\\) reward.\n\nThe episode ends \\(d\\) (dones) if either of the following happens:\n\nTermination: The agent loses all lives (typically 5).\nTruncation: The agent clears all bricks or reaches a built-in time limit (varies by implementation).\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nIs there a mistake with the following loss function for DQN?\n\\[\nF(\\theta_{t}) = (\\text{TD-Target}_{j} - \\hat{Q}(S_{j+1},a; \\theta_{t}))^{2}\n\\]\nWhy does this suggest that we are updating the MSE of a scalar and a vector?\nWhat is actually happening here during the gradient update step?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nBellemare, Marc G, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. ‚ÄúThe Arcade Learning Environment: An Evaluation Platform for General Agents.‚Äù Journal of Artificial Intelligence Research 47: 253‚Äì79.",
    "crumbs": [
      "Lecture 8: Deep Q-Networks",
      "<span class='chapter-number'>41</span>¬† <span class='chapter-title'>8.2 Deep Q-Networks (DQN)</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lo.html",
    "href": "private/lecture9/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 9: Policy Gradients üéØ\n\n\n\n\nPolicy Gradients.\nCartPole-v1 environment using gymnasium & tensorflow.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>42</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lecture9-1.html",
    "href": "private/lecture9/lecture9-1.html",
    "title": "9.1 Policy Gradient Theorem",
    "section": "",
    "text": "Stochastic Gradient Ascent\nDQN is unstable and does not guarantee convergence.\nFollowing deterministic or \\(\\epsilon\\)-soft policies is not always optimal.\nPolicy gradient algorithms search for a local maximum in \\(V^{\\pi_{\\theta}}(s)\\) using stochastic gradient ascent (SGA):\n\\[\n\\Delta \\theta = \\alpha \\nabla_{\\theta} V(S_{0}; \\theta)\n\\]",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>43</span>¬† <span class='chapter-title'>9.1 Policy Gradient Theorem</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lecture9-1.html#stochastic-gradient-ascent",
    "href": "private/lecture9/lecture9-1.html#stochastic-gradient-ascent",
    "title": "9.1 Policy Gradient Theorem",
    "section": "",
    "text": "\\(S_0\\) denotes the starting state of the process or episode. Analyzing the value at the initial state is important because it represents the expected return starting from the beginning of an episode. As opposed to a general state \\(s\\).",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>43</span>¬† <span class='chapter-title'>9.1 Policy Gradient Theorem</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lecture9-1.html#policy-gradient-theorem",
    "href": "private/lecture9/lecture9-1.html#policy-gradient-theorem",
    "title": "9.1 Policy Gradient Theorem",
    "section": "Policy Gradient Theorem",
    "text": "Policy Gradient Theorem\nAssume \\(\\pi\\) is differentiable where it is non-zero.\nIdeally, we want to compute the gradient \\(\\nabla_{\\theta} V(S_{0}; \\theta)\\) analytically.\n\n\nTo derive something analytically means to find an exact mathematical expression for it, rather than estimating it through sampling or approximation.\n\\[\nV(S_{0}; \\theta) = \\sum_{a} \\pi(a|S_{0}; \\theta) Q(S_{0},a; \\theta)\n\\]\n\nStep 1: Express Value Function in Terms of Trajectories\nThe value function \\(V(S_{0}; \\theta)\\) can also be expressed in terms of trajectories \\(\\tau\\):\n\\[\n\\begin{align*}\n    V(S_{0}; \\theta) &= \\sum_{a} \\pi(a|S_{0}; \\theta) Q(S_{0},a; \\theta) \\\\\n    &= \\sum_{\\tau} \\underbrace{P(\\tau; \\theta)}_{\\text{Probability of Trajectory}} \\ \\underbrace{R(\\tau)}_{\\text{Reward of Trajectory}}\n\\end{align*}\n\\]\n\n\nRecall that a trajectory \\(\\tau\\) is the tuple.\n\\(\\tau = (S_{0},A_{0},R_{1}, ..., S_{T-1},A_{T-1},R_{T})\\)\n\n\nStep 2: Leverage Likelihood Ratios\nNow we can take the gradient with respect to neural network parameters \\(\\theta\\) using likelihood ratios:\n\\[\n\\nabla_\\theta V(\\theta) = \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\nabla_\\theta \\text{log} P(\\tau; \\theta)\n\\]\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\\[\n\\begin{align*}\n    \\nabla_\\theta V(\\theta) &= \\nabla_\\theta \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\\\\n    &= \\sum_{\\tau} \\nabla_\\theta P(\\tau; \\theta) R(\\tau) \\quad \\gets \\text{Swap} \\sum \\text{and} \\nabla \\\\\n    &= \\sum_{\\tau} R(\\tau) \\nabla_\\theta P(\\tau; \\theta) \\quad \\gets \\text{Switch the order of multiplication} \\\\\n    &= \\sum_{\\tau} R(\\tau) \\nabla_\\theta P(\\tau; \\theta) \\frac{P(\\tau; \\theta)}{P(\\tau; \\theta)} \\quad \\gets \\text{Multiply by} \\ 1 \\\\\n    &= \\sum_{\\tau} R(\\tau) P(\\tau; \\theta)  \\nabla_\\theta \\text{log} P(\\tau; \\theta) \\quad \\gets \\text{Likelihood Ratio} \\ \\frac{\\nabla_\\theta P(\\tau; \\theta)}{P(\\tau; \\theta)} = \\nabla_\\theta \\text{log} P(\\tau; \\theta)\\\\\n    &= \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\nabla_\\theta \\text{log} P(\\tau; \\theta)\n\\end{align*}\n\\]\n\n\n\n\n\nStep 3: Approximate Empirical Gradient\nWe can approximate the expectation using an empirical estimate for \\(m\\) sample trajectories:\n\\[\n\\nabla_{\\theta} V(\\theta) \\approx \\hat{g} = \\frac{1}{m} \\sum^{m}_{i = 1} R(\\tau^{i}) \\nabla_{\\theta} \\log P(\\tau^{i}; \\theta)\n\\]\nProblem is that we do not necessarily know the dynamics of trajectories \\(\\nabla_\\theta \\text{log} P(\\tau^{i}; \\theta)\\)\n\n\nStep 4: Decompose Dynamic Function\nLuckily we can decompose the dynamics term into states and actions:\n\\[\n\\nabla_\\theta \\text{log} P(\\tau^{i}; \\theta) = \\sum^{T-1}_{t=0} \\text{log} \\nabla_\\theta \\pi(A_{t}|S_{t}, \\theta)\n\\]\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\\[\n\\begin{align*}\n    \\hspace{-0.7cm} \\nabla_\\theta \\text{log} P(\\tau^{i}; \\theta) &= \\nabla_\\theta \\text{log} [\\underbrace{\\mu(S_{0})}_{\\text{Initial State}} \\prod^{T-1}_{t=0} \\underbrace{\\pi(A_{t}|S_{t};\\theta)}_{\\text{Policy}} \\underbrace{P(S_{t+1}|S_{t},A_{t})}_{\\text{Dynamic Function}}] \\\\\n    \\hspace{-0.7cm} &= \\nabla_\\theta [\\text{log}\\mu(S_{0}) + \\text{log}\\sum^{T-1}_{t=0} \\pi(A_{t}|S_{t};\\theta) + \\text{log} P(S_{t+1}|S_{t},A_{t})] \\gets \\text{Distribute log} \\\\\n    \\hspace{-0.7cm} &= \\underbrace{\\nabla_\\theta \\text{log}\\mu(S_{0})}_{=0} + \\nabla_\\theta \\text{log}\\sum^{T-1}_{t=0} \\pi(A_{t}|S_{t};\\theta) + \\underbrace{\\nabla_\\theta \\text{log} P(S_{t+1}|S_{t},A_{t})}_{=0} \\\\\n    \\hspace{-0.7cm} &= \\sum^{T-1}_{t=0} \\text{log} \\nabla_\\theta \\pi(A_{t}|S_{t}, \\theta)\n\\end{align*}\n\\]\n\n\n\nAnd we arrive to a term that does not depend on any dynamics, which we call the score function.\n\n\n\n\n\n\nTipPolicy Gradient Theorem\n\n\n\nLet \\(\\pi(a|s;\\theta)\\) be a differentiable policy. The gradient of the expected reward \\(F(\\theta)\\) with respect to the policy parameters \\(\\theta\\) is given by:\n\\[\n\\nabla_\\theta F(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi(a|s;\\theta) Q^{\\pi_\\theta}(s, a)\\right]\n\\]",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>43</span>¬† <span class='chapter-title'>9.1 Policy Gradient Theorem</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lecture9-2.html",
    "href": "private/lecture9/lecture9-2.html",
    "title": "9.2 Addressing Sparse Rewards",
    "section": "",
    "text": "What if you had to learn from signals that show up only rarely ‚Äî or not at all for a while? üå´Ô∏è\n\n\n\nBefore we can apply policy gradient methods effectively, we first need to solve the problem of high variance in the reward estimate:\n\\[\n\\hat{g} = \\frac{1}{m} \\sum^{m}_{i = 1} R(\\tau^{i}) \\sum^{T-1}_{t=0} \\text{log} \\nabla_\\theta \\pi(A_{t}|S_{t}, \\theta)\n\\]\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we learn effective policies when reward signals are sparse and only received at the end of long trajectories?\n\n\n\n\n\n\n\n\nNoteExample: CartPole\n\n\n\nConsider the following CartPole environment (Barto, Sutton, and Anderson 1983):\n\n\n\nThe state information \\(\\mathbf{s}\\) is now the following vector:\n\\[\n\\mathbf{s} = \\begin{bmatrix} cp \\in (-4.8,4.8) \\gets \\text{Cart Position} \\\\ cv \\in (-\\inf,\\inf) \\gets \\text{Cart Velocity} \\\\ pa \\in (\\approx -0.418 \\ \\text{rad}(-24¬∞),\\approx 0.418 \\ \\text{rad}(-24¬∞)) \\gets \\text{Pole Angle} \\\\ pav_t \\in (-\\inf,\\inf) \\gets \\text{Pole Angular Velocity} \\\\ \\end{bmatrix}\n\\]\nThe environment has a discrete action space \\(\\mathcal{A}\\):\n\\[\n\\mathcal{A} = \\{0 \\gets \\text{Push cart to the left}, 1 \\gets \\text{Push cart to the right}\\}\n\\]\nThe environment‚Äôs state transition dynamics \\(P(s‚Äô, r \\mid s, a)\\) are calculated using classical mechanics and Euler Method:\n\\[\n\\begin{align}\n    g &= 9.8 \\quad \\gets \\text{gravity} \\\\\n    m &= 0.1 \\quad \\gets \\text{pole mass} \\\\\n    M &= 1.0 \\quad \\gets \\text{cart mass} \\\\\n    l &= 0.5 \\quad \\gets \\text{half pole length} \\\\\n    \\tau &= 0.02 \\quad \\gets \\text{time step} \\\\\n\\end{align}\n\\]\nAngular Acceleration:\n\\[\n\\ddot{pa_t} = \\frac{\ng \\sin(pa_t) + \\cos(pa_t) \\left(-\\text{action} - m \\cdot l \\cdot pav_t^2 \\sin(pa_t)\\right)/(M + m)\n}{\nl \\left(\\frac{4}{3} - \\frac{m \\cos^2(pa_t)}{M + m} \\right)\n}\n\\]\nCart acceleration:\n\\[\n\\ddot{cv_t} = \\frac{\n\\text{action} + m \\cdot l \\left(pav_t^2 \\sin(pa_t) - \\ddot{p}_a \\cos(pa_t)\\right)\n}{M + m}\n\\]\nEuler integration:\n\\[\n\\begin{align}\ncp_{t+1} &= cp_t + \\tau \\cdot cv_t \\\\\ncv_{t+1} &= cv_t + \\tau \\cdot \\ddot{cv_t} \\\\\npa_{t+1} &= pa_t + \\tau \\cdot pav_t \\\\\npav_{t+1} &= pav_t + \\tau \\cdot \\ddot{pa_t}\n\\end{align}\n\\]\nDefault Reward Function:\nSince the goal is to keep the pole upright for as long as possible, by default, a reward of \\(+1\\) is given for every step taken, including the termination step. The default reward threshold is \\(500\\) for v1 and \\(200\\) for v0 due to the time limit on the environment.\nSparse Rewards:\nIf sutton_barto_reward=True, then a reward of \\(0\\) is awarded for every non-terminating step and \\(-1\\) for the terminating step. As a result, the reward threshold is \\(0\\) for v0 and v1.\nThe episode ends \\(d\\) (dones) if either of the following happens:\n\nTermination: Pole Angle is greater than \\(\\pm12¬∞\\).\nTermination: Cart Position is greater than \\(\\pm2.4\\) (center of the cart reaches the edge of the display).\nTruncation: Episode length is greater than \\(500\\) (\\(200\\) for v0).\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarto, Andrew G, Richard S Sutton, and Charles W Anderson. 1983. ‚ÄúNeuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.‚Äù Technical Report, Institute for Cybernetic Studies, University of Massachusetts. https://psycnet.apa.org/record/1984-25798-001.",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>44</span>¬† <span class='chapter-title'>9.2 Addressing Sparse Rewards</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lecture9-3.html",
    "href": "private/lecture9/lecture9-3.html",
    "title": "9.3 Action Selections",
    "section": "",
    "text": "What if every choice you made wasn‚Äôt based on value ‚Äî but came straight from your instinct? üé≤\n\n\n\nNow that we have a robust empirical gradient:\n\\[\n\\hat{g} = \\frac{1}{m} \\sum^{m}_{i = 1}  \\sum^{T-1}_{t=0} \\log \\nabla_\\theta \\pi(A_{t}|S_{t}, \\theta) \\ \\hat{A}_{t}\n\\]\n\n\n\n\n\n\nWarningProblem\n\n\n\nIf the policy \\(\\pi(A_{t}|S_{t}, \\theta)\\) is parametrized by a neural network \\(\\theta\\), how do we select actions in action spaces \\(\\mathcal{A}\\) that are discrete or continuous?\nHint: Think about Lecture 8\n\n\n\n\n\n\n\n\nNoteExample: Pusher\n\n\n\nConsider the following Pusher MuJoCo environment (Todorov, Erez, and Tassa 2012):\n\n\n\nThe environment has a continuous action space \\(\\mathcal{A}\\):\n\\[\n\\mathcal{A} = \\begin{bmatrix} \\text{Rotation of the panning the shoulder} \\in (-2,2) \\\\\n                              \\text{Rotation of the shoulder lifting joint} \\in (-2,2) \\\\\n                              \\text{Rotation of the shoulder rolling joint} \\in (-2,2) \\\\\n                              \\text{Rotation of hinge joint that flexed the elbow} \\in (-2,2) \\\\\n                              \\text{Rotation of hinge that rolls the forearm} \\in (-2,2) \\\\\n                              \\text{Rotation of flexing the wrist} \\in (-2,2) \\\\\n                              \\text{Rotation of rolling the wrist} \\in (-2,2) \\\\\n                \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodorov, Emanuel, Tom Erez, and Yuval Tassa. 2012. ‚ÄúMuJoCo: A Physics Engine for Model-Based Control.‚Äù In *Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. http://www.mujoco.org/.",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>45</span>¬† <span class='chapter-title'>9.3 Action Selections</span>"
    ]
  },
  {
    "objectID": "private/lecture9/lecture9-4.html",
    "href": "private/lecture9/lecture9-4.html",
    "title": "9.4 Vanilla Policy Gradient",
    "section": "",
    "text": "What if you improved your strategy by following the slope ‚Äî no tricks, no constraints, just raw feedback? üßó‚Äç‚ôÇÔ∏è\n\n\n\n\n‚úÖ We calculated the policy gradient analytically.\n‚úÖ We addressed the problem of sparse rewards.\n‚úÖ We know how to select actions in discrete and continuous action spaces.\n\n\n\n\n\n\n\nWarningProblem\n\n\n\nNow we just need an algorithm that:\n\nLeverages neural networks \\(\\theta\\).\nLeverages a classical Reinforcement Learning method (Monte Carlo).\nLeverages our empirical estimate of the gradient \\(\\hat{g}\\).\nEmpirically performs well.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion ü§î\n\n\n\nMatch the following concepts:\n\n\n\n\n\n\n\nConcept\nNotation\n\n\n\n\nLikelihood Ratio\n\\(\\frac{\\nabla_{\\theta} \\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)}\\)\n\n\nScore Function\n\\(\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s)\\)\n\n\nPolicy Gradient\n\\(\\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi(a|s;\\theta) Q^{\\pi_\\theta}(s, a)\\right]\\)\n\n\nEmpirical Estimate\n\\(\\frac{1}{m} \\sum^{m}_{i = 1}  \\sum^{T-1}_{t=0} \\log \\nabla_\\theta \\pi(A_{t}|S_{t}, \\theta) \\hat{A}_{t}\\)\n\n\nBaseline\n\\(b(s)\\)",
    "crumbs": [
      "Lecture 9: Policy Gradients I",
      "<span class='chapter-number'>46</span>¬† <span class='chapter-title'>9.4 Vanilla Policy Gradient (VPG)</span>"
    ]
  },
  {
    "objectID": "private/lecture10/lo.html",
    "href": "private/lecture10/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 10: Advanced Policy Gradients üéØ\n\n\n\n\nTrust Region Policy Optimization (TRPO).\nProximal Policy Optimization (PPO).\nHalfCheetah-v5 environment using gymnasium & tensorflow.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 10: Policy Gradients II",
      "<span class='chapter-number'>47</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture10/lecture10-1.html",
    "href": "private/lecture10/lecture10-1.html",
    "title": "10.1 Trust Regions",
    "section": "",
    "text": "Trust Regions\nVPG allowed us to leverage our robust empirical gradient \\(\\hat{g}\\):\n\\[\n\\hat{g} = \\frac{1}{m} \\sum^{m}_{i = 1}  \\sum^{T-1}_{t=0} \\log \\nabla_\\theta \\pi(A_{t}|S_{t}, \\theta) \\ \\hat{A}_{t}\n\\]\nTo update the neural network \\(\\theta\\) policy parameters using SGA:\n\\[\n\\theta_{t+1} = \\theta_{t} + \\alpha \\hat{g}\n\\]\nConsider an infinite MDP.\n\\[\ns_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3,...\n\\]",
    "crumbs": [
      "Lecture 10: Policy Gradients II",
      "<span class='chapter-number'>48</span>¬† <span class='chapter-title'>10.1 Trust Regions</span>"
    ]
  },
  {
    "objectID": "private/lecture10/lecture10-1.html#trust-regions",
    "href": "private/lecture10/lecture10-1.html#trust-regions",
    "title": "10.1 Trust Regions",
    "section": "",
    "text": "1. Defining the Preliminary Notation\nWe express \\(\\eta(\\pi)\\) as the expected discounted reward following a stochastic policy \\(\\pi\\) as:\n\\[\n\\eta(\\pi) = \\mathbb{E}_{s_0, a_0, \\dots} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n\\]\nWith minor tweaks, we can add a baseline by substituting with the standard advantage notation:\n\\[\n\\eta(\\pi) = \\mathbb{E}_{s_0, a_0, \\dots} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A_\\pi(s, a) \\right]\n\\]\nThe following useful identity expresses the expected return of another policy \\(\\tilde{\\pi}\\) in terms of the advantage over \\(\\pi\\), accumulated over timesteps:\n\\[\n\\eta(\\tilde{\\pi}) = \\eta(\\pi) + \\mathbb{E}_{s_0, a_0, \\dots \\sim \\tilde{\\pi}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A_\\pi(s_t, a_t) \\right]\n\\]\n\nAdditionally, let \\(\\rho_{\\pi}(s)\\) be the discounted visitation frequencies following \\(\\pi\\):\n\\[\n\\rho_{\\pi}(s) = P(s_{0}) + \\gamma P(s_{1}) + \\gamma^{2} P(s_{2}) + ...\n\\]\nNow we can rewrite \\(\\eta(\\tilde{\\pi})\\) equation to sum over states instead of timesteps by introducing \\(\\rho_{\\pi}(s)\\):\n\\[\n\\eta(\\tilde{\\pi}) = \\eta(\\pi) + \\sum_s \\rho_{\\tilde{\\pi}}(s) \\sum_a \\tilde{\\pi}(a|s) A_\\pi(s, a)\n\\]\n\nThis equation tells us something important:\nIf we update our policy from \\(\\pi\\) to \\(\\tilde{\\pi}\\) so that, at every state \\(s\\), the expected advantage is nonnegative (i.e., \\(\\sum_a \\tilde{\\pi}(a|s)A_\\pi(s, a) \\geq 0\\)), then the new policy will perform at least as well as the old one. If the expected advantage is strictly positive anywhere, the new policy will do better.\nThis is the same idea as the Policy Improvement Theorem from Lecture 4.\nHowever, in practice, things are not so simple. For approximate or stochastic policies, there may be some states where the expected advantage is negative due to estimation errors or function approximation. This makes it hard to guarantee improvement everywhere, and direct optimization becomes challenging.\n\n\n2. Defining a Local Approximation to \\(\\eta\\)\nTo make things simpler, let‚Äôs use an easier approximation for \\(\\eta\\) that doesn‚Äôt depend on the new policy‚Äôs state visitation frequencies \\(\\rho_{\\tilde{\\pi}}(s)\\).\nWe define a local approximation:\n\\[\nL_\\pi(\\tilde{\\pi}) = \\eta(\\pi) + \\sum_s \\rho_{\\pi}(s) \\sum_a \\tilde{\\pi}(a|s) A_\\pi(s, a)\n\\]\nThis formula is a shortcut: instead of tracking how the new policy changes which states we visit, we just use the old policy‚Äôs frequencies.\nWhy is this useful? If we use a parameterized policy \\(\\pi_\\theta\\), where \\(\\theta\\) are the parameters, this local approximation \\(L\\) matches the true return \\(\\eta\\) at the current parameters:\n\\[\nL_{\\pi_{\\theta_0}}(\\pi_{\\theta_0}) = \\eta(\\pi_{\\theta_0}),\n\\]\nand the gradients also match at \\(\\theta_0\\):\n\\[\n\\nabla_\\theta L_{\\pi_{\\theta_0}}(\\pi_\\theta) \\Big|_{\\theta = \\theta_0} = \\nabla_\\theta \\eta(\\pi_\\theta) \\Big|_{\\theta = \\theta_0}.\n\\]\nSo, if we take a small step from \\(\\theta_0\\) to a new \\(\\tilde{\\theta}\\) that improves \\(L\\), it will also improve the true return \\(\\eta\\)‚Äîat least for small steps.\n\n\n3. Conservative Policy Iteration\nKakade and Langford‚Äôs key insight was to find a guaranteed lower bound on how much your policy can improve after an update.\n\\[\n\\eta(\\pi_{\\text{new}}) \\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{2 \\epsilon \\gamma}{(1 - \\gamma)^2} \\alpha^2\n\\]\nwhere\n\\[\n\\epsilon = \\max_s \\left| \\mathbb{E}_{a \\sim \\pi_0(a|s)} \\left[ A_\\pi(s, a) \\right] \\right|\n\\]\nThis means: if you use this formula, you can be sure your new policy will do at least as well as the right-hand side, no matter what.\nHowever, this guarantee only works for a special kind of policy update called a ‚Äúmixture policy,‚Äù which is just a weighted average of your old policy and a new candidate policy:\n\\[\n\\pi_{\\text{new}}(a|s) = (1 - \\alpha)\\,\\pi_{\\text{old}}(a|s) + \\alpha\\,\\pi^{'}(a|s)\n\\]\nIn other words, you don‚Äôt jump all the way to a new policy‚Äîyou blend a little bit of the new policy into the old one, controlled by \\(\\alpha\\).",
    "crumbs": [
      "Lecture 10: Policy Gradients II",
      "<span class='chapter-number'>48</span>¬† <span class='chapter-title'>10.1 Trust Regions</span>"
    ]
  },
  {
    "objectID": "private/lecture10/lecture10-2.html",
    "href": "private/lecture10/lecture10-2.html",
    "title": "10.2 Monotonic Improvement",
    "section": "",
    "text": "Monotonic Improvement\nWe defined a local approximation to \\(\\eta\\), the expected return of another policy \\(\\tilde{\\pi}\\) in terms of the advantage over \\(\\pi\\)\n\\[\nL_\\pi(\\tilde{\\pi}) = \\eta(\\pi) + \\sum_s \\rho_{\\pi}(s) \\sum_a \\tilde{\\pi}(a|s) A_\\pi(s, a)\n\\]\nThis approximation helped us find a lower bound of how much \\(\\pi\\) could improve after an update:\n\\[\n\\eta(\\pi_{\\text{new}}) \\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{2 \\epsilon \\gamma}{(1 - \\gamma)^2} \\alpha^2\n\\]\nwhere\n\\[\n\\epsilon = \\max_s \\left| \\mathbb{E}_{a \\sim \\pi_0(a|s)} \\left[ A_\\pi(s, a) \\right] \\right|\n\\]\nHowever, conservative policy iteration only works on a mixture of policies.\n\\[\n\\pi_{\\text{new}}(a|s) = (1 - \\alpha)\\,\\pi_{\\text{old}}(a|s) + \\alpha\\,\\pi^{'}(a|s)\n\\]\nTo guarantee monotonic improvement, we need to find a way to extend conservative policy iteration to general stochastic policies rather than mixture policies.\nTo do this we replace \\(\\alpha\\) with a distance measure between \\(\\pi\\) and \\(\\tilde{\\pi}\\), and changing the constant \\(\\epsilon\\) appropriately.",
    "crumbs": [
      "Lecture 10: Policy Gradients II",
      "<span class='chapter-number'>49</span>¬† <span class='chapter-title'>10.2 Monotonic Improvement</span>"
    ]
  },
  {
    "objectID": "private/lecture10/lecture10-2.html#monotonic-improvement",
    "href": "private/lecture10/lecture10-2.html#monotonic-improvement",
    "title": "10.2 Monotonic Improvement",
    "section": "",
    "text": "1. Bounding with KL Divergence\nThe distance measure used is KL Divergence for discrete probability distributions (as illustrated in the demo above):\n\\[\nD_{TV}(p||q) = \\frac{1}{2} \\sum_{i}|p_i - q_i|\n\\]\nParticularly, to solve the issue of the mixture policies we introduce the distance measure as the total variation divergence between two policies:\n\\[\nD_{TV}(\\pi||\\tilde{\\pi}) = \\text{max}_s D_{TV}(\\pi(\\cdot | s) || \\tilde{\\pi}(\\cdot | s))\n\\]\n\n\n\n\n\n\nTipMonotonic Improvement Theorem\n\n\n\nLet \\(\\alpha = D^{\\text{max}}_{TV}(\\pi||\\tilde{\\pi})\\). Then the following bound holds:\n\\[\n\\eta(\\pi_{\\text{new}}) \\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{4 \\epsilon \\gamma}{(1 - \\gamma)^2} \\alpha^2\n\\]\nwhere\n\\[\n\\epsilon = \\max_{s,a} \\left|  A_\\pi(s, a) \\right|\n\\]\n\n\n\n\n\n2. Surrogate Objectives\nAll that is left to do now is to maximize our surrogate objective \\(L_{\\theta_{\\text{old}}}\\) subject to the trust region constraint:\n\\[\nD_{TV}(\\pi_{\\theta_{\\text{old}}}(\\cdot | s) || \\pi_{\\theta}(\\cdot | s)) \\leq \\delta\n\\]\nThis leads to the following optimization problem:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\max_{\\theta} \\quad &\n\\mathbb{E}_{s \\sim \\rho_{\\theta_{\\text{old}}}, \\, a \\sim q}\n\\left[\n\\frac{\\pi_{\\theta}(a|s)}{q(a|s)} Q_{\\theta_{\\text{old}}}(s,a)\n\\right] \\\\\n\\text{subject to} \\quad &\n\\mathbb{E}_{s \\sim \\rho_{\\theta_{\\text{old}}}}\n\\left[ D_{\\text{KL}} \\!\\left(\n\\pi_{\\theta_{\\text{old}}}(\\cdot|s) \\,\\|\\, \\pi_{\\theta}(\\cdot|s)\n\\right) \\right] \\leq \\delta.\n\\end{aligned}\n\\end{equation}\n\\]\nThe procedure for solving this optimization can be summarized as follows:\n\nCollect data: Sample state‚Äìaction pairs \\((s,a)\\) together with Monte Carlo estimates of their Q-values.\nEstimate the objective: Use these samples to form empirical estimates of the surrogate objective.\nOptimize under constraints: Approximately solve the constrained optimization problem to update the policy parameters \\(\\theta\\), typically using the conjugate gradient method.",
    "crumbs": [
      "Lecture 10: Policy Gradients II",
      "<span class='chapter-number'>49</span>¬† <span class='chapter-title'>10.2 Monotonic Improvement</span>"
    ]
  },
  {
    "objectID": "private/lecture10/lecture10-3.html",
    "href": "private/lecture10/lecture10-3.html",
    "title": "10.3 Proximal Policy Optimization (PPO)",
    "section": "",
    "text": "What if you could move fast ‚Äî but with guardrails that keep you from tipping over? üéõÔ∏è\n\n\n\nTRPO‚Äôs main drawback has to do with the calculation of the Hessian matrix with respect to the KL-Divergence:\n\\[\n\\mathbf{H} = \\nabla^2 D_{KL}(\\pi_{\\theta_{t}} \\| \\pi_{\\theta_{t+1}})\n\\]\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we design an algorithm that achieves stable policy updates like TRPO, but avoids the computational complexity of calculating the Hessian matrix? Could clipping the probability ratio be a simpler yet effective solution?\n\n\n\n\nviewof r = Inputs.range([0, 2], {step: 0.01, value: 1.2, label: tex`r = \\frac{\\pi_\\theta}{\\pi_{\\text{old}}}`, width: 250})\nviewof A = Inputs.range([-2, 2], {step: 0.1, value: 1.0, label: tex`\\hat{A}`, width: 250})\nviewof eps = Inputs.range([0, 0.5], {step: 0.01, value: 0.2, label: tex`\\epsilon`, width: 250})\n\n// Function to compute PPO clipped objective at a single ratio\nfunction ppoClipObj(ratio, advantage, epsilon) {\n  const clip_ratio = Math.min(1 + epsilon, Math.max(1 - epsilon, ratio))\n  if (advantage &gt;= 0) {\n    return Math.min(ratio * advantage, clip_ratio * advantage)\n  } else {\n    return Math.max(ratio * advantage, clip_ratio * advantage)\n  }\n}\n\n// Compute current objective at slider r\nobjective = ppoClipObj(r, A, eps)\n\n// Data for visualization\nratio_values = d3.range(0, 2.01, 0.01)\ncurve_data = ratio_values.map(rr =&gt; ({\n  r: rr,\n  unclipped_obj: rr * A,\n  clipped_obj: ppoClipObj(rr, A, eps)\n}))\n\n// Plot the surrogate objective vs ratio\nPlot.plot({\n  style: \"overflow: visible; display: block; margin: 0 auto;\",\n  width: 600,\n  height: 400,\n  y: {grid: true, label: \"Objective Value\"},\n  x: {label: \"Probability Ratio r\", domain: [0,2]},\n  marks: [\n    Plot.line(curve_data, {x: \"r\", y: \"unclipped_obj\", stroke: \"steelblue\", strokeWidth: 2, label: \"Unclipped\"}),\n    Plot.line(curve_data, {x: \"r\", y: \"clipped_obj\", stroke: \"orange\", strokeWidth: 2, label: \"Clipped\"}),\n    Plot.ruleX([1 - eps, 1 + eps], {stroke: \"red\", strokeDasharray: \"4,4\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"text-align: center; margin-top: 1em;\"&gt;\n  ${tex.block`L^{\\text{CLIP}}(\\theta) = \n    ${A &gt;= 0 ? \"\\\\min(r\\\\hat{A}, \\\\text{clip}(r,1-\\\\epsilon,1+\\\\epsilon)\\\\hat{A})\" : \n                \"\\\\max(r\\\\hat{A}, \\\\text{clip}(r,1-\\\\epsilon,1+\\\\epsilon)\\\\hat{A})\"}`}\n  ${tex.block`= ${objective.toFixed(3)}`}\n  &lt;p&gt;${tex`\\text{Red dashed lines show the clipping range } [1-\\epsilon, 1+\\epsilon]`}&lt;/p&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution",
    "crumbs": [
      "Lecture 10: Policy Gradients II",
      "<span class='chapter-number'>50</span>¬† <span class='chapter-title'>10.3 Proximal Policy Optimization (PPO)</span>"
    ]
  },
  {
    "objectID": "private/lecture11/lo.html",
    "href": "private/lecture11/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 11: Monte Carlo Tree Search üéØ\n\n\n\n\nLet‚Äôs talk about Exam 2!\nMonte Carlo Tree Search (MCTS).\nAdvanced Monte Carlo Tree Search.\nCartPole-v0 environment using gymnasium & tensorflow.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 11: Monte Carlo Tree Search",
      "<span class='chapter-number'>51</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture11/lecture11-1.html",
    "href": "private/lecture11/lecture11-1.html",
    "title": "11.1 Model-based Reinforcement Learning",
    "section": "",
    "text": "What if you could practice in your imagination before stepping into the real world? üß©\n\n\n\nIn Lectures 5-10, we explored Model-free Reinforcement Learning.\nModel-free Reinforcement Learning emphasizes learning directly from interactions with the environment without relying on a model of its dynamics.\n\n\n\n\n\n\n\nAgents need to sample many environment interactions to learn environment dynamics.\n\\[\nP(s^{'}, r| s, a)\n\\]\nExploration is blind without a model of environment dynamics. Model-free methods focus on immediate rewards.\n\n\n\n\n\n\nWarningProblem\n\n\n\nBut what if agents could predict the outcomes of their actions without directly interacting with the environment? Could this lead to more efficient learning?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nThink about how we humans often plan.\nFor example, imagine you are about to graduate from GW. You could take two possible actions:\n\n\n\n\n\nAction \\(A_1\\): Consulting Job. Reward \\(R\\): This path has consistently provided the highest immediate payoff ‚Äî strong career growth and financial stability make it the best-known choice.\n\n\n\n\n\n\n\nAction \\(A_2\\): PhD Program. Reward \\(R\\): A modest performer so far, but not explored much. With more investment, it could reveal higher potential in research opportunities and long-term impact.\n\n\n\n\n\nThe outcomes of each choice are not immediately visible ‚Äî you can‚Äôt just try both and ‚Äúreset.‚Äù Instead, you have to simulate in your head what the future might look like based on your prior knowledge and expectations:\n\nWhat career growth might the consulting job bring?\n\nWhat opportunities could the PhD open up?\n\nHow long will each path take?\n\nUnlike trial-and-error learning, where feedback comes directly from experience, here you are relying on a mental model of the world to forecast what might happen and make a choice.\n\n\n\n\n\n\n\n\nTipSolution",
    "crumbs": [
      "Lecture 11: Monte Carlo Tree Search",
      "<span class='chapter-number'>52</span>¬† <span class='chapter-title'>11.1 Model-based Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture11/lecture11-2.html",
    "href": "private/lecture11/lecture11-2.html",
    "title": "11.2 Monte Carlo Tree Search (MCTS)",
    "section": "",
    "text": "What if you could look ahead ‚Äî but only explore what looks promising? üå≥\n\n\n\nModel-based RL leverages environment dynamics to plan actions.\n\n\n\n\n\n\nWarningProblem\n\n\n\nHow can we focus our search on the most promising actions to make planning more effective?\n\n\n\n\n\n\n\n\nNoteReal Life Example üß†\n\n\n\nSuppose you are playing a game of chess and arrive at a complicated position:\n\n\\(S\\) ‚Äî The current board position in front of you.\n\n\\(A\\) ‚Äî Several possible candidate moves you could make.\n\n\\(R\\) ‚Äî You won‚Äôt know the true reward immediately ‚Äî but you can simulate outcomes by imagining how the game might unfold.\n\n\\(S'...\\) ‚Äî For each action, you mentally ‚Äúroll out‚Äù future positions by predicting your opponent‚Äôs possible replies and continuing a few moves ahead.\n\nYou don‚Äôt need to explore every line to the very end of the game ‚Äî instead, you simulate a sample of promising moves, update your estimates, and bias future simulations toward the moves that look strongest.\n\n\n\n\n\n\\(S\\) Current position on the board ‚Äî you must choose a move.\n\n\n\n\n\n\n\n\\(A_{1,...,27}\\) Candidate moves: knight, pawn, bishop or queen.\n\n\n\n\n\n\n\n\\(A\\) Candidate move selected: knight jump.\n\n\n\n\n\n\n\n\\(S'...\\) Simulated rollouts of each move against possible replies.\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution",
    "crumbs": [
      "Lecture 11: Monte Carlo Tree Search",
      "<span class='chapter-number'>53</span>¬† <span class='chapter-title'>11.2 Monte Carlo Tree Search (MCTS)</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lo.html",
    "href": "private/lecture12/lo.html",
    "title": "Learning Objectives",
    "section": "",
    "text": "Learning Objectives for Lecture 12: Conclusion üéØ\n\n\n\n\nAdvanced Topics in Reinforcement Learning\nIdentify the Reinforcement Learning Application.\nOutlook of Reinforcement Learning.\n\n\n\n\n\nTaxonomy of Reinforcement Learning",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>54</span>¬† <span class='chapter-title'>Learning Objectives</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html",
    "href": "private/lecture12/lecture12-1.html",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "",
    "text": "Imitation Learning\nLearn a policy \\(\\pi(a|s)\\) by mimicking an expert‚Äôs demonstrations, \\((s, a)\\), without requiring explicit reward signals.",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#imitation-learning",
    "href": "private/lecture12/lecture12-1.html#imitation-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "",
    "text": "Application: Autonomous Driving  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#inverse-reinforcement-learning",
    "href": "private/lecture12/lecture12-1.html#inverse-reinforcement-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "Inverse Reinforcement Learning",
    "text": "Inverse Reinforcement Learning\nInfer the reward function \\(R(s, a)\\) given expert trajectories to derive an optimal policy, \\(\\pi^*\\).\n\n  Application: Predicting Driver Behavior and Route Recommendation  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#offline-reinforcement-learning",
    "href": "private/lecture12/lecture12-1.html#offline-reinforcement-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "Offline Reinforcement Learning",
    "text": "Offline Reinforcement Learning\nLearn a policy \\(\\pi(a|s)\\) from a fixed dataset \\(D = \\{(s, a, r, s')\\}\\) without further environment interaction.\n\n  Application: Robotic Manipulation  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#multi-agent-reinforcement-learning",
    "href": "private/lecture12/lecture12-1.html#multi-agent-reinforcement-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "Multi-Agent Reinforcement Learning",
    "text": "Multi-Agent Reinforcement Learning\nOptimize multiple agents‚Äô policies \\(\\pi_i(a|s)\\) interacting in a shared environment, considering cooperation or competition.\n\n  Application: Strategic Game-play in Dota2  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#hierarchical-reinforcement-learning",
    "href": "private/lecture12/lecture12-1.html#hierarchical-reinforcement-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "Hierarchical Reinforcement Learning",
    "text": "Hierarchical Reinforcement Learning\nDecompose tasks into a hierarchy of policies, \\(\\pi_\\text{high}(g|s)\\) for goals and \\(\\pi_\\text{low}(a|s, g)\\) for actions.\n\n  Application: MuJoCo Ant Maze Path Finding  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#multi-objective-reinforcement-learning",
    "href": "private/lecture12/lecture12-1.html#multi-objective-reinforcement-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "Multi-Objective Reinforcement Learning",
    "text": "Multi-Objective Reinforcement Learning\nOptimize a policy \\(\\pi(a|s)\\) under multiple conflicting objectives, \\(\\{R_1, R_2, \\dots\\}\\)\n\n  Application: Resource Allocation  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-1.html#meta-learning",
    "href": "private/lecture12/lecture12-1.html#meta-learning",
    "title": "12.1 Advanced Topics in Reinforcement Learning",
    "section": "Meta Learning",
    "text": "Meta Learning\nTrain agents to quickly adapt to new tasks \\(\\mathcal{T}\\) by optimizing over task distributions \\(p(\\mathcal{T})\\).\n\n  Application: Few Shot Learning  Link to Research Paper",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>55</span>¬† <span class='chapter-title'>12.1 Advanced Topics in Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-2.html",
    "href": "private/lecture12/lecture12-2.html",
    "title": "12.2 Identify the Reinforcement Learning Application",
    "section": "",
    "text": "Task 1: Email Spam Detection",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>56</span>¬† <span class='chapter-title'>12.2 Identify the Reinforcement Learning Application</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-2.html#task-1-email-spam-detection",
    "href": "private/lecture12/lecture12-2.html#task-1-email-spam-detection",
    "title": "12.2 Identify the Reinforcement Learning Application",
    "section": "",
    "text": "TipAnswer",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>56</span>¬† <span class='chapter-title'>12.2 Identify the Reinforcement Learning Application</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-2.html#task-2-customer-segmentation-for-a-retail-store",
    "href": "private/lecture12/lecture12-2.html#task-2-customer-segmentation-for-a-retail-store",
    "title": "12.2 Identify the Reinforcement Learning Application",
    "section": "Task 2: Customer Segmentation for a Retail Store",
    "text": "Task 2: Customer Segmentation for a Retail Store\n\n\n\n\n\n\n\nTipAnswer",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>56</span>¬† <span class='chapter-title'>12.2 Identify the Reinforcement Learning Application</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-2.html#task-3-online-ad-optimization",
    "href": "private/lecture12/lecture12-2.html#task-3-online-ad-optimization",
    "title": "12.2 Identify the Reinforcement Learning Application",
    "section": "Task 3: Online Ad Optimization",
    "text": "Task 3: Online Ad Optimization\n\n\n\n\n\n\n\nTipAnswer",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>56</span>¬† <span class='chapter-title'>12.2 Identify the Reinforcement Learning Application</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-2.html#task-4-warehouse-robot-path-finding",
    "href": "private/lecture12/lecture12-2.html#task-4-warehouse-robot-path-finding",
    "title": "12.2 Identify the Reinforcement Learning Application",
    "section": "Task 4: Warehouse Robot Path-finding",
    "text": "Task 4: Warehouse Robot Path-finding\n\n\n\n\n\n\n\nTipAnswer",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>56</span>¬† <span class='chapter-title'>12.2 Identify the Reinforcement Learning Application</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-2.html#task-5-fine-tuning-llms-with-human-feedback",
    "href": "private/lecture12/lecture12-2.html#task-5-fine-tuning-llms-with-human-feedback",
    "title": "12.2 Identify the Reinforcement Learning Application",
    "section": "Task 5: Fine-tuning LLMs with Human Feedback",
    "text": "Task 5: Fine-tuning LLMs with Human Feedback\n\n\n\n\n\n\n\nTipAnswer",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>56</span>¬† <span class='chapter-title'>12.2 Identify the Reinforcement Learning Application</span>"
    ]
  },
  {
    "objectID": "private/lecture12/lecture12-3.html",
    "href": "private/lecture12/lecture12-3.html",
    "title": "12.3 Outlook of Reinforcement Learning",
    "section": "",
    "text": "üîí Outlook of Reinforcement Learning",
    "crumbs": [
      "Lecture 12: Conclusion",
      "<span class='chapter-number'>57</span>¬† <span class='chapter-title'>12.3 Outlook of Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "private/applications/finance.html",
    "href": "private/applications/finance.html",
    "title": "Finance",
    "section": "",
    "text": "Application: Finance üí°\n\n\n\n\n\n\n\n\n\nNoteFinancial Asset Recommendation using Bandit Algorithm Techniques (Capstone Group 10)\n\n\n\n\n\n\n\n\nThe research paper FAR-Trans: An Investment Dataset for Financial Asset Recommendation (Sanz-Cruzado, Droukas, and McCreadie 2024) provides a public dataset on Financial Asset Recommendations (FAR), composed of a rich collection of user investment transactions.\n\n\nUniversity of Glasgow & National Bank of Greece (2024) (Sanz-Cruzado, Droukas, and McCreadie 2024)  Link to Research Paper\n\nLeveraging this public dataset, the goal of this research project three-fold:\n\nDesigning a bandit framework for FAR.\n\nUnifying a single regret-based metric \\(\\rho_{\\text{empirical}}\\) to benchmark collaborative filtering, non-contextual (\\(\\epsilon\\)-greedy, Upper Confidence Boudary (UCB) and Thompson Sampling), and contextual bandit models (LinUCB).\n\nAddressing the non-stationarity of arms and asset returns, where each arm \\(a \\in \\mathcal{A}\\) represents a unique financial asset.\n\nThe action space \\(\\mathcal{A}\\) consists of the available financial assets:\n\\[\n\\mathcal{A} = \\{ \\text{Asset}_1, \\ \\text{Asset}_2, \\ldots, \\ \\text{Asset}_k \\}\n\\]\nTo organize the features over time, assets, and dimensions, we define a feature tensor \\(\\mathsf{X}\\) as:\n\\[\n\\mathsf{X}_{(T, |\\mathcal{A}|, d)} =\n\\begin{bmatrix}\n[ \\mathbf{x}_{1,1} & \\mathbf{x}_{1,2} & \\cdots & \\mathbf{x}_{1,|\\mathcal{A}|} ] \\\\\n[ \\mathbf{x}_{2,1} & \\mathbf{x}_{2,2} & \\cdots & \\mathbf{x}_{2,|\\mathcal{A}|} ] \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n[ \\mathbf{x}_{T,1} & \\mathbf{x}_{T,2} & \\cdots & \\mathbf{x}_{T,|\\mathcal{A}|} ]\n\\end{bmatrix},\n\\quad\n\\mathbf{x}_{t,a} \\in \\mathbb{R}^{1 \\times d}\n\\]\nThe reward function \\(R\\) is defined as the relative change in closing prices between consecutive time steps:\n\\[\nR = \\frac{\\text{Closing Price}[A_t]_t - \\text{Closing Price}[A_t]_{t-1}}{\\text{Closing Price}[A_t]_{t-1}}\n\\]\nThe empirical regret quantifies the performance gap between the optimal policy and the agent‚Äôs chosen policy. In this case, it would quantify the best relative change in closing price for all assets minus the one experienced:\n\\[\n\\rho_T^\\text{empirical} = \\sum_{t=1}^{T} \\Big( \\max_{k \\in \\{1,\\dots,K\\}} r_t^k - r_t^{A_t} \\Big)\n\\]\n\n\n\n\n\n\nSanz-Cruzado, Javier, Nikolaos Droukas, and Richard McCreadie. 2024. ‚ÄúFAR-Trans: An Investment Dataset for Financial Asset Recommendation.‚Äù https://arxiv.org/abs/2407.08692.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>58</span>¬† <span class='chapter-title'>Finance</span>"
    ]
  },
  {
    "objectID": "private/applications/pseudo-labeling.html",
    "href": "private/applications/pseudo-labeling.html",
    "title": "Pseudo-Labeling",
    "section": "",
    "text": "Application: Pseudo-Labeling üí°\n\n\n\n\n\n\n\n\n\nNoteA Framework for Pseudo-labeling with Deep Reinforcement Learning (Capstone Group 11)\n\n\n\n\n\n\n\n\nSelf-supervised learning is a type of learning in machine learning (alongside supervised and unsupervised learning) where the task is to train a model (usually referred to as the downstream model) to generate supervisory signals for creating labels. Reinforcement learning can be used in this context to leverage sequential decision-making to improve the performance of the downstream model.\nSuppose the dataset selected is CIFAR-10.\nThe state space \\(\\mathcal{S}\\) consists of the state vector \\(\\mathbf{s}\\), which is constructed as a flattened concatenation of:\n\\[\n\\mathbf{s} =\n\\begin{bmatrix}\n\\mathbf{X} \\gets \\text{features at the current timestep} \\\\\n\\text{softmax}(\\mathbf{y}) \\gets \\text{softmax labels predicted by the downstream model} \\\\\n\\mathcal{L} \\gets \\text{current loss value} \\\\\n\\end{bmatrix}\n\\]\nThe action space \\(\\mathcal{A}\\) consists of the dataset labels and an additional option to skip labeling:\n\\[\n\\mathcal{A} = \\{ 0, 1, \\ldots, 9, \\ \\text{skip} \\}\n\\]\nThe piecewise reward function \\(R\\) is defined as:\n\\[\nR =\n\\begin{cases}\n-1 & \\text{if } a = \\text{skip}, \\\\\n\\text{metric}_t - \\text{metric}_{t-1} & \\text{if } a \\neq \\text{skip}.\n\\end{cases}\n\\]\nThe environment dynamics \\(P(s', r \\mid s, a)\\) correspond to transitioning to the next feature in the dataset.\nThe episode ends when the agent has processed all samples in the dataset:\n\nTermination: once the complete length of the dataset has been reached.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>59</span>¬† <span class='chapter-title'>Pseudo-Labeling</span>"
    ]
  },
  {
    "objectID": "private/applications/recommendation-systems.html",
    "href": "private/applications/recommendation-systems.html",
    "title": "Recommendation Systems",
    "section": "",
    "text": "Application: Recommendations Systems üí°\n\n\n\n\n\n\n\n\n\nNoteFairfax County Public Schools (FCPS) Recommendation Systems\n\n\n\nFairfax County Public Schools (FCPS) sales.csv and production.csv preprocessed data enables us to construct recommendation systems leveraging Contextual Multi-Armed Bandits (CMAB).\n\nHealth-Aware School Meal Recommendations (Capstone Group 8)\nUsing FCPS sales.csv:\n\n\n\n\n\nSuppose that our action space \\(\\mathcal{A}\\) is composed of our menu items being served:\n\\[\n\\mathcal{A} \\gets \\{ \\text{Cereal} , ... , \\text{Bagels} \\}\n\\]\nFor a particular date \\(t\\), we restrict our selection of menu items to only those that were actually served (mask):\n\\[\n\\mathcal{A}_{t} \\gets \\{  \\text{Cereal} , \\text{Apples} , \\text{Juice} \\}\n\\]\nFor each available item \\(a \\in \\mathcal{A}_t\\), we construct feature vectors \\(\\mathbf{x}_{t,a}\\) that include nutritional, popularity and school related information:\n\\[\n\\mathbf{x}_{t,a} = \\begin{bmatrix} p \\in \\mathbb{R} \\gets \\text{protein grams (one serving)} \\\\\n                            c \\in \\mathbb{R} \\gets \\text{carbohydrate grams (one serving)} \\\\\n                            f_a \\in \\mathbb{R} \\gets \\text{fats grams (one serving)} \\\\\n                            f_i \\in \\mathbb{R} \\gets \\text{fiber grams (one serving)} \\\\\n                            s \\in \\mathbb{R} \\gets \\text{sugar grams (one serving)} \\\\\n                            h \\in \\mathbb{N} \\gets \\text{historical sales count} \\\\\n                            g \\gets \\text{grade level} \\\\\n                            d \\gets \\text{day of week} \\\\\n                            \\vdots \\end{bmatrix}\n\\]\nWe then compute the parameter estimates \\(\\hat{\\boldsymbol{\\theta}}_a\\) and evaluate the scores \\(\\mathbf{p}_{t,a}\\). The action is selected according to\n\\[\nR_t = \\text{total sales count} + \\lambda \\ \\text{health score}\n\\]\nwhere \\(\\lambda\\) is a penalization factor that controls the trade-off between popularity (sales) and nutritional quality (health). A higher \\(\\lambda\\) places greater emphasis on nutritional value, while a lower \\(\\lambda\\) prioritizes maximizing sales volume.\n\n\nWaste Minimization Recommendations (Capstone Group 1)\nUsing FCPS production.csv:\n\n\n\n\n\nSuppose that our action space \\(\\mathcal{A}\\) is composed of our menu items being served:\n\\[\n\\mathcal{A} \\gets \\{ \\text{Cereal} , ... , \\text{Bagels} \\}\n\\]\nFor a particular date \\(t\\), we restrict our selection of menu items to only those that were actually served (mask):\n\\[\n\\mathcal{A}_{t} \\gets \\{  \\text{Cereal} , \\text{Apples} , \\text{Juice} \\}\n\\]\nFor each available item \\(a \\in \\mathcal{A}_t\\), we construct feature vectors \\(\\mathbf{x}_{t,a}\\) that include nutritional factors and contextual information:\n\\[\n\\mathbf{x}_{t,a} = \\begin{bmatrix} l \\in \\mathbb{N} \\gets \\text{Historical Leftover} \\\\\n                            s \\in \\mathbb{N} \\gets \\text{Historical Served Count} \\\\\n                            g \\gets \\text{grade level} \\\\\n                            d \\gets \\text{day of week} \\\\\n                            \\vdots \\end{bmatrix}\n\\]\nWe then compute the parameter estimates \\(\\hat{\\boldsymbol{\\theta}}_a\\) and evaluate the scores \\(\\mathbf{p}_{t,a}\\). The action is selected according to the negative discarded cost:\n\\[\nR_t = - (\\text{Left Over Cost} + \\text{Discarded Cost})\n\\]\nwhere the bandit aims to minimize waste. A higher (less negative) reward corresponds to items with lower discarded cost, guiding the algorithm to favor menu items that are both consumed and cost-efficient.\n\n\n\n\n\n\n\n\n\nNoteGoogle RecSim\n\n\n\nAs seen in lecture 1, Google provides configurable recommendation system using OpenAI Gymnasium.\n\n  Google Research 2019 (Ie et al. 2019)  Link to Research Paper\n\nTo install recsim simply just run the following pip command:\npip install recsim\nmain.py should run as follows:\n\n\n\n\n\nenvironment.py should contain the following:\n\n\n\n\n\n\nAdaptation to User Preference Drift (Capstone Group 12 & Greedy Policy Crew üí∞)\nDemo of how to run recsim by Google.\n\nEnvironment (Capstone Group 12)\nDemo of how to create a enviornment by Google.\nEnvironment Document Database\n\nDriftingDocument (subclass of recsim.document.AbstractDocument)  This class would hold documents \\(d_i\\) with features: \\(\\mathbf{x}\\) topic, \\(p\\) popularity score, and \\(q\\) quality score.\n\n\\[\nd_i = \\Big( \\mathbf{x}_i, \\; p_i, \\; q_i \\Big)\n\\]\n\nDriftingDocumentSampler (subclass of recsim.document.AbstractDocumentSampler)  This class would generate documents at each time step by sampling their features \\(\\Big( \\mathbf{x}, \\; p, \\; q \\Big)\\) from a distribution.\n\n\\[\n\\mathcal{D}_t = \\{ d_1, d_2, \\dots, d_{N_t} \\}, \\quad d_i \\sim P_D(\\mathbf{x}, p, q)\n\\]\nEnvironment User\n\nDriftingUserState (subclass of recsim.user.AbstractUserState)  This class would store for a particular user \\(\\ u_j \\ \\) at time \\(\\ t\\): \\(\\boldsymbol{\\theta}_{j,t}\\) latent preference vector, \\(\\sigma_{j,t}\\) user satisfaction or engagement score, \\(\\phi_{j,t}\\) fatigue variable, and \\(\\tau_{j,t}\\) current timestep.\n\n\\[\nu_{j,t} = (\\boldsymbol{\\theta}_{j,t}, \\; \\sigma_{j,t}, \\; \\phi_{j,t}, \\; \\tau_{j,t})\n\\]\n\nDriftingUserSampler (subclass of recsim.user.AbstractUserSampler)  This class would generate a population of users \\(\\mathcal{U}_0\\) at time \\(t=0\\):\n\n\\[\n\\mathcal{U}_0 = \\{ u_{1,0}, u_{2,0}, \\dots, u_{M,0} \\}, \\quad u_{j,0} \\sim P_U(\\boldsymbol{\\theta}, \\sigma, \\phi)\n\\]\n\nDriftingResponseModel (subclass of recsim.user.AbstractResponseModel)  This class would define how a user reacts to a document, mapping (user state, document features) into probabilities of click.\n\n\\[\n\\Pr(\\text{click}_{j,i,t} = 1) = \\sigma \\Big( \\boldsymbol{\\theta}_{j,t}^\\top \\mathbf{x}_i + \\beta_1 q_i + \\beta_2 p_i + \\epsilon_{j,i,t} \\Big)\n\\]\n\nDriftingUserModel (subclass of recsim.user.AbstractUserModel)  This class would coordinate the simulation: advancing the user state, calling the response model, and updating preferences via a drift process.\n\n\\[\n\\boldsymbol{\\theta}_{j,t+1} = (1-\\alpha)\\boldsymbol{\\theta}_{j,t} + \\alpha \\mathbf{x}_{i,t} + \\boldsymbol{\\epsilon}_{j,t}\n\\]\n\n\n\n\n\n\n\n\n\nIe, Eugene, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. ‚ÄúRecSim: A Configurable Simulation Platform for Recommender Systems.‚Äù https://arxiv.org/abs/1909.04847.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>60</span>¬† <span class='chapter-title'>Recommendation Systems</span>"
    ]
  },
  {
    "objectID": "private/homework/homework1.html",
    "href": "private/homework/homework1.html",
    "title": "Homework 1",
    "section": "",
    "text": "No Homework! Enjoy! üòä",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>61</span>¬† <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html",
    "href": "private/homework/homework2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\nWrite some of the elements of the following sets:\nWrite the following sets in set notation:",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-1",
    "href": "private/homework/homework2.html#question-1",
    "title": "Homework 2",
    "section": "",
    "text": "\\(\\{ 5x-1: x \\in \\mathbb{Z} \\}\\)\n\\(\\{ x \\in \\mathbb{R}: \\sin \\pi x = 0 \\}\\)\n\\(\\{X : X \\subseteq \\{3,2,a\\} \\text{ and } |X|=2 \\}\\)\n\n\n\n\\(\\{ 2, 4, 8, 16, 32, 64, ...\\}\\)\n\\(\\{0,1,4,9,16,25,36, ...\\}\\)\n\\(\\{..., \\frac{1}{8},\\frac{1}{4},\\frac{1}{2},1,2,4,8,... \\}\\)",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-2",
    "href": "private/homework/homework2.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA retail store accepts either American Express or VISA. The percentages of customers carrying each card are: \n\nAmerican Express: \\(24%\\)\nVISA: \\(61%\\)\nBoth: \\(11%\\)\n\nWhat percentage of customers carry a card accepted by the store?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-3",
    "href": "private/homework/homework2.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSixty percent of students wear neither a ring nor a necklace. Given: \n\n\\(20%\\) wear a ring\n\\(30%\\) wear a necklace\n\nFind the probability that a randomly chosen student wears:\n\nA ring or a necklace\nBoth a ring and a necklace",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-4",
    "href": "private/homework/homework2.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nTwo fair dice are rolled. Find the conditional probability that at least one die lands on \\(6\\), given that they land on different numbers.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-5",
    "href": "private/homework/homework2.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nAn urn contains \\(6\\) white and \\(9\\) black balls. If \\(4\\) balls are selected without replacement, what is the probability that the first \\(2\\) are white and the last \\(2\\) are black?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-6",
    "href": "private/homework/homework2.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nA defendant is judged guilty if at least \\(2\\) out of \\(3\\) judges vote guilty. Given:\n\nProbability of a guilty vote when defendant is guilty: \\(0.7\\) \nProbability of a guilty vote when defendant is innocent: \\(0.2\\) \n\\(70%\\) of defendants are guilty\n\nCompute the conditional probability that judge \\(3\\) votes guilty given:\n\nJudges \\(1\\) and \\(2\\) vote guilty. \nJudges \\(1\\) and \\(2\\) split votes. \nJudges \\(1\\) and \\(2\\) vote not guilty.\n\nAre the judges‚Äô votes independent? Conditionally independent? Explain.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-7",
    "href": "private/homework/homework2.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7\nGiven the distribution function of \\(X\\):\n\\[\nF_{X}(x) =\n\\begin{cases}\n0, & x &lt; 0 \\\\\n\\frac{1}{2}, & 0 \\leq x &lt; 1 \\\\\n\\frac{3}{5}, & 1 \\leq x &lt; 2 \\\\\n\\frac{4}{5}, & 2 \\leq x &lt; 3 \\\\\n\\frac{9}{10}, & 3 \\leq x &lt; 3.5 \\\\\n1, & x \\geq 3.5 \\\\\n\\end{cases}\n\\]\nFind \\(p_X(x)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#question-8",
    "href": "private/homework/homework2.html#question-8",
    "title": "Homework 2",
    "section": "Question 8",
    "text": "Question 8\nA player rolls a fair die and flips a fair coin. If heads, they win twice the die value; if tails, they win half. Determine the expected winnings.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework2.html#coding-exercise-1-binomial-distribution",
    "href": "private/homework/homework2.html#coding-exercise-1-binomial-distribution",
    "title": "Homework 2",
    "section": "Coding Exercise 1: Binomial Distribution",
    "text": "Coding Exercise 1: Binomial Distribution\nThe binomial distribution PMF is:\n\\[\np_X(x) = {n \\choose k} x^n (1-x)^{n-k}\n\\]\nUsing Python, generate binomial data and create visualizations for \\(p_X(x)\\) and \\(F_X(x)\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>62</span>¬† <span class='chapter-title'>Homework 2</span>"
    ]
  },
  {
    "objectID": "private/homework/homework3.html",
    "href": "private/homework/homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Coding Exercise 1: Load Environments\nLoad existing Bernoulli and Gaussian environments from create_environment function using a random seed of \\(123\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>63</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "private/homework/homework3.html#coding-exercise-2-recommendation-systems",
    "href": "private/homework/homework3.html#coding-exercise-2-recommendation-systems",
    "title": "Homework 3",
    "section": "Coding Exercise 2: Recommendation Systems",
    "text": "Coding Exercise 2: Recommendation Systems\nUsing the existing Epsilon Greedy (\\(\\epsilon\\) = 0.10), Upper Confidence Boundary (UCB) and Thompson Sampling code, create a recommendation system.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>63</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "private/homework/homework3.html#coding-exercise-3-mab-performance",
    "href": "private/homework/homework3.html#coding-exercise-3-mab-performance",
    "title": "Homework 3",
    "section": "Coding Exercise 3: MAB Performance",
    "text": "Coding Exercise 3: MAB Performance\nFor 10,000 recommendations:\n\nDoes Epsilon-Greedy (\\(\\epsilon = 0.10\\)) perform better in the Bernoulli or Gaussian environment?\nDoes UCB perform better in the Bernoulli or Gaussian environment?\nDoes Thompson Sampling perform better in the Bernoulli or Gaussian environment?\nWhich algorithm performs best in the Bernoulli environment?\nWhich algorithm performs best in the Gaussian environment?\n\nHint: Check the performance of each MAB by observing the most frequently recommended arm.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>63</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "private/homework/homework3.html#coding-exercise-4-random-seed-analysis",
    "href": "private/homework/homework3.html#coding-exercise-4-random-seed-analysis",
    "title": "Homework 3",
    "section": "Coding Exercise 4: Random Seed Analysis",
    "text": "Coding Exercise 4: Random Seed Analysis\nUsing random seeds \\(0\\)-\\(50\\), for \\(10,000\\) recommendations, do the algorithms perform the same?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>63</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "private/homework/homework3.html#coding-exercise-5-amazon-dataset-analysis",
    "href": "private/homework/homework3.html#coding-exercise-5-amazon-dataset-analysis",
    "title": "Homework 3",
    "section": "Coding Exercise 5: Amazon Dataset Analysis",
    "text": "Coding Exercise 5: Amazon Dataset Analysis\nFor the Amazon.csv advertisement dataset repeat exercise E.4. Which arm (ad) would you recommend (advertise)?\n Download Amazon.csv here",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>63</span>¬† <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "private/homework/homework4.html",
    "href": "private/homework/homework4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Question 1\nIf the current state is \\(S_{t}\\) and actions are selected according to stochastic policy \\(\\pi\\), then what is the expectation of \\(R_{t+1}\\) in terms of \\(\\pi\\) and the four-argument function \\(p\\)?",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>64</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "private/homework/homework4.html#question-2",
    "href": "private/homework/homework4.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\nGive an equation for \\(v_{\\pi}\\) in terms of \\(q_{\\pi}\\) and \\(\\pi\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>64</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "private/homework/homework4.html#question-3",
    "href": "private/homework/homework4.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\nGive an equation for \\(q_{\\pi}\\) in terms of \\(v_{\\pi}\\) and \\(\\pi\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>64</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "private/homework/homework4.html#coding-exercise-1-markov-chain-i",
    "href": "private/homework/homework4.html#coding-exercise-1-markov-chain-i",
    "title": "Homework 4",
    "section": "Coding Exercise 1: Markov Chain I",
    "text": "Coding Exercise 1: Markov Chain I\nWith a discount factor of \\(\\gamma = 0.9\\), calculate \\(v(t)\\) when \\(t = 100\\) for the following Markov Chain:",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>64</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "private/homework/homework4.html#coding-exercise-2-markov-chain-ii",
    "href": "private/homework/homework4.html#coding-exercise-2-markov-chain-ii",
    "title": "Homework 4",
    "section": "Coding Exercise 2: Markov Chain II",
    "text": "Coding Exercise 2: Markov Chain II\nWith a discount factor of \\(\\gamma = 0.9\\), calculate \\(v(t)\\) when \\(t = 100\\) for the following Markov Chain:",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>64</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "private/homework/homework4.html#coding-exercise-3-value-iteration",
    "href": "private/homework/homework4.html#coding-exercise-3-value-iteration",
    "title": "Homework 4",
    "section": "Coding Exercise 3: Value Iteration",
    "text": "Coding Exercise 3: Value Iteration\nWith a discount factor of \\(\\gamma = 0.9\\), code Value Iteration algorithm for GridWorldEnv using the provided hyperparameters.\n\n\n\n\n\nNote: The gray shaded areas are barriers. Moving into a barrier incurs a reward of \\(R = -1\\).",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>64</span>¬† <span class='chapter-title'>Homework 4</span>"
    ]
  },
  {
    "objectID": "private/homework/homework5.html",
    "href": "private/homework/homework5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Coding Exercise 1: On-Policy Monte Carlo Control\nFor the GridWorldEnv environment, code the On-Policy Monte Carlo Control algorithm using the provided hyperparameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>65</span>¬† <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "private/homework/homework5.html#coding-exercise-2-off-policy-monte-carlo-control",
    "href": "private/homework/homework5.html#coding-exercise-2-off-policy-monte-carlo-control",
    "title": "Homework 5",
    "section": "Coding Exercise 2: Off-Policy Monte Carlo Control",
    "text": "Coding Exercise 2: Off-Policy Monte Carlo Control\nFor the GridWorldEnv environment, code the Off-Policy Monte Carlo Control algorithm using the provided hyperparameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>65</span>¬† <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "private/homework/homework6.html",
    "href": "private/homework/homework6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Coding Exercise 1: SARSA\nFor the GridWorldEnv environment, code the SARSA algorithm using the provided hyperparameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>66</span>¬† <span class='chapter-title'>Homework 6</span>"
    ]
  },
  {
    "objectID": "private/homework/homework6.html#coding-exercise-2-q-learning",
    "href": "private/homework/homework6.html#coding-exercise-2-q-learning",
    "title": "Homework 6",
    "section": "Coding Exercise 2: Q-Learning",
    "text": "Coding Exercise 2: Q-Learning\nFor the GridWorldEnv environment, code the Q-learning algorithm using the provided hyperparameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>66</span>¬† <span class='chapter-title'>Homework 6</span>"
    ]
  },
  {
    "objectID": "private/homework/homework6.html#coding-exercise-3-double-q-learning",
    "href": "private/homework/homework6.html#coding-exercise-3-double-q-learning",
    "title": "Homework 6",
    "section": "Coding Exercise 3: Double Q-Learning",
    "text": "Coding Exercise 3: Double Q-Learning\nFor the GridWorldEnv environment, code the Double Q-learning algorithm using the provided hyperparameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>66</span>¬† <span class='chapter-title'>Homework 6</span>"
    ]
  },
  {
    "objectID": "private/homework/homework7.html",
    "href": "private/homework/homework7.html",
    "title": "Homework 7",
    "section": "",
    "text": "Coding Exercise 1: Semi-Gradient SARSA\nFor the MountainCar-v0 environment, code the update function for Semi-Gradient SARSA algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>67</span>¬† <span class='chapter-title'>Homework 7</span>"
    ]
  },
  {
    "objectID": "private/homework/homework8.html",
    "href": "private/homework/homework8.html",
    "title": "Homework 8",
    "section": "",
    "text": "Coding Exercise 1: Deep Q-Network (DQN)\nFor the CartPole-v1 environment, code the update function for the Deep Q-Netowrk (DQN) algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>68</span>¬† <span class='chapter-title'>Homework 8</span>"
    ]
  },
  {
    "objectID": "private/homework/homework9.html",
    "href": "private/homework/homework9.html",
    "title": "Homework 9",
    "section": "",
    "text": "Coding Exercise 1: Vanilla Policy Gradient\nFor the CartPole-v1 environment, code the update function for the Vanilla Policy Gradient algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>69</span>¬† <span class='chapter-title'>Homework 9</span>"
    ]
  },
  {
    "objectID": "private/homework/homework10.html",
    "href": "private/homework/homework10.html",
    "title": "Homework 10",
    "section": "",
    "text": "Coding Exercise 1: Proximal Policy Optimization (PPO) Clip\nFor the HalfCheetah-v5 environment, code the update function for the Proximal Policy Optimization: Clip algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>70</span>¬† <span class='chapter-title'>Homework 10</span>"
    ]
  },
  {
    "objectID": "private/homework/homework11.html",
    "href": "private/homework/homework11.html",
    "title": "Homework 11",
    "section": "",
    "text": "Coding Exercise 1: Monte Carlo Tree Search (MCTS)\nFor the CartPole-v0 environment, code the update function for the Monte Carlo Tree Search (MCTS) algorithm using the provided parameters.",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>71</span>¬† <span class='chapter-title'>Homework 11</span>"
    ]
  },
  {
    "objectID": "private/homework/homework12.html",
    "href": "private/homework/homework12.html",
    "title": "Homework 12",
    "section": "",
    "text": "No Homework! Enjoy! üòä",
    "crumbs": [
      "Homeworks",
      "<span class='chapter-number'>72</span>¬† <span class='chapter-title'>Homework 12</span>"
    ]
  },
  {
    "objectID": "private/exams/exam1.html",
    "href": "private/exams/exam1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Instructions\nOnly Classical Reinforcement Learning algorithms that are model-free will be considered:",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>73</span>¬† <span class='chapter-title'>Exam 1</span>"
    ]
  },
  {
    "objectID": "private/exams/exam1.html#instructions",
    "href": "private/exams/exam1.html#instructions",
    "title": "Exam 1",
    "section": "",
    "text": "You will work in your current teams.\nAfter class, each team will be sent an email with their specific alias name and exam_env.py.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou must keep the teams alias anonymous!\nNot doing so will result in a penalty of \\(10\\%\\) of the total grade of Exam 1 for you and your group.\n\n\n\nFrom Monday (October 20th) to Friday (October 24th), each team is in charge of coding one main.py and model.py for the Dynamic GridWorld environment.\n\n\n\nOn-Policy Monte Carlo\nOff-Policy Monte Carlo\nSARSA\nQ-Learning\nDouble Q-Learning\nn-step Bootstrapping\n\n\nPlease keep the following hyperparameters intact:\n\nSEED = 123\nSIZE = 10\nEPISODES = 100\nFPS = 0\n\n\n\n\n\n\nImportant\n\n\n\nDo not alter exam_env.py!\nDoing so might result in skewed results for your team.",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>73</span>¬† <span class='chapter-title'>Exam 1</span>"
    ]
  },
  {
    "objectID": "private/exams/exam1.html#submission",
    "href": "private/exams/exam1.html#submission",
    "title": "Exam 1",
    "section": "Submission",
    "text": "Submission\nEach team is responsible for submitting one ALIAS_main.py and ALIAS_model.py as a DAY#_ALIAS.zip.\nRemember to put the corresponding alias name for your team in ALIAS_main.py.\nALIAS = 'your alias goes here'\n\n\n\n\n\n\nTip\n\n\n\nSubmit your work on the same email!\nRemember the email you and your team members will get after class with your alias name? Submit your DAY#_ALIAS.zip by simply replying to that email chain.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease make sure that your code runs properly!\nAny submission that is not following any of the steps above or does not run in ALIAS_main.py will not be accepted.",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>73</span>¬† <span class='chapter-title'>Exam 1</span>"
    ]
  },
  {
    "objectID": "private/exams/exam1.html#leaderboard",
    "href": "private/exams/exam1.html#leaderboard",
    "title": "Exam 1",
    "section": "Leaderboard",
    "text": "Leaderboard\nEach day a leaderboard will be updated here to display the results of each team.\n\n\n\n\n\n\nNoteDay 1 (Monday, October 20th)\n\n\n\n\n\n\nRank\nAlias\n\\(\\bar{G}\\)\n\n\n\n\nü•á 1st Place\nBarcelona\n1.83\n\n\nü•à 2nd Place\nHanoi\n1.36\n\n\nü•â 3rd Place\nDelhi\n-0.32\n\n\n4th Place\nCaracas\n-22.6\n\n\n5th Place\nMarkham\n-56.43\n\n\n6th Place\nWashington\n-218.78\n\n\n\n\n\n\n\n\n\n\n\nNoteDay 2 (Tuesday, October 21st)\n\n\n\n\n\n\nRank\nAlias\n\\(\\bar{G}\\)\n\n\n\n\nü•á 1st Place\nDelhi\n8.09\n\n\nü•à 2nd Place\nMarkham\n2.23\n\n\nü•â 3rd Place\nBarcelona\n1.83\n\n\n4th Place\nHanoi\n1.36\n\n\n5th Place\nCaracas\n-22.6\n\n\n6th Place\nWashington\n-218.78\n\n\n\n\n\n\n\n\n\n\n\nNoteDay 3 (Wednesday, October 22nd)\n\n\n\n\n\n\nRank\nAlias\n\\(\\bar{G}\\)\n\n\n\n\nü•á 1st Place\nHanoi\n8.69\n\n\nü•à 2nd Place\nCaracas\n8.68\n\n\nü•â 3rd Place\nDelhi\n8.57\n\n\n4th Place\nBarcelona\n8.36\n\n\n5th Place\nMarkham\n2.23\n\n\n6th Place\nWashington\n-218.78\n\n\n\n\n\n\n\n\n\n\n\nNoteDay 4 (Thursday, October 23rd)\n\n\n\n\n\n\nRank\nAlias\n\\(\\bar{G}\\)\n\n\n\n\nü•á 1st Place\nDelhi\n9.53\n\n\nü•à 2nd Place\nCaracas\n9.17\n\n\nü•â 3rd Place\nHanoi\n8.91\n\n\n4th Place\nBarcelona\n8.36\n\n\n5th Place\nMarkham\n6.01\n\n\n6th Place\nWashington\n5.23\n\n\n\n\n\n\n\n\n\n\n\nNoteDay 5 (Friday, October 24th)\n\n\n\n\n\n\nRank\nAlias\n\\(\\bar{G}\\)\n\n\n\n\nü•á 1st Place\nDelhi\n9.53\n\n\nü•à 2nd Place\nHanoi\n9.52\n\n\nü•â 3rd Place\nWashington\n9.45\n\n\n4th Place\nCaracas\n9.37\n\n\n5th Place\nBarcelona\n8.36\n\n\n6th Place\nMarkham\n6.01",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>73</span>¬† <span class='chapter-title'>Exam 1</span>"
    ]
  },
  {
    "objectID": "private/exams/exam2.html",
    "href": "private/exams/exam2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 2 Information - Coming Soon üì¢",
    "crumbs": [
      "Exams",
      "<span class='chapter-number'>74</span>¬† <span class='chapter-title'>Exam 2</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-overview.html",
    "href": "private/final-project/final-project-overview.html",
    "title": "Final Project Overview",
    "section": "",
    "text": "Instructions\nInstruction table for the Final Project.",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>75</span>¬† <span class='chapter-title'>Final Project Overview</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-overview.html#instructions",
    "href": "private/final-project/final-project-overview.html#instructions",
    "title": "Final Project Overview",
    "section": "",
    "text": "Multi-Armed Bandit (MAB)Classical/Deep Reinforcement Learning\n\n\nUsing the different Multi-Armed Bandit algorithms learned in Lecture 3. Your task is to:\n\nDefine the Reinforcement Learning Framework: Formulate a Multi-Armed Bandit problem.\n\n\nDefine the data.\nSpecify the action space \\(\\mathcal{A}\\).\nSpecify the reward structure \\(R\\).\n\n\nDefine the model: Implement at least two Reinforcement Learning algorithms to solve the problem.\n\n\n\\(\\epsilon\\)-Greedy.\nUpper Confidence Boundary (UCB).\nThompson Sampling.\n\n\nDefine the metrics: To evaluate performance, we will use:\n\n\nCumulative reward: Total return over a time horizon.\n(Optional) Regret: The difference between the reward of the best fixed arm and the reward obtained by the algorithm.\n(Optional) Stability score: Standard deviation of the reward over time.\n(Optional) Adaptability: Performance when the environment shifts.\n\n\nEffectively communicate your findings: CV style.\n\nAccomplished [X] as measured by [Y], by doing [Z].\nExample: Improved asset allocation strategy stability as measured by lower reward variance across trials, by tuning \\(\\epsilon\\) in an \\(\\epsilon\\)-Greedy policy.\n\n\nUsing a Classical or Deep Reinforcement Learning algorithm learned in Lectures 5-11. Your task is to:\n\nDefine the Reinforcement Learning Framework: Formulate a Markov Decision Process (MDP).\n\n\nDefine the environment dynamics \\(P(s',r|s,a)\\).\nDefine the state space \\(\\mathcal{S}\\).\nDefine the action space \\(\\mathcal{A}\\).\nDefine the reward function \\(R\\).\n\nDefine the episode structure.\n\n\nDefine the model: Implement at least one Reinforcement Learning algorithm to solve the problem.\n\n\nOn-Policy Monte Carlo\nOff-Policy Monte Carlo\nSARSA\nQ-Learning\nDouble Q-Learning\nn-step Bootstrapping\nSemi-Gradient SARSA\nDeep Q-Network (DQN)\nVanilla Policy Gradient (VPG)\nProximal Policy Optimization (PPO)\nMonte Carlo Tree Search (MCTS)\n\n\nDefine the metrics: To evaluate performance, we will use:\n\n\nCumulative reward: Total return over a time horizon.\n(Optional) Stability score: Standard deviation of the reward over time.\n\n\nEffectively communicate your findings: CV style.\n\nAccomplished [X] as measured by [Y], by doing [Z].\nExample: Improved asset allocation strategy stability as measured by lower reward variance across trials, by tuning \\(\\epsilon\\) in an \\(\\epsilon\\)-Greedy policy.",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>75</span>¬† <span class='chapter-title'>Final Project Overview</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-overview.html#presentation",
    "href": "private/final-project/final-project-overview.html#presentation",
    "title": "Final Project Overview",
    "section": "Presentation",
    "text": "Presentation\nYour team must clearly present all of the following points:\n\nDefine the Reinforcement Learning Framework.\nDefine the model.\nDefine the metrics.\nEffectively communicate your findings.\nConclusion.\n\nUse clear and concise slides with visuals (e.g., charts, diagrams) to support your points. Ensure the presentation is structured logically and is easy to follow.\nHere‚Äôs an example from Capstone Group 11:",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>75</span>¬† <span class='chapter-title'>Final Project Overview</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-overview.html#submission",
    "href": "private/final-project/final-project-overview.html#submission",
    "title": "Final Project Overview",
    "section": "Submission",
    "text": "Submission\nEach team must submit prior to the deadline, each of the following:\n\nCode .zip: A fully functional implementation of your Reinforcement Learning algorithms, with main.py, model.py and env.py.\nReport .pdf: A detailed report summarizing your approach, findings, and conclusions.\nPresentation .pdf: A concise and clear presentation of your work, including visuals and key takeaways.\n\nLate submissions will incur a \\(10\\%\\) penalty for the team.",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>75</span>¬† <span class='chapter-title'>Final Project Overview</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-overview.html#grading",
    "href": "private/final-project/final-project-overview.html#grading",
    "title": "Final Project Overview",
    "section": "Grading",
    "text": "Grading\nSummary of the grading table:\n\n\n\n\n\n\n\n\nCriteria\nDescription\nWeight\n\n\n\n\nProblem Formulation\nClear definition of the Reinforcement Learning framework.\n20%\n\n\nAlgorithm Implementation\nCorrect and efficient implementation of at least two Reinforcement Learning algorithms.\n30%\n\n\nPerformance Evaluation\nUse of appropriate metrics to evaluate the algorithms‚Äô performance.\n20%\n\n\nCommunication of Findings\nClear and concise presentation of results, including visuals and insights.\n20%\n\n\nCode Quality and Documentation\nWell-structured, readable, and documented code.\n10%",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>75</span>¬† <span class='chapter-title'>Final Project Overview</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-overview.html#optional-capstone-or-research",
    "href": "private/final-project/final-project-overview.html#optional-capstone-or-research",
    "title": "Final Project Overview",
    "section": "(Optional) Capstone or Research",
    "text": "(Optional) Capstone or Research\nIf you or your team would like to discuss this project or potential research opportunities in more detail, feel free to reach out to me at twallett@gwu.edu.",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>75</span>¬† <span class='chapter-title'>Final Project Overview</span>"
    ]
  },
  {
    "objectID": "private/final-project/final-project-ideas.html",
    "href": "private/final-project/final-project-ideas.html",
    "title": "Final Project Ideas",
    "section": "",
    "text": "Final Project Ideas üèÅ\n\n\n\n\nMulti-Armed Bandits (MAB)Classical/Deep Reinforcement Learning\n\n\n\n\n\n\n\n\nTipSmart Ad Click Optimizer\n\n\n\n\n\nBuild a system that learns which ads get clicked most often. Start with fake ad data and test different strategies like Œµ-greedy (sometimes try random ads) and Thompson Sampling (use probability to guide choices).\nWhat you‚Äôll learn: How to balance exploring new options vs.¬†using what works best.\n\n\n\n\n\n\n\n\n\nTipPersonalized Article Recommender\n\n\n\n\n\nCreate a recommendation system that suggests articles based on context (time of day, device type, user history). Uses contextual bandits that consider multiple factors when making decisions.\nWhat you‚Äôll learn: How to incorporate user context into decision-making.\n\n\n\n\n\n\n\n\n\nTipDynamic Pricing Bot\n\n\n\n\n\nBuild a pricing algorithm that adjusts product prices daily based on demand patterns. The system learns optimal pricing to maximize revenue while adapting to market changes.\nWhat you‚Äôll learn: How to handle changing environments and non-stationary problems.\n\n\n\n\n\n\n\n\n\nTipWebsite Banner Optimizer\n\n\n\n\n\nAutomatically choose which banner ad to show on a website to get the most clicks. The system adapts in real-time as user preferences change throughout the day.\nWhat you‚Äôll learn: Real-time optimization and A/B testing with learning algorithms.\n\n\n\n\n\n\n\n\n\nTipSmart Menu Recommender for Food Apps\n\n\n\n\n\nSimulate a food delivery app that learns which dishes to highlight based on factors like time of day, weather, and day of the week to increase orders.\nWhat you‚Äôll learn: Seasonal patterns and time-based optimization.\n\n\n\n\n\n\n\n\n\n\n\nTipRL-Driven Recommendation Engine (Google RecSim Inspired)\n\n\n\n\n\nBuild a small-scale recommendation system using RL to model long-term engagement, inspired by Google‚Äôs RecSim research, but on a synthetic dataset.\nWhat you‚Äôll learn: Sequential user interaction modeling and long-term recommendation optimization.\n\nGoogle RecSim ‚Äì Official Repository\n\nRecSim Research Paper ‚Äì arXiv\n\nGoogle AI Blog ‚Äì RecSim Platform\n\nRecSim Implementation Example ‚Äì GitHub\n\n\n\n\n\n\n\n\n\n\nTipHighway Driving AI Agent\n\n\n\n\n\nTrain a virtual car to drive on highways using the highway-env simulator. The agent learns lane changing, speed control, and safe overtaking maneuvers.\nWhat you‚Äôll learn: Continuous control and safety constraints in RL.\n\nHighway-Env RL Project ‚Äì GitHub\nHighway-Env RL Documentation\n\n\n\n\n\n\n\n\n\n\nTipRobot Arm Controller\n\n\n\n\n\nTrain a simulated robot to pick up and place objects using Gymnasium‚Äôs robot environments. Start with simple reaching tasks and progress to manipulation.\nWhat you‚Äôll learn: Robotics control and continuous action spaces.\n\nRobotics-Env RL Project - GitHub\n\nRobotics-Env RL Documentation\n\n\n\n\n\n\n\n\n\n\nTipAI Trading Bot with FinRL\n\n\n\n\n\nCreate a trading algorithm that learns to buy and sell stocks/ETFs using historical data. Focus on risk management and portfolio optimization strategies.\nWhat you‚Äôll learn: Financial applications of RL and risk-reward optimization.\nhttps://finrl.readthedocs.io/en/latest/index.html\n\nFinRL - GitHub\n\nFinRL Documentation\n\n\n\n\n\n\n\n\n\n\nTipHuman Feedback Learning Interface\n\n\n\n\n\nBuild a web app where users rate AI-generated text responses. The system learns from human preferences to improve its outputs over time (simplified RLHF demo).\nWhat you‚Äôll learn: Human-in-the-loop learning and preference modeling.\n\nTRL Examples - HuggingFace\nRLHF ‚Äì Medium\n\nStreamlit Documentation",
    "crumbs": [
      "Final Project",
      "<span class='chapter-number'>76</span>¬† <span class='chapter-title'>Final Project Ideas</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acero, Fernando, Parisa Zehtabi, Nicolas Marchesotti, Michael Cashmore,\nDaniele Magazzeni, and Manuela Veloso. 2024. ‚ÄúDeep Reinforcement\nLearning and Mean-Variance Strategies for Responsible Portfolio\nOptimization.‚Äù https://arxiv.org/abs/2403.16667.\n\n\nBarto, Andrew G, Richard S Sutton, and Charles W Anderson. 1983.\n‚ÄúNeuronlike Adaptive Elements That Can Solve Difficult Learning\nControl Problems.‚Äù Technical Report, Institute for Cybernetic\nStudies, University of Massachusetts. https://psycnet.apa.org/record/1984-25798-001.\n\n\nBellemare, Marc G, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013.\n‚ÄúThe Arcade Learning Environment: An Evaluation Platform for\nGeneral Agents.‚Äù Journal of Artificial Intelligence\nResearch 47: 253‚Äì79.\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to\nProbability. 2nd ed. Belmont, MA: Athena Scientific.\n\n\nBrockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\nSchulman, Jie Tang, and Wojciech Zaremba. 2016. ‚ÄúOpenAI\nGym.‚Äù https://arxiv.org/abs/1606.01540.\n\n\nBrunskill, Emma. 2022. ‚ÄúCS234: Reinforcement Learning - Lecture\n1.‚Äù Course Lecture Slides, Stanford University. https://web.stanford.edu/class/cs234/slides/lecture1pre.pdf.\n\n\nFawzi, Alhussein, Matej Balog, Atri Huang, et al. 2022.\n‚ÄúDiscovering Faster Matrix Multiplication Algorithms with\nReinforcement Learning.‚Äù Nature 610: 47‚Äì53. https://doi.org/10.1038/s41586-022-05172-4.\n\n\nHagan, Martin T., Howard B. Demuth, Mark H. Beale, and Orlando De Jes√∫s.\n2014. Neural Network Design. 2nd ed. Martin Hagan. https://hagan.okstate.edu/NNDesign.pdf.\n\n\nHammack, Richard H. 2013. Book of Proof. Richard Hammack.\n\n\nIe, Eugene, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar,\nJing Wang, Rui Wu, and Craig Boutilier. 2019. ‚ÄúRecSim: A\nConfigurable Simulation Platform for Recommender Systems.‚Äù https://arxiv.org/abs/1909.04847.\n\n\nLevine, Sergey. 2019. ‚ÄúIntroduction to Deep Reinforcement\nLearning.‚Äù Course Lecture Slides, Deep RL Course, UC Berkeley. https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-1.pdf.\n\n\nLi, Lihong, Wei Chu, John Langford, and Robert E. Schapire. 2010.\n‚ÄúA Contextual-Bandit Approach to Personalized News Article\nRecommendation.‚Äù In Proceedings of the 19th International\nConference on World Wide Web, 661‚Äì70. ACM.\n\n\nMartin T. Hagan, Amir Jafari. 2024. ‚ÄúNNDesignDeepLearning.‚Äù\nhttps://github.com/NNDesignDeepLearning/NNDesignDeepLearning.\n\n\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. 2013. ‚ÄúPlaying\nAtari with Deep Reinforcement Learning.‚Äù https://arxiv.org/abs/1312.5602.\n\n\nMoore, Andrew William. 1990. ‚ÄúEfficient Memory-Based Learning for\nRobot Control.‚Äù University of Cambridge.\n\n\nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\nPamela Mishkin, Chong Zhang, et al. 2022. ‚ÄúTraining Language\nModels to Follow Instructions with Human Feedback.‚Äù https://arxiv.org/abs/2203.02155.\n\n\nSanz-Cruzado, Javier, Nikolaos Droukas, and Richard McCreadie. 2024.\n‚ÄúFAR-Trans: An Investment Dataset for Financial Asset\nRecommendation.‚Äù https://arxiv.org/abs/2407.08692.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement\nLearning: An Introduction. 2nd ed. Cambridge, MA: MIT Press.\n\n\nTodorov, Emanuel, Tom Erez, and Yuval Tassa. 2012. ‚ÄúMuJoCo: A\nPhysics Engine for Model-Based Control.‚Äù In *Proceedings of the\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS)*. http://www.mujoco.org/.",
    "crumbs": [
      "References"
    ]
  }
]